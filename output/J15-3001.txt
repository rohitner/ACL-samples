#######################
## output of pdf2txt ##
#######################

Large Linguistic Corpus Reduction with
SCP Algorithms

∗

Nelly Barbot
IRISA, University of Rennes 1

∗
Olivier Bo¨effard
IRISA, University of Rennes 1

Jonathan Chevelu
IRISA, University of Rennes 1

∗

∗

Arnaud Delhay
IRISA, University of Rennes 1

Linguistic corpus design is a critical concern for building rich annotated corpora useful in
different domains of applications. For example, speech technologies such as ASR (Automatic
Speech Recognition) or TTS (Text-to-Speech) need a huge amount of speech data to train data-
driven models or to produce synthetic speech. Collecting data is always related to costs (recording
speech, verifying annotations, etc.), and as a rule of thumb, the more data you gather, the
more costly your application will be. Within this context, we present in this article solutions
to reduce the amount of linguistic text content while maintaining a sufﬁcient level of linguistic
richness required by a model or an application. This problem can be formalized as a Set Covering
Problem (SCP) and we evaluate two algorithmic heuristics applied to design large text corpora
in English and French for covering phonological information or POS labels. The ﬁrst considered
algorithm is a standard greedy solution with an agglomerative/spitting strategy and we propose
a second algorithm based on Lagrangian relaxation. The latter approach provides a lower bound
to the cost of each covering solution. This lower bound can be used as a metric to evaluate the
quality of a reduced corpus whatever the algorithm applied. Experiments show that a suboptimal
algorithm like a greedy algorithm achieves good results; the cost of its solutions is not so far
from the lower bound (about 4.35% for 3-phoneme coverings). Usually, constraints in SCP are
binary; we proposed here a generalization where the constraints on each covering feature can be
multi-valued.

∗ IRISA, University of Rennes 1, 6 rue de Kerampont, Lannion 22300, France.
E-mail: {nelly.barbot, olivier.bo(cid:127)effard, jonathan.chevelu, arnaud.delhay}@irisa.fr.
This work is partially supported by the French National Research Agency (ANR) in the framework of the
project Phorevox.

Submission received: 7 May 2013; revised submission received: 20 November 2014; accepted for publication:
18 March 2015.

doi:10.1162/COLI a 00225

© 2015 Association for Computational Linguistics

Computational Linguistics

Volume 41, Number 3

1. Introduction

In automatic speech and language processing, many technologies make extensive use
of written or read text sets. These linguistic corpora are a necessity to train models or to
extract rules, and the quality of the results strongly depends on a corpus’ content. Often,
the reference corpus should provide a maximum diversity of content. For example, in
Tian, Nurminen, and Kiss (2005), and Tian and Nurminen (2009), it turns out that max-
imizing the text coverage of the learning corpus improves an automatic syllabiﬁcation
based on a neural network. Similarly, a high quality speech synthesis system based on
the selection of speech units requires a rich corpus in terms of diphones, diphones
in context, triphones, and prosodic markers. In particular, Bunnell (2010) shows the
importance of a good coverage of diphones and triphones for the intelligibility of a
voice produced by a unit selection speech synthesis system.

To cover the best attributes needed for a task, several strategies are then possible. A
ﬁrst method—very simple—is to collect text randomly, but it soon becomes expensive
because of the natural distribution of linguistic events following the Zipf’s law. Very
few events are extremely frequent and many events are very rare. This problem is
often made difﬁcult by the fact that many technologies require several variants of the
same event (as in a Text-to-Speech [TTS] system using several acoustical versions of
the same phonological unit). Usually, a large volume of data needs to be collected.
However, and depending on the applications, building such corpora is often achieved
under a constraint of parsimony. As an example, for a TTS system, a high-quality
synthetic voice generally needs a huge number of speech recordings. But minimizing
the duration of a recording is also a critical point to ensure uniform quality of the
voice, to reduce the drudgery of the recording, to reduce the ﬁnancial cost, or to
follow a technical constraint on the amount of collected data for embedded systems.
Moreover, a reduced set tends to limit the need of human implication for checking
the data (transcription and annotation). Similarly, in the natural language processing
ﬁeld (NLP), the adaptation of a generic model to a speciﬁc domain often requires new
annotated data that illustrate its speciﬁcities (as in Candito, Anguiano, and Seddah
2011). However, the creation cost of such data highly depends on the kind of labels
used to adapt the model. In particular, the annotation in syntax trees is really more
expensive than in Part-of-Speech (POS) tags. Then, it could be more efﬁcient to an-
notate a compact corpus that reﬂects the phenomena variability than a corpus with
a natural distribution of events, which implies many redundancies (see Neubig and
Mori 2010).

In a machine learning framework, the active learning strategy can be used as
an alternative that reduces the manual data annotation effort to design the training
corpus without diminishing the quality of the model to train (see Settles 2010 or Schein,
Sandler, and Ungar 2004). It consists of building the corpus iteratively by choosing
an item according to an external source of information (a user or an experimental
measure). This approach has been applied in NLP, speech recognition, and spoken
language understanding (see for instance Tomanek and Olsson 2009 and Gotab, B´echet,
and Damnati 2009).

A second alternative, when no direct quality measure is available, consists of
covering a large set of attributes that may impact the ﬁnal quality (after annotation
or recording). This kind of approach might also be preferred when the ﬁnal corpus
is built in one batch (for instance, because of out-sourcing or annotator/performer
consistency constraints). A method could be an automatic extraction from a huge
text corpus of a minimal sized subset that covers the identiﬁed attributes. This

356

Barbot et al.

Large Linguistic Corpus Reduction

problem is a generalization of the Set-Covering Problem (SCP), which is an NP-
hard problem, as shown in Karp (1972). It is then necessary to use heuristics or
sub-optimal algorithms for a reasonable computation time. Moreover, Raz and Safra
(1997) and Alon, Moshkovitz, and Safra (2006) have shown that the SCP cannot
be polynomially approximated with ratio c × ln(n) unless P = NP, when c is a con-
stant, and n refers to the size of the universe to cover. That means that one can-
not be certain to obtain a result under this ratio with any polynomial algorithm.
However, the latter complexity results are given for any kind of distribution in the
mono-representation case. One can ask if good multi-represented coverages can be
achieved efﬁciently on data following Zipf’s law, which is usual in the domain of
NLP.

Within the ﬁeld of speech processing, the most frequently used strategy is a greedy
method based on an agglomeration policy. This iterative algorithm selects the sen-
tence with the highest score at each iteration. The score reﬂects the contribution of
the sentence to the covering under construction. In Gauvain, Lamel, and Esk´enazi
(1990), this methodology has been applied to build a database of read speech from
a text corpus for the evaluation of speech recognition systems using hierarchically
organized covering attributes. Van Santen and Buchsbaum (1997) have tested different
variants of greedy selection of texts by varying the units to cover (diphones, dura-
tion, etc.) and the “scores” for a sentence depending on the considered applications.
In Tian, Nurminen, and Kiss (2005), the learning corpus for an automatic system of
syllabiﬁcation is designed using a greedy approach with the Levenshtein distance
as a score function in order to maximize its text diversity. In Franc¸ois and Bo¨effard
(2001), the methodology gives a priority to the rarest categories of allophones. The
latter methodology has been implemented for the deﬁnition of the multi-speaker corpus
Neologos in Krstulovi´c et al. (2006). In the article of Krul et al. (2006), the authors
constructed a corpus where the distribution of diphonemes/triphonemes matches a
uniform distribution. A greedy algorithm is led by a score function based on the
Kullback-Liebler divergence. A similar method is used in Krul et al. (2007) to design a
reduced database in accordance with a speciﬁc domain distribution. Kawai et al. (2000)
propose a pair exchange mechanism that Rojc and Kaˇciˇc (2000) apply after a ﬁrst reverse
greedy algorithm—also called spitting greedy—deleting the useless sentences. In Cadic,
Boidin, and d’Alessandro (2010), the covering of “sandwich” units (deﬁned to be more
adapted to corpus-based speech synthesis) is carried out by generating new sentences
in a semi-automatic way. Candidates are generated using ﬁnite state transducers. The
sentences are ordered according to a greedy criterion (their sandwiches richness) and
presented to a human evaluator. This collection of artiﬁcial and rich sentences en-
ables an effective reduction of the size of the covering but requires expensive human
intervention to obtain semantically correct sentences that will be therefore easier to
record.

The results of these previously cited studies are difﬁcult to compare because of
the different initial corpora and covering constraints (partial or full covering) and
evaluation criteria (the number of gathered sentences, the Kullback divergence, etc.).
In Zhang and Nakamura (2008), a priority policy for the rare units is added into an
agglomerative greedy algorithm in order to get a covering of triphoneme classes from a
large text corpus in Chinese language. The results show that this priority policy driven
by the score function and the phonetic content of the sentences reduces the covering
size compared with a standard agglomerative greedy algorithm.

Similarly, in Franc¸ois and Bo¨effard (2002), several combinations of greedy algo-
rithms (agglomeration, spitting, pair exchange, or priority to rare units) were applied

357

Computational Linguistics

Volume 41, Number 3

to the construction of a corpus for speech synthesis in French containing at least three
representatives of the most frequent diphones. Based on this work, the best strategy
would be the application of an agglomerative greedy followed by a spitting greedy
algorithm. During the agglomeration phase, the score of a sentence corresponds to the
number of its unit instances that remain to be covered normalized by its length. During
the spitting phase, at each iteration, the longest redundant sentence is removed from the
covering. This algorithm is called the Agglomeration and Spitting Algorithm (ASA).
As an alternative to a greedy algorithm, which is sub-optimal, solving the SCP using
Lagrangian relaxation principles can provide an exact solution for problems of reason-
able size. However, for speech processing, the SCP has several millions of sentences with
tens of thousands of covering features. Considering these practical constraints, Chevelu
et al. (2007) adapted a Lagrangian relaxation based algorithm proposed by Caprara,
Fischetti, and Toth (1999). In the context of Italian railways, Caprara, Fischetti, and Toth
proposed heuristics to solve scheduling problems and won a competition, called
Faster, organized by the Italian Operational Research Society in 1994, ahead of other
Lagrangian relaxation heuristics–based algorithms, like Ceria, Nobili, and Sassano
(1998).

In Chevelu et al. (2007, 2008), the algorithm takes into account the constraints of
multi-representation. A minimal number of representatives for the same unit may be
required. The proposed algorithm, called LamSCP — Lagrangian-based Algorithm
for Multi-represented SCP — is applied to extract coverings of diphonemes with a
mono- or a 5-representation and coverings of triphonemes with mono-representation
constraints. These results are compared with the greedy strategy ASA and are about 5%
to 10% better. Besides, the LamSCP provides a lower bound for the cost of the optimal
covering and allows for evaluating the quality of the results.

In Barbot, Bo¨effard, and Delhay (2012), phonological content of diphoneme cover-
ings is studied regarding many parameters. These coverings are obtained by different
algorithms (LamSCP, ASA, greedy based on the Kullback divergence) and some of
the coverings are randomly completed to reach a given size (from 20,000 to 30,000
phones). It turns out that the coverings obtained using LamSCP and ASA provide a
good representation of short units and the representation of long units mainly depends
on the length of the corpus.

In this article, we present in more detail the LamSCP algorithm and its score func-
tions and heuristics that take into account multi-representation constraints. We deepen
the study about the performance of LamSCP for the construction of a phonologically
rich corpus according to the size of the search space. We evaluate LamSCP and ASA
algorithms on a corpus of sentences in English for a covering of multi-represented
diphones, where the minimal number of required unit representatives varies from one
to ﬁve. We also compare them in the case of very constrained triphoneme coverings in
English and French, which represent about 12 times more units to cover. Additionally,
both algorithms are tested to provide multi-represented coverings of POS tags in order
to assess their ability to deal with different kinds of linguistic data. A particular effort
has been made on methodology to obtain comparable measures, to study the stability
of both algorithms, and to establish conﬁdence intervals for each solution.

This article is organized as follows. In Section 2, the SCP framework and the
associated notations are introduced. The ASA algorithm is described in Section 3 and
the LamSCP is detailed in Section 4. The experimental methodology is presented in
Section 5 and results are discussed in Section 6. Before concluding in Section 8, we
present experiments in the context of TTS where we evaluate on that task the beneﬁts
of a reduction in section 7.

358

Barbot et al.

Large Linguistic Corpus Reduction

2. The Set-Covering Problem

Before describing the SCP-solving algorithms proposed in this article, we introduce in
this section some notations and the Lagrangian properties used by LamSCP.
Let us consider a corpus A composed of n sentences s1, : : : , sn. According to the
target applications, these sentences are annotated with respect to phonological, acoustic,
prosodic attributes, and so forth. Each sentence is then associated with a family of units
of different types. The set of units present in A is denoted U = {u1, : : : , um} and A can
be represented by a matrix A = (aij), where aij is the number of instances of unit ui in the
sentence sj. Therefore, the jth column of A corresponds to sentence sj in A. To simplify
the writing, we deﬁne the sets M = {1, : : : , m} and N = {1, : : : , n}.
For a given vector of integers B = (b1, : : : , bm)T, a reduction X of A, also called
covering of U, is deﬁned as a subset of A which contains, for every i ∈ M, at least bi
instances of ui. It can be described by a vector X = (x1, : : : , xn)T where xj = 1 if sj belongs
to X and xj = 0 otherwise. In other words, a covering is a solution X ∈ {0, 1}n of the
following system

∀i ∈ M,

aijxj ≥ bi

(1)

∑

j2N

that is, AX ≥ B where B is called the constraint vector.

Our aim is to optimize a covering according to a cost function minimization crite-
rion. The covering cost is given by summing the costs of the sentences that compose the
covering. The optimization problem can be formulated as the following SCP:

(cid:3)

X

= arg min
X∈{0,1}n
AX≥B

CX

(2)

where C = (c1, : : : , cn) is the cost vector and cj the cost of the sentence sj. Because of the
objective to minimize the total length of the covering, we have chosen to deﬁne the cost
of a sentence as one of its length features. According to the considered application, the
sentence cost can be deﬁned as its number of phones (one of our objectives is to design
a phonetically rich script with a minimal speech recording duration), or its number of
words, part-of-speech tags, breath groups, and so on.

In Caprara, Fischetti, and Toth (1999), Caprara, Toth, and Fischetti (2000), and Ceria,
Nobili, and Sassano (1998), the studied crew scheduling problem is a particular case of
Equation (2) where A is a binary matrix and B = 1Rm (i.e., with mono-representation
constraints). In order to ensure that Equation (1) admits a solution, we assume that, for
each i ∈ M, the minimal number bi of ui instances required in the covering is not greater
than the number (A1Rn )i of ui instances in A, that is A1Rn ≥ B. Under this assumption, A
is the maximal size solution of Equation (1), represented by X = 1Rn. In the case where
bi is greater than the number of ui instances in A, bi is set to (A1Rn )i.
To drive the SCP algorithms during the sentence selection phase, the covering
capacity (cid:22)j of sentence sj is deﬁned as the number of its unit instances required in the
covering in view of the constraint vector:

{

}

∑

(cid:22)j =

min

i2M

aij, bi

(3)

359

Computational Linguistics

Volume 41, Number 3

Let us notice that (cid:22)j does not consider the excess unit instances: For example, if sj
contains aij = 10 instances of ui and at least bi = 3 instances of ui are required, the
contribution of ui to (cid:22)j derivation only takes into account three instances of ui.

3. Greedy Algorithm ASA

In this section, the two main steps that compose the algorithm ASA are brieﬂy de-
scribed. First, an agglomerative greedy procedure is applied to A so as to derive a
covering. Next, a spitting greedy procedure reduces this covering in order to approach
the optimal solution of Equation (2).

3.1 Agglomerative Greedy Strategy

The greedy strategy builds a sub-optimal solution to the SCP Equation (2) in an iterative
way. At each iteration, the lowest cost sentence is chosen from A. If several sentences
correspond to the lowest cost, the one coming ﬁrst (i.e., the one with the lowest index)
is chosen. Initially, the set of selected sentences X is empty, the matrix ˜A associated with
the candidate sentences is assigned to A, the current covering capacity of sj is given by
˜(cid:22)j = (cid:22)j, and the current constraint vector ˜B = B. The cost of sentence sj is deﬁned by

{

(cid:27)j =

cj= ˜(cid:22)j if ˜(cid:22)j ̸= 0
∞ otherwise

(4)

Indeed, if ˜(cid:22)j = 0, it turns out that sj does not cover any unit missing in the solution X
under construction and its inﬁnite cost (cid:27)j avoids its selection.
At each iteration, the selected sentence s is added to X . Taking into account the
content of s, ˜B is updated to max
where the jth entry of ∆ equals 1 if
sj = s and 0 otherwise. Next, the associated column of s in ˜A is set to 0Rm. For each
sentence sj with a non-zero ˜(cid:22)j feature, ˜(cid:22)j is then updated using ˜A and ˜B in Equation (3).
At last, the agglomerative greedy algorithm is stopped as soon as all the constraints

˜B − ˜A∆, 0Rm

{

}

are satisﬁed, that is, ˜B = 0Rm.

3.2 Spitting Greedy Strategy

The spitting greedy strategy also consists in building iteratively a sub-optimal solution
Y to Equation (2) by reducing the size of a covering. The initial covering Y is set to the
solution X derived by the agglomerative phase described earlier. At each iteration, the
set of the redundant sentences of Y is calculated and the costliest one (according to
the cost function C) is removed from Y. An element s of Y is said to be redundant if for
each ui ∈ U, its number of instances into Y, denoted mi(Y ), and into Y \ {s}, denoted
mi(Y \ {s}), check min{mi(Y ), bi} = min{mi(Y \ {s}), bi}. In other words, s is a redun-
dant element of the covering Y if Y \ {s} is also a covering solution of Equation (1).
The spitting greedy algorithm stops when the redundant sentence set is empty.

4. Lagrangian Relaxation Based–Algorithm

This section describes the main phases of the algorithm called LamSCP. This algorithm
takes advantage of the Lagrangian relaxation properties reviewed herein in order to
approach the optimal solution of Equation (2) as close as possible. Strongly inspired by

360

Barbot et al.

Large Linguistic Corpus Reduction

Caprara, Fischetti, and Toth (1999), but generalized to the multi-representation prob-
lem, this algorithm provides a lower bound of the optimal solution cost. Having such
information is very useful for assessing the achievements of the SCP algorithms.

4.1 Lagrangian Relaxation Principles

Let us brieﬂy recall the main principles of Lagrangian relaxation on which LamSCP is
based to solve Equation (2) (see Fisher [1981] for more details on Lagrangian relaxation).

First, the Lagrangian function associated with Equation (2) is deﬁned by

L(X, Λ) = CX + ΛT(B − AX)

= ΛTB + C(Λ)X

where Λ ∈(
every covering X and every Λ ∈(

)m, X ∈ {0, 1}n, and C(Λ) = C − ΛTA. The coordinates of Λ =
)m, the Lagrangian function satisﬁes L(X, Λ) ≤ CX.

((cid:21)1, : : : , (cid:21)m)T are called Lagrangian multipliers and can be interpreted as a weighting
of constraints (1). The jth entry of C(Λ), called Lagrangian cost cj(Λ) of sentence sj, takes
into account its cost cj and the adequacy of its composition to address Equation (2). For

R+

(5)

R+

Thus, the dual Lagrangian function deﬁned by

L(Λ) = min
X2f0,1gn

L(X, Λ)

(6)

presents the following fundamental property: For every Λ ∈ Rm
+ and every covering
X, we have L(Λ) ≤ CX. Hence, L(Λ) is a lower bound of the minimal covering cost, CX
(cid:3)
,
but does not necessarily correspond to the cost of a covering. In order to compute L(Λ),
an acceptable solution for the vector X minimizing L(X, Λ) is X(Λ) = (x1(Λ), : : : , xn(Λ))T
where

{
1 if cj(Λ) < 0
0 if cj(Λ) > 0
∈ {0, 1} otherwise

xj(Λ) =

(7)

Additionally, the dual Lagrangian function and the Lagrangian costs inform about
the potential usefulness of sentences in the optimal covering. More precisely, for a given
Λ and a known upper bound UB of minimal covering cost, the gap g(Λ) = UB − L(Λ)
measures the relaxation quality. If cj(Λ) is strictly greater than g(Λ), we can check that
any covering containing sj has a cost value strictly greater than UB. Hence, sentence
sj is not selected and xj can be ﬁxed at zero. Similarly, if cj(Λ) < −g(Λ), any covering
with a cost lower than UB contains sj and one can ﬁx xj to 1. Therefore, an optimal
covering is made up of sentences with a low Lagrangian cost, as done in Caprara, Toth,
and Fischetti (2000) and Ceria, Nobili, and Sassano (1998), and the higher the relaxation
quality (i.e., the lower g(Λ)) is, the cheaper the covering will be.

The resolution of the dual problem of Equation (2) consists in ﬁnding Λ

(cid:3) ∈ Rm

+ that

maximizes the lower bound L(Λ), that is

(cid:3)
Λ

= arg max
(cid:3)2Rm

+

L(Λ)

(8)

361

Computational Linguistics

Volume 41, Number 3

Because this real variable function L is concave and piecewise afﬁne, a well-known
approach for ﬁnding a near-optimal multiplier vector is the subgradient algorithm.

4.2 The Three Phases

The LamSCP is an iterative algorithm, composed of several procedures that aim to
either improve the current best solution or reduce the combinatorial issue related to the
considered problem. In order to derive a good solution, the algorithm calls on a great
number of greedy procedures to solve sub-problems with the help of the Lagrangian
costs. As for the combinatorial reduction, the most frequently used heuristic consists
of downsizing the problem by mainly considering the sentences with low Lagrangian
costs.

The algorithm is organized around a main procedure called 3-phases. This pro-
cedure can single-handedly solve a multi-represented SCP. As its name suggests,
the 3-phases functioning consists in iterating a sequence of the three following sub-
procedures as shown in Figure 1:

r
r

r

(cid:3)

The subgradient phase calculates an estimation ˜Λ of Λ
that maximizes
the dual Lagrangian function. This procedure requires an upper bound
UB of the optimal covering cost. UB is initialized by a greedy algorithm
(rather than the cost of the whole corpus A). This phase is detailed in
Section 4.2.1.
The heuristic phase explores the neighborhood of ˜Λ by generating a
great number of Lagrangian vectors ˜Λp. A greedy-like procedure is
associated with each ˜Λp so as to compute a covering using the
Lagrangian cost vector C( ˜Λp). If, during this exploration, a less costly
covering than the best known one (corresponding to the cost UB)
is found, the upper bound UB is then updated to the cost of this
less costly solution. Similarly, if a better estimation of Λ
˜Λ is updated. This phase is described in Section 4.2.2.
The column ﬁxing phase selects a set F of sentences that are most
likely to belong to the optimal covering. This phase is detailed in
Section 4.2.3.

is obtained,

(cid:3)

Following the column ﬁxing phase, the constraint vector is updated and the un-
selected sentences deﬁne a set-covering sub-problem, called a residual problem. This
sub-problem is processed similarly, via an additional iteration of the three phases. This
iterative process is stopped when the residual problem is empty or when the associated
dual Lagrangian function indicates a cost is too high. Indeed, because this function
indicates a minimal cost for covering the sub-problem, its addition to the cost CF of the
sentences already retained in F gives a lower bound of the total cost of the solution
under construction, which should not rise beyond the cost UB of the best known
solution so as to be potentially more advantageous.

4.2.1 Subgradient Phase. In order to reach the quality goal, the subgradient phase pro-
vides a near-optimal solution ˜Λ of the dual Lagrangian problem (8) using a subgradient

362

Barbot et al.

Large Linguistic Corpus Reduction

Figure 1
LamSCP algorithm. The rectangle blocks are the steps that aim at improving solution quality
and the ellipses are those that try to reduce the size of the problem.

type algorithm. This iterative approach generates a sequence (Λp) using the following
updating formula (see Caprara, Fischetti, and Toth 1999)

{

}
g(Λp)
||S(Λp)||2 S(Λp), 0

Λp+1 = max

Λp + (cid:22)

(9)

where S(Λp) = B − AX(Λp) so as to take into account the multi-representation con-
straints. Parameter (cid:22) is adjusted to ﬁt the convergence fastness according to the method
proposed by Caprara, Fischetti, and Toth (1999).

At the ﬁrst call of 3-phases, Λ0 is arbitrarily deﬁned as follows: for each i ∈ M,

(cid:21)0
i = min
j∈N
̸=0
aij

cj
(cid:22)j

(10)

As for UB, its initial value is set to the cost of a covering previously calculated. In
order to evaluate how much the covering cost derived by ASA can be improved, we
have chosen to initialize UB by this value. At the following iterations of 3-phases, Λ0 is
given by a random perturbation (less than 10%) of the best known vector ˜Λ (of which
the entries of the sentences ﬁxed in the last column ﬁxing phase are removed) and UB
corresponds to the cost of the best covering found (after subtraction of the cost of the
sentences ﬁxed in the last column ﬁxing phase). In another approach proposed by Ceria,
Nobili, and Sassano (1998), UB corresponds to the upper bound of a dual Lagrangian
problem, and the subgradient procedure simultaneously estimates the upper bound and
the lower bound, generating two sequences of multipliers.

The subgradient phase also calls two procedures: pricing and spitting. Procedure
pricing aims to reduce the size of the search space. For each unit ui, the pricing selects
the 5bi smallest Lagrangian cost sentences covering ui. If this selection contains less than
5m sentences, where m is the maximal entry of B, it is completed by low Lagrangian cost
sentences (less than 0.1) to the limit of 5m sentences. The set of the chosen sentences is
denoted P and its design guarantees a sufﬁcient number of instances for each unit to
cover and a small variety in its composition. Actually, the subgradient method is applied
on P instead of A, and P is updated every 10 subgradient iterations.

363

Computational Linguistics

Volume 41, Number 3

Finally, at each iteration, deﬁnition (7) of X(Λp) and the large number of Lagrangian
costs close to zero allow a considerable number of vectors S(Λp). In order to get around
the computational difﬁculty of ﬁnding the steepest accent direction, Caprara, Fischetti,
and Toth (1999) propose a heuristic that, according to the experimental results, accel-
erates the convergence of the subgradient phase. This heuristic is implemented in the
spitting procedure. Called at each iteration, this procedure extracts from P the subset S
of sentences with a Lagrangian cost lower than 0.001. S is then reduced using a spitting
greedy algorithm to remove its redundant elements in decreasing Lagrangian cost order.
At last, for every j ∈ M, xj(Λp) = 1 if sj ∈ S and xj(Λp) = 0 otherwise. Thus, S(Λp) does
not necessarily correspond to a subgradient vector.

{

}

4.2.2 Heuristic Phase. The heuristic phase calculates a large number of coverings before
keeping the best one. To that end, a sequence of 150 multiplier vectors is generated by
perturbing ˜Λ using the formula ˜Λp+1 = max
where ˜Λ0 = ˜Λ and
(cid:22) is provided by the subgradient phase, so as to allow for a change in a large number of
˜Λp. With each ˜Λp, an agglomerative greedy algorithm followed by a spitting greedy one
are associated in order to calculate a covering.

˜Λp + (cid:22)g( ˜Λp)S( ˜Λp), 0

The agglomerative greedy chooses at each iteration the sentence sj with the lowest

cost (cid:27)j( ˜Λp) where

 cj( ˜Λp) ∗ ˜(cid:22)j if cj( ˜Λp) < 0 and ˜(cid:22)j > 0

if cj( ˜Λp) ≥ 0 and ˜(cid:22)j > 0
if ˜(cid:22)j = 0

cj( ˜Λp)= ˜(cid:22)j
∞

(cid:27)j( ˜Λp) =

(11)

This cost function provides an advantage to low Lagrangian cost sentences sj containing
˜(cid:22)j unit instances that could be helpful to the covering under construction. The agglom-
erative step uses several heuristics so as to reduce the search space. It is run within a
limited subset Pl of P, composed of the sentences sj with the lowest costs (cid:27)j( ˜Λk). At
each iteration, a sentence of Pl is selected and the cost of the sentences of P are updated.
If the maximum sentence cost in Pl becomes greater than the minimal cost in P \ Pl,
the working subset Pl is also updated. The deﬁnitions of P and Pl guarantee that the
agglomeration step provides a nonpartial solution of the considered SCP. This solution
is then reduced during the spitting step by iteratively removing its redundant sentences
sj with the highest costs cj.
(cid:3)
and its cost CX
(stored in UB) are kept as well as the highest value of L( ˜Λp) (found during the sub-
gradient or heuristic phases).

At the end of the heuristic phase, the best found covering X (cid:3)

4.2.3 Column Fixing Phase. The column ﬁxing phase aims to reduce the problem size
by choosing “promising” sentences among the ones with very low Lagrangian cost or
containing rare unit instances. The unselected sentences are less interesting for resolving
the SCP and the residual problem associated with these sentences should be the subject
of another call of the 3-phases.
More precisely, the column ﬁxing phase calculates the subset Q composed of sen-
tences sj with a negative Lagrangian cost cj( ˜Λ). Q is represented by the binary vector
Q = (q1, : : : , qn)T where qj = 1 if sj ∈ Q. For each ui ∈ U, the number of its instances
covered by Q is given by (AQ)i. If (AQ)i ≤ bi, then ui is considered as a rare unit
and all the elements of Q containing some instances of ui are ﬁxed in a set F. The
covering constraints that are not satisﬁed by F constitute a residual SCP. In order to

364

Barbot et al.

Large Linguistic Corpus Reduction

)

the max{(

complete F with a few sentences of the best known covering, a greedy-type algorithm
is run on X (cid:3) \ F to derive a solution of this residual SCP. From the obtained solution,
=20, 1} lowest Lagrangian cost sentences are also added in F. The
sentences that are “ﬁxed” in F during the column ﬁxing phase stay in F up to the end
of the 3-phases. After the column ﬁxing phase, the residual sub-problem is processed
by iterating the three phases and the next ﬁxed sentences are added to F.

BT1Rm

4.3 Reﬁning Procedure

The 3-phases procedure is encapsulated in an outer loop that permits the partial re-
consideration of the solution X (cid:3)
provided by this procedure. To that end, the reﬁning
procedure, proposed in Caprara, Fischetti, and Toth (1999), is used in order to select
elements of X (cid:3)
that contribute at least to the gap g( ˜Λ). In the case of the SCP with multi-
representation constraints, the deﬁnition of the contribution of sj ∈ X (cid:3)
can be adapted
as follows:

(cid:14)j = max{cj( ˜Λ), 0} +

˜(cid:21)i(AX

(cid:3) − B)i

aij
(AX(cid:3))i

(12)

∑

i∈M
aij>0

(cid:3) − B)i of the
The second term of Equation (12) consists of sharing the contribution ˜(cid:21)i(AX
excess instances of ui in X (cid:3)
according to the distribution of ui instances in X (cid:3)
. Therefore,
the reﬁning procedure ranks in an increasing order the elements sj of X (cid:3)
according to
their (cid:14)j value, and ﬁxes the ﬁrst elements in a set G until its given covering rate (cid:28)G
reaches (cid:25). (cid:28)G represents the rate of covering constraints satisﬁed by G and is deﬁned by

∑
i2M max{bi − (AG)i, 0}

BT1Rm

(cid:28)G = 1 −

(13)

where G denotes the binary vector corresponding to G.

4.4 The Overall LamSCP

The LamSCP is made up of the main procedures introduced in the previous sections
interlinked by the following steps. First, because of the adaptation of the algorithm to
the SCP with multi-representation constraints, the entries of matrix A are clipped to
the constraint vector in order to simplify the calculations such as the ones of (cid:22)1, : : : , (cid:22)n.
This threshold application implies that the excess instances of each unit ui are taken into
account in each sentence sj beyond the number bi, in the derivation of (cid:14)j.
After the initialization of Λ0 using Equation (10) and the upper bound UB, pro-
cedure 3-phases is called and provides a solution X to the complete SCP. The reﬁning
function ﬁxes a sentence subset G such that G covers a given rate (cid:25) of the covering
constraints. Parameter (cid:25) starts at a minimum value (cid:25)min = 0:3. The residual SCP is then
processed by 3-phases. The (cid:25) value grows at a rate of 20 percent whenever 3-phases
does not improve the solution to the complete SCP. If (cid:25) is greater or equal to 1, the
reﬁning procedure ﬁxes the whole best solution and the residual problem is then empty.
On the other hand, (cid:25) is set to (cid:25)min if a better solution is found in order to challenge
G and improve this solution. This iterative sequence composed of the 3-phases and
reﬁning procedures is carried out until the residual problem is not empty, the gap g( ˜Λ)
is positive, and the number of iterations has not reached 20.

365

Computational Linguistics

Volume 41, Number 3

5. Experiments

We propose a twofold comparison of the ASA and LamSCP algorithms: One part is fo-
cused on the covering cost for a large SCP, and the other on the stability of the solutions.
Moreover, in order to assess the ability and the behavior of both algorithms to process
different linguistic data, a ﬁrst set of experiments deals with phonological attributes
(mainly covering co-occurrences of phonemes) and a second set with grammatical
labels (mainly covering co-occurrences of POS labels).

Both attribute types, often involved in automatic linguistic processing, were chosen
because their distribution consists of few highly frequent events and numerous rare
events. On the one hand, for TTS tasks, the phonological type covering is a useful
preliminary step of the text corpus design before the recording step. In order to produce
the signal corresponding to a requested sentence, the unit selection engine requires at
least one instance of each phone (or 2-phone, depending on the concatenation process).
Because the recording and the post-recording annotation process are expensive tasks,
the recording length of such a corpus has to be as short as possible. On the other hand,
in order to train a domain-speciﬁc dependency parser, the covering of POS sequences
may be useful for increasing the diversity of syntax patterns. Because the dependency
annotation is a highly expensive task, the adaptation corpus to annotate needs to be
as small as possible, containing characteristic examples of the speciﬁc lexical variation
rather than following the natural distribution. One can expect that increasing its diver-
sity of POS sequences may lead to more diversity in the syntax trees.

Experiments on covering co-occurrences of phonemes are carried out on two large
phonologically annotated text corpora, and consist of covering at least k instances of
each phoneme, diphoneme until n-phoneme (i.e., triphoneme if n = 3, diphoneme if
n = 2). The cost cj of the sentence sj is given by its number of phones. From this point,
this kind of SCP is called a “k-covering of n-phonemes.”

A ﬁrst corpus, Gutenberg, is composed of texts in English, mainly extracted from
novels and short stories. This corpus is the production of the Gutenberg Project, pre-
sented by Hart (2003), and has been used by Kominek and Black (2003) to design the
speech corpus Arctic. A second corpus, in French, named Le-Monde, is extracted from
articles published in the newspaper Le Monde in 1997. Table 1 summarizes the main
features of both corpora.

The phonological annotation of the Gutenberg corpus comes from the Arctic/
Festvox database (see Kominek and Black 2003), and the annotation of the Le-Monde
corpus is a by-product of the Neologos project, detailed by Krstulovi´c et al. (2006).

Table 1
Statistics of the studied corpora.

Number of sentences
Number of phonemes
Corpus size (number of phones)
Number of diphonemes
Number of diphones
Number of triphonemes
Number of triphones
Sentence length mean (phones) & Std. Dev.

Le-Monde

Gutenberg

172,168
35
16,668,609
1,172
16,496,441
26,443
16,324,273
96.81 (60.46)

53,996
57
1,539,735
1,955
1,485,739
27,477
1,431,743
28.51 (10.52)

366

Barbot et al.

Large Linguistic Corpus Reduction

Table 2
Statistics of 1-POS and 2-POS occurrences, corpus Le-Monde. On average, a sentence contains
27:64 POS with a standard deviation of 16:45.

Number of distinct units Number of occurrences

1-POS
2-POS

141
6,716

4,587,320
4,421,371

For each corpus, we have collected every phoneme, diphoneme, triphoneme, and
their occurrences in each sentence so as to deﬁne the set U of units to cover and the
matrix A. A is built by collecting one sentence after the other following the ordering
inside the corpus, and one unit after the other inside the sentences. After this matrix
translation, we obtain two description ﬁles and two index ﬁles. The ﬁrst ﬁle describes
the matrix A and the second one the cost vector C. Because of the low matrix density, we
have chosen a sparse representation to save space and computation time: For instance,
the 2-phoneme Gutenberg matrix is about 2.2% dense. We only store the cells of A that
have a non-zero value so as to get a sparse matrix. The index ﬁles are made for the
correspondence between the general covering problem and the application domain.
The implementation is made in C. In terms of software engineering, our algorithms
are working on an SCP that does not depend on the application data. For example,
there is no information on what types of units are to be covered. The algorithms only
have the matrix of occurrences A, the cost vector C, and the constraint vector B. A set
of translation ﬁles (from application data to SCP and from SCP to application data) is
built before each computation. As a consequence, there is no difﬁculty in addressing a
different set of features to cover on the same or on a different corpus.

To study the achievements of ASA and LamSCP on different types of data, we
have also chosen to address the “k-covering of n-POS” on the corpus Le-Monde. The
grammatical and syntactical analyses are processed by the Synapse development
analyzer presented in Synapse (2011). In order to consider a SCP with a substantial
number of required units, a very detailed level of POS tagging has been selected,
providing 141 distinct tags after analyzing Le-Monde. For example, this level provides
tags like “Determiner male singular Article” or “Noun female singular,” whereas the
simplest level gives “Determiner” or “Noun.” This latter level of description would
have given only nine different POS tags after analyzing Le-Monde. The main associated
statistics are given in Table 2. For these experiments of POS covering, the cost of a
sentence is deﬁned as its number of POS occurrences.

We used a PC with 2 CPUs (E5320/1.86GHz/4 cores/64bits) and 32 GB RAM for the
phonological coverings and the POS coverings were computed using a PC with 8 CPUs
(Intel Xeon X7550/2.00Ghz/8 cores/64bits) and 128 GB RAM. Our implementations do
not take advantage of any parallelism.

The following sections detail more precisely the different experiments conducted

on French or English.

5.1 Performance of the Algorithms for 1-Covering of 2-Phonemes in French

The aim of Experiment 1 is the assessment of the achievements of both algorithms, ASA
and LamSCP, and the robustness of the results when the sentence ordering is modiﬁed

367

Computational Linguistics

Volume 41, Number 3

in the corpus to reduce. Indeed, one of the difﬁculties of the greedy methodology is that
the score function has discrete values and several sentences can yield the same score. In
our implementation, among the sentences showing the best current score, the ﬁrst one
encountered is chosen. We would like to measure the inﬂuence of this random choice on
the stability of the results. LamSCP uses greedy strategies based on Lagrangian costs.
Because the Lagrangian costs cj(Λ) take the SCP in its entirety into account and are
continuous real-value functions of Λ, they would be more selective than the sentence
costs used by ASA. A simple solution for evaluating the stability consists of proceeding
with an important amount of experiments on the same SCP by randomly modifying
the sentence ranking in A. Experiment 1 measures the impact of these permutations
on the solutions computed by both algorithms. The considered SCP is the 1-covering
of 2-phonemes on the corpus Le-Monde. Considering the computation time (more than
5 hours for LamSCP), only 47 instances of the SCP are considered, each instance corre-
sponding to a random sentence ordering in Le-Monde. The 95% conﬁdence intervals are
derived using the bootstrap method, concerning the covering cost, the number and the
length of sentences in coverings, the computation time, and the “distance” between the
covering costs and the associated lower bound L( ˜Λ).

5.2 Stability of the Algorithms for the k-Covering of 2-Phonemes in English

One of the goals of Experiment 2 is to compare the achievements of both algorithms on
corpus Gutenberg, which has different features from the ones of Le-Monde. The sentences
in Gutenberg are shorter on average and the associated variation of sentence length is
lower. Furthermore, Gutenberg is 10 times smaller than Le-Monde.

In order to compare with the results of the previous experiment done on Le-Monde,
we ﬁrst observed a 1-covering of 2-phonemes on the Gutenberg corpus. The search
space seems smaller than in Experiment 1. According to Table 1, Le-Monde is com-
posed of more sentences than Gutenberg. Le-Monde contains 33,165,050 occurrences of
2-phonemes and Gutenberg only 3,025,474. Moreover, the number of attributes to cover
is lower: 1,207 2-phonemes in Le-Monde and 2,012 in Gutenberg. We can also notice that
ﬁve 2-phonemes have only one occurrence in Le-Monde and that the total cost of the
ﬁve sentences covering these rare units is 751 phones, whereas this is the case for 109
2-phonemes in Gutenberg and the total cost of the 104 concerned sentences is 3,606
phones. Finally, if we consider the density of matrix A, 8.4% of the cells are non-empty
for Experiment 1 and 2.2% for Experiment 2. As with Experiment 1, the sentence order-
ing in Gutenberg has been randomly modiﬁed to produce 60 instances of the SCP and
similar solution statistics have been computed for both algorithms.

The second objective is to test and compare the ability of two algorithms to deal
with the constraints of multi-representation. For this, we apply the same methodology
to the k-covering of 2-phonemes in Gutenberg, for k from 2 to 5. We note that for the same
original corpus to reduce, the size of the search space decreases when k increases. These
different SCPs enable us to compare the performance of two algorithms depending on
the size of the search space.

5.3 1-Covering of 3-Phonemes in English

In Experiment 3, the aim is to observe the behavior of both algorithms on very con-
strained problems. For this, we study their ability to treat a covering of 3-phonemes. We
try to assess the impact on the solution features and on the stability of such an increase
in the number of attributes to cover with many rare events. So as to compute statistics

368

Barbot et al.

Large Linguistic Corpus Reduction

on the 1-covering of 3-phonemes, an instance of Gutenberg has been proposed to ASA
and to LamSCP. This instance counts 29,489 units to cover and the density of matrix
A is 0.24%. The computation time is nearly 5 days for LamSCP and we then chose
to carry out 35 instances of the 1-covering of 3-phonemes on Gutenberg. Additionally,
it is interesting to compare these results with those of Experiment 2 concerning the
1-covering of 2-phonemes on the same corpus, which corresponds to a larger search
space.

5.4 1-Covering of 3-Phonemes in French

In order to pursue the objective set out in the description of Experiment 3, that is, the
ability of algorithms to treat with numerous constraints and a heavy-tail distribution
of units, Experiment 4 consisted of testing both algorithms on the 1-covering of 3-
phonemes on Le-Monde. The search space seems larger than in the previous experiment.
Let us recall that Le-Monde contains 3.18 times more sentences than Gutenberg and it
counts 27,650 units to cover. Furthermore, Gutenberg contains 5,000 3-phonemes with
only one occurrence, which requires the selection of 4,180 sentences with a total length
equal to 137,714 phones, whereas Le-Monde contains 2,274 rare 3-phonemes scattered
in 2,107 sentences measuring a total of 283,208 phones. The associated matrix density
is 0.69%. Because the computation of a ﬁrst instance takes more than 8 days, we have
limited the number of instances to 30 for this SCP.

5.5 k-Covering of 1-POS and 2-POS in French

The main goal of Experiment 5 is to study the behavior of both algorithms, ASA and
LamSCP, dealing with another kind of linguistic attribute, and to compare this with
the previous experiments. To achieve this goal, we consider POS attributes and the
associated SCP: 1- and 5-coverings of POS, 1- and 5-coverings of 2-POS deﬁned on
Le-Monde. Indeed, we can observe in Table 2 that the global statistics of POS tags
in Le-Monde are quite different from their phonological counterparts summarized in
Table 1. In particular, the density of matrix A is 11.03% for a 1-POS covering and 0.57%
for a 2-POS covering. Also, the search space size seems to decrease when considering
successively the mono- and the multi-coverings of 1-POS, and the mono- and the multi-
coverings of 2-POS, permitting us to compare them with the results coming from the
experiments on phonological coverings. We evaluate the stability by computing 50
randomly mixed versions of the corpus Le-Monde.

6. Results and Discussion

In this section, the results of the experiments described in Section 5 are provided and
discussed. As a consequence, the organization of this section and Section 5 are similar.

6.1 Performance of Algorithms for 1-Covering of 2-Phonemes in French

Table 3 shows the main results of Experiment 1, concerning the 1-covering of 2-
phonemes from the corpus Le-Monde. Symbol ± indicates that the mentioned value
corresponds to a 95% conﬁdence interval, calculated using the bootstrap method from

369

Computational Linguistics

Volume 41, Number 3

Table 3
Statistics of the solutions of 1-covering of 2-phonemes computed by ASA and LamSCP from
Le-Monde. The last column represents the best lower bounds found by LamSCP.

Experiment 1: 1-covering of 2-phonemes, corpus Le-Monde

Covering size (phones)
Reduction rate relative to ASA (%)
Covering size [min; max]
Covering size Std. Dev.
Sentence number
Sentence length
Time CPU (seconds)

ASA

8,555 ± 22
[8,447; 8,669]
57.40
335.73 ± 1.69
25.48 ± 0.10

51 ± 0

LamSCP
7,786 ± 4
–9.00 ± 0.20
[7,767; 7,829]
13.01
268.76 ± 1.08
28.97 ± 0.12
20,478 ± 508

L( ˜Λ)
7,689 ± 5
–10.13 ± 0.19
[7,649; 7,715]
13.83
–
–
–

For one instance of the SCP, let CX

the 47 instances of the SCP. In order to cover each of the 1,207 2-phonemes of Le-Monde,
ASA drastically reduces the size of the initial corpus by 99.94% (±0.00). However, on
average, LamSCP calculates a 9.00% shorter covering. The lower bound L( ˜Λ) for the
optimal covering cost is 7,689 ± 5 phones. L( ˜Λ) is not a minimum value and may
not correspond to the cost of a real covering. Because this lower bound is updated all
along the execution of LamSCP, we do not mention a speciﬁc calculation time for this
result.
(cid:3)
ASA be the size of the solution given by ASA. The
quantity (cid:28)ASA = 1 − L( ˜Λ)=CX
(cid:3)
ASA indicates that the optimum solution to SCP is at most
(cid:28)ASA times shorter than the covering calculated by ASA. It can be observed that the
optimal solution is at most 10.13% (±0.19) shorter than the one yielded by ASA and at
most 1.24% (±0.08) shorter than the solution yielded by LamSCP. The solutions obtained
by LamSCP and the optimal solution to the SCP are therefore very close. Considered
among the 47 instances of the SCP the best solutions yielded by ASA (8,447 phones)
and LamSCP (7,767 phones), LamSCP is 8.75% better than ASA in terms of covering
costs, while the best lower bound for the SCP is 7,715 phones, only 0.67% (respectively,
8.66%) shorter than the best covering by LamSCP (respectively, ASA).

The average length of the sentences selected by both algorithms is far below the
average length of the sentences in the corpus (96.81 phones). LamSCP tends to choose
sentences that are slightly longer than ASA, with an average 28.97 (±0.12) phones
compared with 25.48 (±0.10) phones. Moreover, ASA selects on average 335.73 (±1.69)
sentences per solution, about 24.91% more than LamSCP, which selects 268.76 (±1.08)
sentences on average. This seems to indicate that LamSCP makes fewer local choices
than ASA. This hypothesis can also be validated through the analysis of the variability
of the results. The relative variation of the covering costs calculated by LamSCP is
13.01/7,786 = 0.16%, and 57.40/8,555 = 0.67% by ASA; that is to say a stability of the
costs 4 times greater for the solutions yielded by LamSCP than for ASA. Moreover, the
solutions are composed of a very stable number of sentences: The associated relative
standard deviation is 5.31/335.73 = 1.58% for the 47 instances solved by ASA, and
3.85/268.76 = 1.43% for the instances solved by LamSCP. It turns out that the results
of both algorithms are very stable when the order of the sentences is modiﬁed in the
original corpus.
Finally, concerning computation time, the resolution of an instance of the SCP
lasts on average 5 hr 41 min 18 sec (±8 min 28 sec) for LamSCP versus 51 sec

370

Barbot et al.

Large Linguistic Corpus Reduction

(±0 sec) for ASA. On average over the 47 instances, LamSCP takes 390 (±9) times as
long as ASA.

6.2 Stability of the Algorithms for k-Covering of 2-Phonemes in English

The considered SCP consists of covering at least k times each of the 2,012 2-phonemes of
the Gutenberg corpus, with k varying from 1 to 5. The results are summarized in Table 4.
For all instances of these SCPs, it has been observed that LamSCP computes shorter
coverings than ASA. However, that advantage diminishes as k grows: The cost advan-
tage offered by LamSCP compared with ASA decreases from 9.73% (±0.13) for k = 1
to 4.50% (±0.04) for k = 5. Also, the solutions obtained from ASA and LamSCP seem
to get closer to the optimal solution as k rises. The corresponding ﬁgures are presented
in Table 5: For instance, the optimal solution is at most 0.75% (±0.02) shorter than that
obtained by LamSCP for k = 1, and 0.27% (±0.00) for k = 5.

Table 4
Experiment 2: Statistics based on 60 instances of a k-covering of 2-phonemes from Gutenberg.
L( ˜Λ)
13,357 ± 2
–10.41 ± 0.12
[13,341; 13,378]

ASA

k = 1

Covering size (phones)
Reduction rate relative to ASA (%)
Covering size [min; max]
Sentence number
Sentence length
Time CPU (seconds)

k = 2

k = 3

k = 4

k = 5

Covering size (phones)
Reduction rate relative to ASA (%)
Covering size [min; max]
Sentence number
Sentence length
Time CPU (seconds)

Covering size (phones)
Reduction rate relative to ASA (%)
Covering size [min; max]
Sentence number
Sentence length
Time CPU (seconds)

Covering size (phones)
Reduction rate relative to ASA (%)
Covering size [min; max]
Sentence number
Sentence length
Time CPU (seconds)

Covering size (phones)
Reduction rate relative to ASA (%)
Covering size [min; max]
Sentence number
Sentence length
Time CPU (seconds)

14,909 ± 23
[14,700; 15,107]
623.75 ± 1.61
23.90 ± 0.04

8 ± 0
27,518 ± 25
[27,278; 27,727]
1080.19 ± 1.60
25.48 ± 0.02

14 ± 0
39,319 ± 34
[39,091; 39,580]
1,507.75 ± 1.75
26.07 ± 0.01

20 ± 0
50,491 ± 28
[50,300; 50,753]
1,908.10 ± 1.73
26.46 ± 0.01

28 ± 0
61,375 ± 30
[61,137; 61,683]
2,288.91 ± 1.88
26.81 ± 0.01

39 ± 0

LamSCP
13,458 ± 2
–9.73 ± 0.13
[13,441; 13,485]
520.85 ± 0.57
25.83 ± 0.02
2,711 ± 68
25,604 ± 4
–6.94 ± 0.09
[25,576; 25,642]
948.30 ± 0.68
26.99 ± 0.01
3,931 ± 68
36,985 ± 4
–5.92 ± 0.07
[36,946; 37,028]
1,359.28 ± 1.13
27.20 ± 0.02
5,974 ± 114
48,023 ± 4
–4.89 ± 0.06
[47,987; 48,075]
1,744.40 ± 1.35
27.53 ± 0.01
6,589 ± 239
58,610 ± 4
–4.50 ± 0.04
[58,582; 58,645]
2,101.62 ± 0.84
27.88 ± 0.01
7,599 ± 344

25,413 ± 1
–7.65 ± 0.08
[25,400; 25,430]

36,746 ± 2
–6.53 ± 0.07
[36,724; 36,769]

47,820 ± 4
–5.29 ± 0.05
[47,780; 47,842]

58,447 ± 2
–4.76 ± 0.04
[58,420; 58,465]

–
–
–

–
–
–

–
–
–

–
–
–

–
–
–

371

Computational Linguistics

Volume 41, Number 3

Table 5
Ratios (cid:28)LamSCP and (cid:28)ASA for the k-covering of 2-phonemes from Gutenberg.
k

2

1

3

4

5

(cid:28)LamSCP

(cid:28)ASA

0.75%
(± 0.02)
10.41%
(± 0.12)

0.74%
(± 0.01)
7.65%
(± 0.08)

0.64%
(± 0.01)
6.53%
(± 0.07)

0.42%
(± 0.01)
5.29%
(± 0.05)

0.27%
(± 0.00)
4.76%
(± 0.04)

Because the search area diminishes as k increases, it may be observed that the
algorithms tend to be more stable. This is true both for the size of the solutions, as well as
for the number of sentences that deﬁne them. Table 6 represents the variation of the size
of the solutions as a function of k. This variation is calculated as follows: For a given
k number and a given algorithm, the standard deviation of the size of the k-covering
computed by that algorithm is divided by the average size of these coverings. Thus, it
can be noted that LamSCP offers a stability 4 to 8 times superior to ASA concerning the
size of the coverings. As for the number of sentences, the relative standard deviation
similarly decreases from 0.97% to 0.28% when k increases from 1 to 5 for ASA solutions,
and from 0.42% to 0.15% for LamSCP ones.

One can note that the increase of the minimal number k of instances of each unit
to cover leads to a selection, by LamSCP and ASA, of longer sentences on average. The
average length of the sentences picked for a 1-covering was quite low. As the constraints
increase along with k, it only seems natural that the algorithms tend to select longer
sentences, as shorter sentences no longer contain enough occurrences of 2-phonemes.
Moreover, as described in Section 2, when the minimal number bi of a unit ui demanded
in the covering exceeds the number of instances of that unit in the initial corpus, all
sentences containing instances of ui in the initial corpus are selected, and bi is set to
(A1Rn )i. Thus, as k increases, the algorithm tends to select more and more sentences,
and their length tends towards the average value over the whole corpus, which is
28.51 phones for Gutenberg.

As for computation time, although it increases as k grows, because of the increasing
number of constraints to update, the ratio between the computation time of LamSCP
and ASA tends to diminish, as shown in Table 7. This tendency may ﬁnd an explana-
tion in the fact that the search space diminishes as k increases, which causes a lesser
number of selected sentences to be questioned during the 3-phase iteration of LamSCP.
Also, we notice that the average computation time of the two algorithms is greater
in Experiment 1, owing to a greater number of sentences in corpus Le-Monde and a
higher density of matrix A. Moreover, the ratio between the computation times of

Table 6
Relative standard deviation of solution cost for both algorithms of a k-covering of 2-phonemes
from Gutenberg.

k

1

2

3

4

5

LamSCP
ASA

0.07% 0.05% 0.05% 0.04% 0.02%
0.57% 0.37% 0.29% 0.18% 0.17%

372

Barbot et al.

Large Linguistic Corpus Reduction

Table 7
Computation time ratio between LamSCP and ASA for a k-covering of 2-phonemes from
Gutenberg.

k
Time LamSCP / Time ASA 333 (± 7)

1

2

280 (± 5)

3

292 (± 6)

4

218 (± 9)

5

194 (± 8)

LamSCP and ASA decreases between Experiments 1 and 2, going from 390 (±9) to
333 (±7). Again, this can be explained by the diminishing of the search space.
For k = 1, the advantage offered by LamSCP on the covering costs compared with
ASA is slightly higher than that observed in Experiment 1: 9.73% (±0.13) in this case,
versus 9.00% (±0.20) in the previous experiment. This seems to contradict the idea that
the performance of LamSCP improves as the search area becomes wider. However,
the distributions of the units to cover in Gutenberg and Le-Monde are different, and the
variation on the length of the sentences in Le-Monde is very high, which may account
for this slight difference in terms of gain. Note that the size of the calculated coverings
and the lower value L( ˜Λ) are closer in the experiment carried out on Gutenberg. It is
difﬁcult, however, to perform further comparisons with Experiment 1 regarding the
“distance“ between the costs of the solutions computed by these algorithms, and the
optimal covering cost, given that the quality of the lower bound cannot be evaluated.
The gain in stability offered by LamSCP, both for the costs of the solutions or the
number of sentences, is more important than that noticed during the previous exper-
iment. We think that the increase is due to a more restricted search space, and less
variability of the length of the sentences in corpus Gutenberg, which may be observed in
Table 1.

6.3 1-Covering of 3-Phonemes in English

Table 8 sums up the main results of Experiment 3, where 35 instances of 1-covering
of 3-phonemes from Gutenberg were processed. According to the L( ˜Λ) values, covering
all 3-phonemes requires a solution size greater than or equal to 226,635 phones. On
average, the solution measures 227,360 ± 12 phones using LamSCP, and 236,828 ± 94

Table 8
Experiment 3: Statistics based on 35 experiments of a 1-covering of 3-phonemes from Gutenberg.

Covering size (phones)
Reduction rate relative to ASA (%)
Covering size [min; max]
Covering size Std. Dev.
Sentence number
Sentence length
Time CPU (seconds)

ASA

236,828 ± 94
[236,075; 237,615]
301.75
8,005.20 ± 5.02
29.88 ± 0.00
2,834 ± 20

LamSCP
227,360 ± 12
–3.99 ± 0.04
[227,317; 227,425]
33.12
7,606.77 ± 2.14
29.58 ± 0.01
371,501 ± 10

L( ˜Λ)
226,559 ± 8
–4.33 ± 0.04
[226,518; 226,635]

23.67

–
–
–

373

Computational Linguistics

Volume 41, Number 3

phones using ASA. The optimal covering is at most 0.35% (±0.00) shorter than solutions
derived by LamSCP and 4.33% (±0.04) shorter than the ones derived by ASA. We can
then observe that both algorithms manage to compute solutions with close sizes when
scaling up the required attribute set. The solutions are very stable, even more than
in Experiment 2: The relative variation of their size is 0.12% for ASA and 0.01% for
LamSCP; the relative variation of their sentence number is 0.17% for ASA and 0.07%
for LamSCP. This increase of stability is due to a smaller search space and the increase
of the number of rare units required, which also compels the algorithms to select a
higher number of inevitable sentences for all the instances of the SCP. Furthermore,
the decrease of the ratio between the computation time of LamSCP and ASA from 332
for the 1-covering of 2-phonemes to 130 for the one of 3-phonemes on Gutenberg may
conﬁrm this idea, which has also been put forward in Experiment 2.

Concerning the length of the selected sentences by both algorithms, it is greater
than the one for the 5-covering of 2-phonemes, observed in Experiment 2, and slightly
deviates from the average sentence length for the whole corpus. Consequently, it turns
out that covering longer and generally rarer units involves a selection of longer sen-
tences. This is conﬁrmed by the fact that the sentences of Gutenberg covering units with
a single occurrence in Gutenberg represent more than half the size of the solutions and
are composed of 33 phones on average.

6.4 1-Covering of 3-Phonemes in French

In this section, we analyze the results of Experiment 4, the 30 instances of the 1-
covering of 3-phonemes from Le-Monde carried out by ASA and LamSCP. The results
are given in Table 9. First, although the main features of Le-Monde and Gutenberg are
different, notice that the closeness between the size of coverings calculated by both
algorithms and the lower bound L( ˜Λ) is comparable to the one observed in Experiment
3. Indeed, the optimal covering size is at most 0.48% (±0:03) and 4.35% (±0:05) shorter
than the solution size derived by LamSCP and ASA, respectively. Similarly, the size
of solutions and the number of selected sentences are as stable as those observed in
the previous experiment: The solution length varies from 0.01% for LamSCP to 0.10%
for ASA and the number of sentences ﬂuctuates about 0.16% for ASA and 0.10% for
LamSCP.

As for the comparison with the results of Experiment 1 (1-covering of 2-phonemes
from Le-Monde), the main trends are similar to the ones observed for the transition
from the 1-covering of 2-phonemes to the 1-covering of 3-phonemes from Gutenberg.

Table 9
Experiment 4: Statistics based on 30 experiments of a 1-covering of 3-phonemes from Le-Monde.

Covering size (phones)
Reduction rate relative to ASA (%)
Covering size [min; max]
Covering size Std. Dev.
Sentence number
Sentence length
Time CPU (seconds)

ASA

620,434 ± 222
[619,319; 622,201]
633.03
6,969.10 ± 4.63
89.02 ± 0.03
7,845 ± 78

LamSCP
596,323 ± 27
–3.89 ± 0.03
[596,190; 596,486]
88.59
6,436.76 ± 2.56
92.64 ± 0.03
660,928 ± 24,355

374

L( ˜Λ)

593,417 ± 127
–4.35 ± 0.04
[592,640; 594,250]
391.82

–
–
–

Barbot et al.

Large Linguistic Corpus Reduction

in Experiment 4,

However,
the average selected sentence length has markedly
increased, approaching the mean value on the whole corpus: 89.02 (±0:03) for ASA
and 92.64 (±0:03) for LamSCP, whereas in Experiment 1 these values are, respectively,
25.48 (±0:10) and 28.97 (±0:12). We have already observed in Experiment 3 that
covering longer units increases the length of selected sentences but this high amplitude
seems to be inherent to the design of corpus Le-Monde. Furthermore, notice that the
1-coverings of 2-phonemes from Le-Monde are almost half as small as the ones from
Gutenberg, whereas the 1-coverings of 3-phonemes from Le-Monde are between twice
and three times longer than the ones from Gutenberg. This is due to the fact that the
3-phonemes with a single instance in Le-Monde are very scattered in long sentences
(their length mean is about 134 phones), and these indispensable sentences represent
nearly half the size of the solutions. The other sentences of the solutions are around
70 phones long. Lastly, the ratio between the computation time of both algorithms is
about 84, which is smaller than the ratios previously observed, but this SCP is the most
time consuming: 2 hr 10 min for ASA and more than 7 days for LamSCP.

6.5 k-Covering of 1-POS and 2-POS in French

Table 10 sums up the main results of Experiment 5, dealing with the 1- and 5-coverings
of 1-POS and 2-POS. For all these SCP, LamSCP produces smaller coverings, com-
posed of longer sentences, than the coverings obtained with ASA. When the search
space diminishes, the relative “distance” between the size of solutions provided by
both algorithms decreases, as well as between the lower bound L( ˜Λ) and the size of
solutions obtained by ASA. These trends were also observed in the earlier experiments.
In particular, as for the 1-covering of 1-POS, not only does LamSCP provide 10.06%
(± 0.00) shorter solutions than ASA, but its solutions are optimal for all 50 instances
of this SCP. Indeed, the lower bound value varies from 482.51 to 482.87 occurrences of
1-POS while all the solutions given by LamSCP are made of exactly 483 occurrences
of POS. For the other k-coverings of n-POS, the optimal solution is at worst 0.39%
(±0.02) shorter than the covering given by LamSCP for (k, n) = (5, 1), 0.11% (±0.00) for
(k, n) = (1, 2), and 0.22% (±0.00) for (k, n) = (5, 2). The solutions obtained by ASA or by
LamSCP are very stable. For example, the relative standard deviation of number of POS
in a covering solution varies from 0.00% to 1.23% for ASA, and from 0.00% to 0.04%
for LamSCP.

As previously observed for both algorithms, their computation times grow when
the number of required covering features increases. However, the ratio between the
computation time of LamSCP and ASA does not behave as in Experiment 2 (see Table 7):
For the k-covering of 1-POS, this ratio increases from 290 (±11) to 657 (± 43) when k goes
from 1 to 5, and for the k-covering of 2-POS, it increases from 75 (±5) to 108 (±6).

7. Evaluation on a Text-to-Speech Synthesis System

In the previous sections, different algorithms dealing with corpus reduction were in-
troduced and studied. The proposed experiments mainly evaluate the effects of these
algorithms in terms of corpus reduction but not according to a practical task. This
section proposes an experiment to assess the impact of the corpus reduction on a unit
selection speech synthesis system.

As explained in Section 1, a corpus reduction for a TTS system is a trade-off between
minimizing the recording and post-processing time to build the speech corpus and

375

Computational Linguistics

Volume 41, Number 3

Table 10
Statistics of 50 instances of a k-covering of n-POS from Le-Monde.

Experiment 5: k-covering of n-POS, corpus Le-Monde

n = 1 and k = 1

Covering size (POS)
Reduction rate relative to ASA (%)
Covering size [min; max]
Covering size Std. Dev.
Sentence number
Sentence length
Time CPU (seconds)

Covering size (POS)
Reduction rate relative to ASA (%)
Covering size [min; max]
Covering size Std. Dev.
Sentence number
Sentence length
Time CPU (seconds)

Covering size (POS)
Reduction rate relative to ASA (%)
Covering size [min; max]
Covering size Std. Dev.
Sentence number
Sentence length
Time CPU (seconds)

Covering size (POS)
Reduction rate relative to ASA (%)
Covering size [min; max]
Covering size Std. Dev.
Sentence number
Sentence length
Time CPU (seconds)

ASA

537.70 ± 1.78
[524; 552]
6.64
72.07 ± 0.51
7.46 ± 0.04

2 ± 0

n = 1 and k = 5
2,659 ± 4
[2,628; 2,682]
12.42
285.00 ± 0.82
9.33 ± 0.02

5 ± 0

n = 2 and k = 1
77,022 ± 18
[76,913; 77,187]
63.33
3,288.22 ± 1.48
23.42 ± 0.00
175 ± 0
n = 2 and k = 5
281,532 ± 17
[281,362; 281,652]
72.09
10,379.67 ± 1.67
27.12 ± 0.00
2,863 ± 113

LamSCP
483.00 ± 0.00
–10.06 ± 0.00
[483; 483]
60.65 ± 0.31
7.97 ± 0.03
735 ± 25

0

2,502 ± 0
–5.90 ± 0.11
[2,501; 2,505]
1.11
238.91 ± 0.65
10.47 ± 0.02
2,712 ± 162

75,281 ± 1
–2.25 ± 0.02
[75,273; 75,297]
3.44
3,120.89 ± 0.91
24.12 ± 0.00
13,095 ± 881

278,114 ± 5
–1.21 ± 0.00
[278,084; 278,152]
15.27
10,034.96 ± 2.66
27.71 ± 0.00
309,095 ± 901

L( ˜Λ)

482.68 ± 0.02
–10.17 ± 0.00
[482.51; 482.87]
0.08

–
–
–

2,492 ± 0
–6.27 ± 0.09
[2,482; 2,493]
1.85

–
–
–

75,192 ± 2
–2.37 ± 0.02
[75,176; 75,220]
8.26

–
–
–

277,497 ± 4
–1.43 ± 0.00
[277,459; 277,524]
14.33

–
–
–

keeping the highest phonological richness of the corpus to ensure the quality of the
synthetic speech. The goal of this experiment is to measure this trade-off by evalu-
ating the quality of the same TTS system fed with different speech corpora uttered
by the same speaker. Note that the intrinsic quality of this system is not the purpose
here.

Firstly, a brief presentation of a state-of-the-art unit selection–based TTS system
is proposed in Section 7.1. The linguistic parameters used by the TTS system are de-
tailed because they are linked to the required features in the reduction stage. In Sec-
tion 7.2, corpora used in the experiment are introduced. The attributes to cover and the

376

Barbot et al.

Large Linguistic Corpus Reduction

methodology of evaluation are described in Section 7.3; the results are given and dis-
cussed in Section 7.4.

7.1 Text-to-Speech System

For this experiment, a state-of-the-art unit selection–based TTS system is used to pro-
duce an acoustic signal from an input text. A linguistic front end processes the text to
extract features taken into account by the algorithm that selects segments in a speech
corpus (see Bo¨effard and d’Alessandro 2012).

The input text is converted into a sequence of phonemes using a French phonetizer
proposed by B´echet (2001). Non-speech sound labels can be added to this sequence
(silences, breaths, para-verbal events, etc.). A vector of features is deﬁned as follows:

1.
2.
3.
4.
5.
6.
7.

The phone or non-speech sound label
Is the described segment a non-speech sound?
Is the phone in the onset of the syllable?
Is the phone in the coda of the syllable?
Is the phone in the last syllable of its syntagm?
Is the current syllable at the end of a word?
Is the current syllable at the beginning of a word?

Extraction of features is done using the ROOTS toolkit described in Bo¨effard et al.
(2012). The unit selection process aims to associate a signal segment from the speech
corpus to each vector of features computed from the input text. This is performed
in two steps. In the ﬁrst step, for each unit, a set of candidates that match the same
features are extracted from the speech corpus. In the second step, given all candidates,
the best path is searched using an optimization algorithm so as to produce the sequence
of speech units. The algorithm tries to minimize three sub-costs, commonly used in unit
selection based TTS systems, which are spectral discrepancies based on MFCC distance,
amplitude, and f0 distances.

7.2 Corpora

Two corpora are used in this experiment. The ﬁrst one, Learning corpus, is an annotated
acoustic corpus used to provide speech data for the TTS engine. It is an expressive
corpus in French, spoken by a male speaker reading Albertine disparue, an excerpt from `A
la recherche du temps perdu by Marcel Proust. The corpus is composed of 3,138 sentences
automatically annotated using a process described in Bo¨effard et al. (2012). The overall
length of the speech corpus is 9 hr 57 min. When creating a voice for a unit selection–
based TTS system, long sentences are generally removed or split into syntagm groups
in order to help the speaker.

A second corpus, named Test corpus, is a text corpus that is synthesized and used
in the listening experiment. It is composed of 30 short sentences randomly extracted
from a phonetically balanced corpus in French, proposed by Combescure (1981). The
use of a corpus with a different linguistic style minimizes the bias introduced by the
learning corpus.

Statistics are given in Table 11.

377

Computational Linguistics

Volume 41, Number 3

Table 11
Characteristics of the two corpora.

Learning corpus

Number of syntagms
Number of labels
Corpus size (labels)
Number of 2-phoneme
Number of diphones
Syntagm length mean (phones) & Std. Dev.

19,587
36
392,865
1,033
373,278
20.1 (11.9)

Test corpus
30
36
813
317
787
27.1 (5.6)

7.3 Methodology

For this experiment, two reduced corpora are evaluated. They are built by reducing
the full learning corpus using the two different algorithms presented in the previous
sections: ASA and LamSCP.

As described in Section 7.1, the unit selection process of the speech synthesis system
is based on a set of phonological attributes. It seems natural to try to cover features that
reﬂect the variability of these attributes. For this experiment, algorithms must cover all
the units at least once, where a unit is described by the following:

r
r
r
r

Its label, that is, one of the 35 phonemes or a non-speech sound label
The structure of the syllable that contains the phoneme, if it is a vowel
The position of the associated syllable in the word (start, middle, or end)
A Boolean indicated if the associated syllable is at the end of a syntagm

The feature extraction is performed by the same set of tools used by the speech synthesis
engine. Given this set of features, the learning corpus contains 1,497 classes of units. The
cost function to minimize by the reduction algorithms is the total length of the set of the
selected syntagms, in phones.

Two speech synthesis systems are deﬁned, extracting the speech units to concate-
nate from the coverings provided by LamSCP and ASA. Two other systems are added as
baselines. First, a system named Full, built with the whole learning corpus, is used as an
upper bound. Second, a system named Random uses a random reduction of the Learning
corpus as a pool corpus of speech units. This reduction is done by randomly selecting
sentences from the whole learning corpus until the size of the covering obtained by
LamSCP is reached. Random is used as a lower bound.

Whereas the optimization efﬁciency is measured by statistics on the reduced cor-
pora, the quality of the synthesized speech signals is evaluated by a listening test.
The protocol is based on a MUSHRA test, presented in ITU-R (2003), where for every
sentence of Test corpus, the signals synthesized by the four systems are presented to
each tester in a random order. If a system is not able to produce a signal for a re-
quested sentence (because of a missing 2-phone in the pool corpus), an empty signal
is presented. Ten native French testers (four naives and six experts) are asked to eval-
uate the overall quality of the stimuli and to give a mark from 0 to 100 (by steps of
5 points).

378

Barbot et al.

Large Linguistic Corpus Reduction

Table 12
Statistics about the reduced corpora computed by ASA and LamSCP from Learning corpus.
The last column concerns the best lower bound found by LamSCP.

Speech synthesis experiment: corpus reduction

ASA

LamSCP

Covering size (phones)
Reduction rate relative to the full corpus (%)
Reduction rate relative to ASA (%)
Number of syntagms

36,538
–90.70

2,191

35,681
–90.92
–2.35
2,119

L( ˜Λ)

35,655
–90.92
–2.42

–

7.4 Results and Discussion

The Learning corpus composed of 19,587 syntagms is ﬁrst reduced using ASA and
LamSCP. Statistics of the resulting solutions are summarized in Table 12. The covering
of the 1,497 constraints divides the input corpus size by almost 10 and reduces the
10 hours of speech to around 1 hr 20 min. As for the previous experiments, even though
different kinds of features are mixed (phonemes, syllable structures, position in a word
or a syntagm) the ASA algorithm produces a solution close to the optimal one. However,
LamSCP is again slightly better in terms of covering size.

For the measure of the acoustic impact of corpus reduction, the listening test results
are presented in Table 13 for the average marks and in Table 14 for the average ranks.
Note that without a natural speech reference during the test, the marks should not be
seen as an absolute score. Even if the LamSCP corpus is slightly smaller than the one
from ASA, the acoustic quality of both systems is comparable according to the testers
(with a slight advantage for the LamSCP corpus). In comparison with the baseline,
which uses the whole learning corpus, the acoustic degradation is signiﬁcant. This
illustrates well the trade-off between corpus size and speech quality: For a 90% corpus
size reduction, the acoustic quality drops by 10 points. Further research should be
focused on the set of attributes to cover and their number of occurrences in order to
improve this compromise. As expected, the baseline built from random sentences is
preferred signiﬁcantly less than the other systems because of the lack of relatively rare
acoustic units.

Table 13
MUSHRA average marks of 30 sentences from the Test corpus with 10 listeners per sentence
(the higher the better).

379

Computational Linguistics

Volume 41, Number 3

Table 14
MUSHRA average ranks for 30 sentences from the Test corpus with 10 listeners per sentence
(the lower the better).

8. Conclusion

This article discussed the building of linguistic-rich corpora under an objective of
parsimony. This task, a generalization of SCP, turns out to be an NP-hard problem that
cannot be polynomially approximated. We studied the behavior of several algorithms
in the particular domain of NLP, where the considered events follow a heavy-tailed type
distribution.

The proposed algorithms have been compared through three kinds of experiments:
The ﬁrst one is the coverings of 2- and 3-phonemes from two text corpora, one in French,
the other in English; the second one consists of the coverings of part-of-speech labels
from a corpus in French; the third one evaluates the impact of both algorithms on the
acoustic quality of a corpus-based TTS system. The ﬁrst algorithm, ASA, is composed of
an agglomerative greedy strategy followed by a spitting greedy stage. The second one,
LamSCP, is based on Lagrangian relaxation principles combined with greedy strate-
gies. LamSCP is our adaptation of an algorithm proposed in Caprara, Fischetti, and
Toth (1999) to the multi-representation constraints. The comparison of SCP solutions
is mainly about their size, their maximal distance with the optimal covering, and their
robustness in case of perturbation of the initial corpus ordering.

Although ASA is much faster than LamSCP, it does not permit us to single-
handedly assess the quality of its solution in terms of size. The main assets of LamSCP
are the calculation of a lower bound to the optimal covering size and shorter solutions
than the ones obtained by ASA. Indeed, in our experiments of phonological coverings,
the optimal solution is at most 1.24% (10.13%, respectively) smaller than the solutions
derived by LamSCP (ASA, respectively). As for the coverings of 1-POS, LamSCP pro-
vides the optimal solution in a case of a mono-representation constraint, whereas the
ASA solution is 10.17% greater than the optimal one. These relative gaps between
the lower bounds and solution sizes of both algorithms generally decrease when the
size of the search space decreases. Thanks to the lower bound derived by LamSCP,
we empirically show that it is possible to get almost optimal solutions in a linguistic
framework following Zipf’s law distribution, despite the theoretic complexity of the
multi-represented SCP. Concerning the last experiment in the TTS framework, even if
LamSCP provides a smaller corpus, the subjective test shows no signiﬁcant difference
between the TTS systems based on LamSCP and ASA corpora. Therefore, we think that
ASA remains the most adequate strategy, in terms of performance, ease of development,
and computation time to solve SCP in the NLP ﬁeld. However, it would be interesting

380

Barbot et al.

Large Linguistic Corpus Reduction

to test a parallelized version of the heuristic phase that calls an important number of
greedy sub-procedures.

Our future prospects for this work are in automatic language processing and speech
synthesis. First, in the framework of the Phorevox project supported by the French
National Research Agency, we are considering the automatic design of exercise contents
for language learning by the selection of texts covering some phonological or linguistic
difﬁculties. Secondly, this work is a preliminary step to building a phonetically rich
script before its recording in order to produce a high quality speech synthesis. The
covering choices, such as the attributes to cover, the number of required occurrences,
or the “sentence” length (utterances, syntagms, etc.) need to be validated. Moreover,
in this article, we have observed the great impact of the distribution of rare units in
the corpus to reduce, and we believe it will be interesting to adapt the “sentence”
granularity according to this distribution.

References
Alon, Noga, Dana Moshkovitz, and Shmuel

Safra. 2006. Algorithmic construction of
sets for k-restrictions. ACM Transactions on
Algorithms (TALG), 2(2):153–177.

Barbot, Nelly, Olivier Bo¨effard, and Arnaud
Delhay. 2012. Comparing performance of
different set-covering strategies for
linguistic content optimization in speech
corpora. In Proceedings of the International
Conference on Language Resources and
Evaluation (LREC), pages 969–974, Istanbul.

B´echet, Fr´ed´eric. 2001. Liaphon: un systeme

complet de phon´etisation de textes.
Traitement automatique des langues,
42(1):47–67.

Bo¨effard, Olivier, Laure Charonnat, S´ebastien

Le Maguer, Damien Lolive, and Ga¨elle
Vidal. 2012. Towards fully automatic
annotation of audiobooks for TTS. In
Proceedings of the International Conference on
Language Resources and Evaluation (LREC),
pages 975–980, Istanbul.

Bo¨effard, Olivier and Christophe

d’Alessandro, 2012. Speech Synthesis. Wiley.

Bunnell, H. Timothy. 2010. Crafting small

databases for unit selection TTS: Effects on
intelligibility. In Proceedings of the ISCA
Tutorial and Research Workshop on Speech
Synthesis (SSW7), pages 40–44, Kyoto.

Cadic, Didier, C´edric Boidin, and Christophe
d’Alessandro. 2010. Towards optimal TTS
corpora. In Proceedings of the International
Conference on Language Resources and
Evaluation (LREC), pages 99–104, Malta.

Candito, Marie, Enrique Henestroza

Anguiano, and Djam´e Seddah. 2011. A
word clustering approach to domain
adaptation: Effective parsing of biomedical
texts. In Proceedings of the 12th International
Conference on Parsing Technologies,
pages 37–42, Dublin.

Caprara, Alberto, Matteo Fischetti, and Paolo

Toth. 1999. A heuristic method for the set
covering problem. Operations Research,
47(5):730–743.

Caprara, Alberto, Paolo Toth, and Matteo

Fischetti. 2000. Algorithms for the set
covering problem. Annals of Operations
Research, 98(1-4):353–371.

Ceria, Sebasti´an, Paolo Nobili, and Antonio

Sassano. 1998. A Lagrangian-based
heuristic for large-scale set covering
problems. Mathematical Programming,
81(2):215–228.

Chevelu, Jonathan, Nelly Barbot, Olivier

Bo¨effard, and Arnaud Delhay. 2007.
Lagrangian relaxation for optimal corpus
design. In Proceedings of the ISCA Tutorial
and Research Workshop on Speech Synthesis
(SSW6), pages 211–216, Bonn.

Chevelu, Jonathan, Nelly Barbot, Olivier

Bo¨effard, and Arnaud Delhay. 2008.
Comparing set-covering strategies for
optimal corpus design. In Proceedings of the
International Conference on Language
Resources and Evaluation (LREC),
pages 2951–2956, Marrakech.

Combescure, Pierre. 1981. 20 listes de 10
phrases phon´etiquement ´equilibr´ees.
Revue d’Acoustique, 56:34–38.

Fisher, Marshall L. 1981. The Lagrangian
relaxation method for solving integer
programming problems. Management
Science, 27(1):1–18.

Franc¸ois, H´el`ene and Olivier Bo¨effard. 2001.

Design of an optimal continuous speech
database for text-to-speech synthesis
considered as a set covering problem. In
Proceedings of the European Conference on
Speech Communication and Technology
(Eurospeech), pages 829–832, Aalborg.

Franc¸ois, H´el`ene and Olivier Bo¨effard. 2002.
The greedy algorithm and its application

381

Computational Linguistics

Volume 41, Number 3

to the construction of a continuous
speech database. In Proceedings of the
International Conference on Language
Resources and Evaluation (LREC),
pages 1420–1426, Las Palmas,
Canary Islands.

Gauvain, Jean-Luc, Lori Lamel, and Maxine
Esk´enazi. 1990. Design considerations and
text selection for Bref, a large French
readspeech corpus. In Proceedings of the
International Conference of Spoken Language
Processing (ICSLP), pages 1097–1100,
Kobe.

Gotab, Pierre, Fr´ed´eric B´echet, and G´eraldine

Damnati. 2009. Active learning for
rule-based and corpus-based spoken
language understanding models. In
Proceedings of the IEEE workshop on
Automatic Speech Recognition and
Understanding (ASRU), pages 444–449,
Merano.

Hart, Michael. 2003. Project gutenberg.
http://www.gutenberg.org/ (Last
consulted April 2015).

ITU-R. 2003. ITU-R recommendation bs.1534:

Method for the subjective assessment of
intermediate quality levels of coding
systems. Radio communication Bureau,
Geneva.

Karp, Richard M. 1972. Reducibility
among combinatorial problems. In
Complexity of Computer Computations,
the IBM Research Symposia Series.
Springer, pages 85–103.

Kawai, Hisashi, Seiichi Yamamoto, Norio

Higuchi, and Tohru Shimizu. 2000. A
design method of speech corpus for
text-to-speech synthesis taking
account of prosody. In Proceedings of the
International Conference on Spoken Language
Processing (ICSLP), pages 420–425,
Beijing.

Kominek, John and Alan W. Black. 2003. The

CMU Arctic speech databases for speech
synthesis research. Technical Report
CMU-LTI-03-177, Carnegie Mellon
University Language Technologies
Institute. Pittsburg, PA.

Krstulovi´c, Sacha, Fr´ed´eric Bimbot, Olivier
Bo¨effard, Delphine Charlet, Dominique
Fohr, and Odile Mella. 2006. Optimizing
the coverage of a speech database through
a selection of representative speaker
recordings. Speech Communication,
48(10):1319–1348.

Krul, Aleksandra, G´eraldine Damnati,

Franc¸ois Yvon, C´edric Boidin, and Thierry
Moudenc. 2007. Adaptive database
reduction for domain speciﬁc speech

382

synthesis. In Proceedings of the ISCA
Tutorial and Research Workshop on Speech
Synthesis (SSW6), pages 217–222,
Bonn.

Krul, Aleksandra, G´eraldine Damnati,

Franc¸ois Yvon, and Thierry Moudenc.
2006. Corpus design based on the
Kullback-Leibler divergence for
Text-To-Speech synthesis application.
In Proceedings of the International
Conference on Spoken Language Processing
(ICSLP), pages 2030–2033,
Pittsburgh, PA.

Neubig, Graham and Shinsuke Mori. 2010.

Word-based partial annotation for efﬁcient
corpus construction. In Proceedings of the
International Conference on Language
Resources and Evaluation (LREC),
pages 2723–2727, Malta.

Raz, Ran and Shmuel Safra. 1997. A

sub-constant error-probability low-degree
test, and a sub-constant error-probability
PCP characterization of NP. In Proceedings
of the twenty-ninth annual ACM symposium
on Theory of computing, STOC ’97,
pages 475–484, El Paso, TX.

Rojc, Matej and Zdravko Kaˇciˇc. 2000. Design
of optimal Slovenian speech corpus for use
in the concatenative speech synthesis
system. In Proceedings of the International
Conference on Language Resources and
Evaluation (LREC), pages 321–326,
Athens.

Schein, Andrew I., Ted S. Sandler, and

Lyle H. Ungar. 2004. Bayesian example
selection using BaBiES. Technical Report
MS-CIS-04-08, Department of Computer
and Information Science, University of
Pennsylvania.

Settles, Burr. 2010. Active learning literature
survey. Technical Report 1648, Department
of Computer Sciences, University of
Wisconsin, Madison.

Synapse. 2011. Documentation technique:

Composant d’´etiquetage et lemmatisation.
http://www.synapse-fr.com/.

Tian, Jilei and Jani Nurminen. 2009.

Optimization of text database using
hierachical clustering. In Proceedings of the
IEEE International Conference on Acoustics,
Speech, and Signal Processing (ICASSP),
pages 4269–4272, Taipei.

Tian, Jilei, Jani Nurminen, and Imre

Kiss. 2005. Optimal subset selection
from text databases. In Proceedings of
the IEEE International Conference on
Acoustics, Speech, and Signal
Processing (ICASSP), pages 305–308,
Philadelphia, PA.

Barbot et al.

Large Linguistic Corpus Reduction

Tomanek, Katrin and Fredrik Olsson. 2009. A
Web survey on the use of active learning to
support annotation of text data. In
Proceedings of the NAACL HLT 2009
Workshop on Active Learning for Natural
Language Processing, pages 45–48,
Boulder, CO.

Van Santen, Jan P. H. and Adam L.

Buchsbaum. 1997. Methods for optimal
text selection. In Proceedings of the European

Conference on Speech Communication and
Technology (Eurospeech), pages 553–556,
Rhodes.

Zhang, Jin-Song and Satoshi Nakamura.

2008. An improved greedy search
algorithm for the development
of a phonetically rich speech
corpus. IEICE Transactions
on Information and Systems,
E91-D(3):615–630.

383



######################
## output of PY2PDF ##
######################

PdfReadWarning: Xref table not zero-indexed. ID numbers for objects will be corrected. [pdf.py:1736]
ComputationalLinguistics
Volume41,Number3
1.Introduction
Inautomaticspeechandlanguageprocessing,manytechnologiesmakeextensiveuse
ofwrittenorreadtextsets.Theselinguisticcorporaareanecessitytotrainmodelsorto
extractrules,andthequalityoftheresultsstronglydependsonacorpus'content.Often,
thereferencecorpusshouldprovideamaximumdiversityofcontent.Forexample,in
Tian,Nurminen,andKiss(2005),andTianandNurminen(2009),itturnsoutthatmax-
imizingthetextcoverageofthelearningcorpusimprovesanautomatic
basedonaneuralnetwork.Similarly,ahighqualityspeechsynthesissystembasedon
theselectionofspeechunitsrequiresarichcorpusintermsofdiphones,diphones
incontext,triphones,andprosodicmarkers.Inparticular,Bunnell(2010)showsthe
importanceofagoodcoverageofdiphonesandtriphonesfortheintelligibilityofa
voiceproducedbyaunitselectionspeechsynthesissystem.
Tocoverthebestattributesneededforatask,severalstrategiesarethenpossible.A
methodŠverysimpleŠistocollecttextrandomly,butitsoonbecomesexpensive
becauseofthenaturaldistributionoflinguisticeventsfollowingtheZipf'slaw.Very
feweventsareextremelyfrequentandmanyeventsareveryrare.Thisproblemis
oftenmadedifbythefactthatmanytechnologiesrequireseveralvariantsofthe
sameevent(asinaText-to-Speech[TTS]systemusingseveralacousticalversionsof
thesamephonologicalunit).Usually,alargevolumeofdataneedstobecollected.
However,anddependingontheapplications,buildingsuchcorporaisoftenachieved
underaconstraintofparsimony.Asanexample,foraTTSsystem,ahigh-quality
syntheticvoicegenerallyneedsahugenumberofspeechrecordings.Butminimizing
thedurationofarecordingisalsoacriticalpointtoensureuniformqualityofthe
voice,toreducethedrudgeryoftherecording,toreducethecost,orto
followatechnicalconstraintontheamountofcollecteddataforembeddedsystems.
Moreover,areducedsettendstolimittheneedofhumanimplicationforchecking
thedata(transcriptionandannotation).Similarly,inthenaturallanguageprocessing
(NLP),theadaptationofagenericmodeltoadomainoftenrequiresnew
annotateddatathatillustrateits(asinCandito,Anguiano,andSeddah
2011).However,thecreationcostofsuchdatahighlydependsonthekindoflabels
usedtoadaptthemodel.Inparticular,theannotationinsyntaxtreesisreallymore
expensivethaninPart-of-Speech(POS)tags.Then,itcouldbemoreeftoan-
notateacompactcorpusthatrthephenomenavariabilitythanacorpuswith
anaturaldistributionofevents,whichimpliesmanyredundancies(seeNeubigand
Mori2010).
Inamachinelearningframework,theactivelearningstrategycanbeusedas
analternativethatreducesthemanualdataannotationefforttodesignthetraining
corpuswithoutdiminishingthequalityofthemodeltotrain(seeSettles2010orSchein,
Sandler,andUngar2004).Itconsistsofbuildingthecorpusiterativelybychoosing
anitemaccordingtoanexternalsourceofinformation(auseroranexperimental
measure).ThisapproachhasbeenappliedinNLP,speechrecognition,andspoken
languageunderstanding(seeforinstanceTomanekandOlsson2009andGotab,B
´
echet,
andDamnati2009).
Asecondalternative,whennodirectqualitymeasureisavailable,consistsof
coveringalargesetofattributesthatmayimpactthequality(afterannotation
orrecording).Thiskindofapproachmightalsobepreferredwhenthecorpus
isbuiltinonebatch(forinstance,becauseofout-sourcingorannotator/performer
consistencyconstraints).Amethodcouldbeanautomaticextractionfromahuge
textcorpusofaminimalsizedsubsetthatcoverstheattributes.This
356



Barbotetal.
LargeLinguisticCorpusReduction
problemisageneralizationofthe
Set-CoveringProblem(SCP)
,whichisanNP-
hardproblem,asshowninKarp(1972).Itisthennecessarytouseheuristicsor
sub-optimalalgorithmsforareasonablecomputationtime.Moreover,RazandSafra
(1997)andAlon,Moshkovitz,andSafra(2006)haveshownthattheSCPcannot
bepolynomiallyapproximatedwithratio
c

ln(
n
)unless
P
=
NP
,when
c
isacon-
stant,and
n
referstothesizeoftheuniversetocover.Thatmeansthatonecan-
notbecertaintoobtainaresultunderthisratiowithanypolynomialalgorithm.
However,thelattercomplexityresultsaregivenforanykindofdistributioninthe
mono-representationcase.Onecanaskifgoodmulti-representedcoveragescanbe
achievedefondatafollowingZipf'slaw,whichisusualinthedomainof
NLP.
Withintheofspeechprocessing,themostfrequentlyusedstrategyisagreedy
methodbasedonanagglomerationpolicy.Thisiterativealgorithmselectsthesen-
tencewiththehighestscoreateachiteration.Thescorerthecontributionof
thesentencetothecoveringunderconstruction.InGauvain,Lamel,andEsk
´
enazi
(1990),thismethodologyhasbeenappliedtobuildadatabaseofreadspeechfrom
atextcorpusfortheevaluationofspeechrecognitionsystemsusinghierarchically
organizedcoveringattributes.VanSantenandBuchsbaum(1997)havetesteddifferent
variantsofgreedyselectionoftextsbyvaryingtheunitstocover(diphones,dura-
tion,etc.)andtheﬁscoresﬂforasentencedependingontheconsideredapplications.
InTian,Nurminen,andKiss(2005),thelearningcorpusforanautomaticsystemof
isdesignedusingagreedyapproachwiththeLevenshteindistance
asascorefunctioninordertomaximizeitstextdiversity.InFranc¸oisandBo
¨
effard
(2001),themethodologygivesaprioritytotherarestcategoriesofallophones.The
lattermethodologyhasbeenimplementedfortheofthemulti-speakercorpus
NeologosinKrstulovi
´
cetal.(2006).InthearticleofKruletal.(2006),theauthors
constructedacorpuswherethedistributionofdiphonemes/triphonemesmatchesa
uniformdistribution.Agreedyalgorithmisledbyascorefunctionbasedonthe
Kullback-Lieblerdivergence.AsimilarmethodisusedinKruletal.(2007)todesigna
reduceddatabaseinaccordancewithadomaindistribution.Kawaietal.(2000)
proposeapairexchangemechanismthatRojcandKa

ci

c(2000)applyafterareverse
greedyalgorithmŠalsocalledspittinggreedyŠdeletingtheuselesssentences.InCadic,
Boidin,andd'Alessandro(2010),thecoveringofﬁsandwichﬂunitstobemore
adaptedtocorpus-basedspeechsynthesis)iscarriedoutbygeneratingnewsentences
inasemi-automaticway.Candidatesaregeneratedusingstatetransducers.The
sentencesareorderedaccordingtoagreedycriterion(theirsandwichesrichness)and
presentedtoahumanevaluator.Thiscollectionofandrichsentencesen-
ablesaneffectivereductionofthesizeofthecoveringbutrequiresexpensivehuman
interventiontoobtainsemanticallycorrectsentencesthatwillbethereforeeasierto
record.
Theresultsofthesepreviouslycitedstudiesarediftocomparebecauseof
thedifferentinitialcorporaandcoveringconstraints(partialorfullcovering)and
evaluationcriteria(thenumberofgatheredsentences,theKullbackdivergence,etc.).
InZhangandNakamura(2008),aprioritypolicyfortherareunitsisaddedintoan
agglomerativegreedyalgorithminordertogetacoveringoftriphonemeclassesfroma
largetextcorpusinChineselanguage.Theresultsshowthatthisprioritypolicydriven
bythescorefunctionandthephoneticcontentofthesentencesreducesthecovering
sizecomparedwithastandardagglomerativegreedyalgorithm.
Similarly,inFranc¸oisandBo
¨
effard(2002),severalcombinationsofgreedyalgo-
rithms(agglomeration,spitting,pairexchange,orprioritytorareunits)wereapplied
357



ComputationalLinguistics
Volume41,Number3
totheconstructionofacorpusforspeechsynthesisinFrenchcontainingatleastthree
representativesofthemostfrequentdiphones.Basedonthiswork,thebeststrategy
wouldbetheapplicationofanagglomerativegreedyfollowedbyaspittinggreedy
algorithm.Duringtheagglomerationphase,thescoreofasentencecorrespondstothe
numberofitsunitinstancesthatremaintobecoverednormalizedbyitslength.During
thespittingphase,ateachiteration,thelongestredundantsentenceisremovedfromthe
covering.Thisalgorithmiscalledthe
AgglomerationandSpittingAlgorithm(ASA)
.
Asanalternativetoagreedyalgorithm,whichissub-optimal,solvingtheSCPusing
Lagrangianrelaxationprinciplescanprovideanexactsolutionforproblemsofreason-
ablesize.However,forspeechprocessing,theSCPhasseveralmillionsofsentenceswith
tensofthousandsofcoveringfeatures.Consideringthesepracticalconstraints,Chevelu
etal.(2007)adaptedaLagrangianrelaxationbasedalgorithmproposedbyCaprara,
Fischetti,andToth(1999).InthecontextofItalianrailways,Caprara,Fischetti,andToth
proposedheuristicstosolveschedulingproblemsandwonacompetition,called
Faster,organizedbytheItalianOperationalResearchSocietyin1994,aheadofother
LagrangianrelaxationheuristicsŒbasedalgorithms,likeCeria,Nobili,andSassano
(1998).
InCheveluetal.(2007,2008),thealgorithmtakesintoaccounttheconstraintsof
multi-representation.Aminimalnumberofrepresentativesforthesameunitmaybe
required.Theproposedalgorithm,called
LamSCPŠLagrangian-basedAlgorithm
forMulti-representedSCP
Šisappliedtoextractcoveringsofdiphonemeswitha
mono-ora5-representationandcoveringsoftriphonemeswithmono-representation
constraints.TheseresultsarecomparedwiththegreedystrategyASAandareabout5%
to10%better.Besides,theLamSCPprovidesalowerboundforthecostoftheoptimal
coveringandallowsforevaluatingthequalityoftheresults.
InBarbot,Bo
¨
effard,andDelhay(2012),phonologicalcontentofdiphonemecover-
ingsisstudiedregardingmanyparameters.Thesecoveringsareobtainedbydifferent
algorithms(LamSCP,ASA,greedybasedontheKullbackdivergence)andsomeof
thecoveringsarerandomlycompletedtoreachagivensize(from20,000to30,000
phones).ItturnsoutthatthecoveringsobtainedusingLamSCPandASAprovidea
goodrepresentationofshortunitsandtherepresentationoflongunitsmainlydepends
onthelengthofthecorpus.
Inthisarticle,wepresentinmoredetailtheLamSCPalgorithmanditsscorefunc-
tionsandheuristicsthattakeintoaccountmulti-representationconstraints.Wedeepen
thestudyabouttheperformanceofLamSCPfortheconstructionofaphonologically
richcorpusaccordingtothesizeofthesearchspace.WeevaluateLamSCPandASA
algorithmsonacorpusofsentencesinEnglishforacoveringofmulti-represented
diphones,wheretheminimalnumberofrequiredunitrepresentativesvariesfromone
toWealsocomparetheminthecaseofveryconstrainedtriphonemecoveringsin
EnglishandFrench,whichrepresentabout12timesmoreunitstocover.Additionally,
bothalgorithmsaretestedtoprovidemulti-representedcoveringsofPOStagsinorder
toassesstheirabilitytodealwithdifferentkindsoflinguisticdata.Aparticulareffort
hasbeenmadeonmethodologytoobtaincomparablemeasures,tostudythestability
ofbothalgorithms,andtoestablishintervalsforeachsolution.
Thisarticleisorganizedasfollows.InSection2,theSCPframeworkandthe
associatednotationsareintroduced.TheASAalgorithmisdescribedinSection3and
theLamSCPisdetailedinSection4.Theexperimentalmethodologyispresentedin
Section5andresultsarediscussedinSection6.BeforeconcludinginSection8,we
presentexperimentsinthecontextofTTSwhereweevaluateonthattaskthe
ofareductioninsection7.
358



Barbotetal.
LargeLinguisticCorpusReduction
2.TheSet-CoveringProblem
BeforedescribingtheSCP-solvingalgorithmsproposedinthisarticle,weintroducein
thissectionsomenotationsandtheLagrangianpropertiesusedbyLamSCP.
Letusconsideracorpus
A
composedof
n
sentences
s
1
,
:::
,
s
n
.Accordingtothe
targetapplications,thesesentencesareannotatedwithrespecttophonological,acoustic,
prosodicattributes,andsoforth.Eachsentenceisthenassociatedwithafamilyofunits
ofdifferenttypes.Thesetofunitspresentin
A
isdenoted
U
=
f
u
1
,
:::
,
u
m
g
and
A
can
berepresentedbyamatrix
A
=
(
a
ij
),where
a
ij
isthenumberofinstancesofunit
u
i
inthe
sentence
s
j
.Therefore,the
j
thcolumnof
A
correspondstosentence
s
j
in
A
.Tosimplify
thewriting,wethesets
M
=
f
1,
:::
,
m
g
and
N
=
f
1,
:::
,
n
g
.
Foragivenvectorofintegers
B
=
(
b
1
,
:::
,
b
m
)
T
,areduction
X
of
A
,alsocalled
coveringof
U
,isasasubsetof
A
whichcontains,forevery
i
2
M
,atleast
b
i
instancesof
u
i
.Itcanbedescribedbyavector
X
=
(
x
1
,
:::
,
x
n
)
T
where
x
j
=
1if
s
j
belongs
to
X
and
x
j
=
0otherwise.Inotherwords,acoveringisasolution
X
2f
0,1
g
n
ofthe
followingsystem
8
i
2
M
,
X
j
2
N
a
ij
x
j

b
i
(1)
thatis,
AX

B
where
B
iscalledtheconstraintvector.
Ouraimistooptimizeacoveringaccordingtoacostfunctionminimizationcrite-
rion.Thecoveringcostisgivenbysummingthecostsofthesentencesthatcomposethe
covering.TheoptimizationproblemcanbeformulatedasthefollowingSCP:
X

=
argmin
X
2f
0,1
g
n
AX

B
CX
(2)
where
C
=
(
c
1
,
:::
,
c
n
)isthecostvectorand
c
j
thecostofthesentence
s
j
.Becauseofthe
objectivetominimizethetotallengthofthecovering,wehavechosentothecost
ofasentenceasoneofitslengthfeatures.Accordingtotheconsideredapplication,the
sentencecostcanbeasitsnumberofphones(oneofourobjectivesistodesign
aphoneticallyrichscriptwithaminimalspeechrecordingduration),oritsnumberof
words,part-of-speechtags,breathgroups,andsoon.
InCaprara,Fischetti,andToth(1999),Caprara,Toth,andFischetti(2000),andCeria,
Nobili,andSassano(1998),thestudiedcrewschedulingproblemisaparticularcaseof
Equation(2)where
A
isabinarymatrixand
B
=
1
R
m
(i.e.,withmono-representation
constraints).InordertoensurethatEquation(1)admitsasolution,weassumethat,for
each
i
2
M
,theminimalnumber
b
i
of
u
i
instancesrequiredinthecoveringisnotgreater
thanthenumber(
A
1
R
n
)
i
of
u
i
instancesin
A
,thatis
A
1
R
n

B
.Underthisassumption,
A
isthemaximalsizesolutionofEquation(1),representedby
X
=
1
R
n
.Inthecasewhere
b
i
isgreaterthanthenumberof
u
i
instancesin
A
,
b
i
issetto(
A
1
R
n
)
i
.
TodrivetheSCPalgorithmsduringthesentenceselectionphase,thecovering
capacity

j
ofsentence
s
j
isasthenumberofitsunitinstancesrequiredinthe
coveringinviewoftheconstraintvector:

j
=
X
i
2
M
min

a
ij
,
b
i

(3)
359



ComputationalLinguistics
Volume41,Number3
Letusnoticethat

j
doesnotconsidertheexcessunitinstances:Forexample,if
s
j
contains
a
ij
=
10instancesof
u
i
andatleast
b
i
=
3instancesof
u
i
arerequired,the
contributionof
u
i
to

j
derivationonlytakesintoaccountthreeinstancesof
u
i
.
3.GreedyAlgorithmASA
Inthissection,thetwomainstepsthatcomposethealgorithmASAarede-
scribed.First,anagglomerativegreedyprocedureisappliedto
A
soastoderivea
covering.Next,aspittinggreedyprocedurereducesthiscoveringinordertoapproach
theoptimalsolutionofEquation(2).
3.1AgglomerativeGreedyStrategy
Thegreedystrategybuildsasub-optimalsolutiontotheSCPEquation(2)inaniterative
way.Ateachiteration,thelowestcostsentenceischosenfrom
A
.Ifseveralsentences
correspondtothelowestcost,theonecoming(i.e.,theonewiththelowestindex)
ischosen.Initially,thesetofselectedsentences
X
isempty,thematrix
Ÿ
A
associatedwith
thecandidatesentencesisassignedto
A
,thecurrentcoveringcapacityof
s
j
isgivenby
Ÿ

j
=

j
,andthecurrentconstraintvector
Ÿ
B
=
B
.Thecostofsentence
s
j
isby
˙
j
=
ˆ
c
j
=
Ÿ

j
ifŸ

j
6
=
0
1
otherwise
(4)
Indeed,ifŸ

j
=
0,itturnsoutthat
s
j
doesnotcoveranyunitmissinginthesolution
X
underconstructionanditscost
˙
j
avoidsitsselection.
Ateachiteration,theselectedsentence
s
isaddedto
X
.Takingintoaccountthe
contentof
s
,
Ÿ
B
isupdatedtomax

Ÿ
B

Ÿ
A

,
0
R
m

wherethe
j
thentryof

equals1if
s
j
=
s
and0otherwise.Next,theassociatedcolumnof
s
in
Ÿ
A
issetto
0
R
m
.Foreach
sentence
s
j
withanon-zeroŸ

j
feature,Ÿ

j
isthenupdatedusing
Ÿ
A
and
Ÿ
B
inEquation(3).
Atlast,theagglomerativegreedyalgorithmisstoppedassoonasalltheconstraints
arethatis,
Ÿ
B
=
0
R
m
.
3.2SpittingGreedyStrategy
Thespittinggreedystrategyalsoconsistsinbuildingiterativelyasub-optimalsolution
Y
toEquation(2)byreducingthesizeofacovering.Theinitialcovering
Y
issettothe
solution
X
derivedbytheagglomerativephasedescribedearlier.Ateachiteration,the
setoftheredundantsentencesof
Y
iscalculatedandthecostliestone(accordingto
thecostfunction
C
)isremovedfrom
Y
.Anelement
s
of
Y
issaidtoberedundantiffor
each
u
i
2
U
,itsnumberofinstancesinto
Y
,denoted
m
i
(
Y
),andinto
Y
nf
s
g
,denoted
m
i
(
Y
nf
s
g
),checkmin
f
m
i
(
Y
),
b
i
g
=
min
f
m
i
(
Y
nf
s
g
),
b
i
g
.Inotherwords,
s
isaredun-
dantelementofthecovering
Y
if
Y
nf
s
g
isalsoacoveringsolutionofEquation(1).
Thespittinggreedyalgorithmstopswhentheredundantsentencesetisempty.
4.LagrangianRelaxationBasedŒAlgorithm
ThissectiondescribesthemainphasesofthealgorithmcalledLamSCP.Thisalgorithm
takesadvantageoftheLagrangianrelaxationpropertiesreviewedhereininorderto
approachtheoptimalsolutionofEquation(2)ascloseaspossible.Stronglyinspiredby
360



Barbotetal.
LargeLinguisticCorpusReduction
Caprara,Fischetti,andToth(1999),butgeneralizedtothemulti-representationprob-
lem,thisalgorithmprovidesalowerboundoftheoptimalsolutioncost.Havingsuch
informationisveryusefulforassessingtheachievementsoftheSCPalgorithms.
4.1LagrangianRelaxationPrinciples
LetusrecallthemainprinciplesofLagrangianrelaxationonwhichLamSCPis
basedtosolveEquation(2)(seeFisher[1981]formoredetailsonLagrangianrelaxation).
First,theLagrangianfunctionassociatedwithEquation(2)isby
L
(
X
,

)
=
CX
+
T
(
B

AX
)
=
T
B
+
C
(

)
X
(5)
where

2

R
+

m
,
X
2f
0,1
g
n
,and
C
(

)
=
C


T
A
.Thecoordinatesof
=
(

1
,
:::
,

m
)
T
arecalled
Lagrangianmultipliers
andcanbeinterpretedasaweighting
ofconstraints(1).The
j
thentryof
C
(

),called
Lagrangiancost
c
j
(

)ofsentence
s
j
,takes
intoaccountitscost
c
j
andtheadequacyofitscompositiontoaddressEquation(2).For
everycovering
X
andevery

2

R
+

m
,theLagrangianfunction
L
(
X
,

)

CX
.
Thus,thedualLagrangianfunctionby
L
(

)
=
min
X
2f
0,1
g
n
L
(
X
,

)
(6)
presentsthefollowingfundamentalproperty:Forevery

2
R
m
+
andeverycovering
X
,wehave
L
(

)

CX
.Hence,
L
(

)isalowerboundoftheminimalcoveringcost,
CX

,
butdoesnotnecessarilycorrespondtothecostofacovering.Inordertocompute
L
(

),
anacceptablesolutionforthevector
X
minimizing
L
(
X
,

)is
X
(

)
=
(
x
1
(

),
:::
,
x
n
(

))
T
where
x
j
(

)
=
ˆ
1if
c
j
(

)
<
0
0if
c
j
(

)
>
0
2f
0,1
g
otherwise
(7)
Additionally,thedualLagrangianfunctionandtheLagrangiancostsinformabout
thepotentialusefulnessofsentencesintheoptimalcovering.Moreprecisely,foragiven

andaknownupperboundUBofminimalcoveringcost,thegap
g
(

)
=
UB

L
(

)
measurestherelaxationquality.If
c
j
(

)isstrictlygreaterthan
g
(

),wecancheckthat
anycoveringcontaining
s
j
hasacostvaluestrictlygreaterthanUB.Hence,sentence
s
j
isnotselectedand
x
j
canbeatzero.Similarly,if
c
j
(

)
<

g
(

),anycovering
withacostlowerthanUBcontains
s
j
andonecan
x
j
to1.Therefore,anoptimal
coveringismadeupofsentenceswithalowLagrangiancost,asdoneinCaprara,Toth,
andFischetti(2000)andCeria,Nobili,andSassano(1998),andthehighertherelaxation
quality(i.e.,thelower
g
(

))is,thecheaperthecoveringwillbe.
TheresolutionofthedualproblemofEquation(2)consistsin


2
R
m
+
that
maximizesthelowerbound
L
(

),thatis


=
argmax

2
R
m
+
L
(

)
(8)
361



ComputationalLinguistics
Volume41,Number3
Becausethisrealvariablefunction
L
isconcaveandpiecewiseafawell-known
approachforanear-optimalmultipliervectoristhesubgradientalgorithm.
4.2TheThreePhases
TheLamSCPisaniterativealgorithm,composedofseveralproceduresthataimto
eitherimprovethecurrentbestsolutionorreducethecombinatorialissuerelatedtothe
consideredproblem.Inordertoderiveagoodsolution,thealgorithmcallsonagreat
numberofgreedyprocedurestosolvesub-problemswiththehelpoftheLagrangian
costs.Asforthecombinatorialreduction,themostfrequentlyusedheuristicconsists
ofdownsizingtheproblembymainlyconsideringthesentenceswithlowLagrangian
costs.
Thealgorithmisorganizedaroundamainprocedurecalled
3-phases
.Thispro-
cedurecansingle-handedlysolveamulti-representedSCP.Asitsnamesuggests,
the
3-phases
functioningconsistsiniteratingasequenceofthethreefollowingsub-
proceduresasshowninFigure1:
r
The
subgradientphase
calculatesanestimation
Ÿ

of


thatmaximizes
thedualLagrangianfunction.Thisprocedurerequiresanupperbound
UBoftheoptimalcoveringcost.UBisinitializedbyagreedyalgorithm
(ratherthanthecostofthewholecorpus
A
).Thisphaseisdetailedin
Section4.2.1.
r
The
heuristicphase
explorestheneighborhoodof
Ÿ

bygeneratinga
greatnumberofLagrangianvectors
Ÿ

p
.Agreedy-likeprocedureis
associatedwitheach
Ÿ

p
soastocomputeacoveringusingthe
Lagrangiancostvector
C
(
Ÿ

p
).If,duringthisexploration,alesscostly
coveringthanthebestknownone(correspondingtothecostUB)
isfound,theupperboundUBisthenupdatedtothecostofthis
lesscostlysolution.Similarly,ifabetterestimationof


isobtained,
Ÿ

isupdated.ThisphaseisdescribedinSection4.2.2.
r
The
columnphase
selectsaset
F
ofsentencesthataremost
likelytobelongtotheoptimalcovering.Thisphaseisdetailedin
Section4.2.3.
Followingthecolumnphase,theconstraintvectorisupdatedandtheun-
selectedsentencesaset-coveringsub-problem,calleda
residualproblem
.This
sub-problemisprocessedsimilarly,viaanadditionaliterationofthethreephases.This
iterativeprocessisstoppedwhentheresidualproblemisemptyorwhentheassociated
dualLagrangianfunctionindicatesacostistoohigh.Indeed,becausethisfunction
indicatesaminimalcostforcoveringthesub-problem,itsadditiontothecost
CF
ofthe
sentencesalreadyretainedin
F
givesalowerboundofthetotalcostofthesolution
underconstruction,whichshouldnotrisebeyondthecostUBofthebestknown
solutionsoastobepotentiallymoreadvantageous.
4.2.1SubgradientPhase.
Inordertoreachthequalitygoal,the
subgradientphase
pro-
videsanear-optimalsolution
Ÿ

ofthedualLagrangianproblem(8)usingasubgradient
362



Barbotetal.
LargeLinguisticCorpusReduction
Figure1
LamSCPalgorithm.Therectangleblocksarethestepsthataimatimprovingsolutionquality
andtheellipsesarethosethattrytoreducethesizeoftheproblem.
typealgorithm.Thisiterativeapproachgeneratesasequence(

p
)usingthefollowing
updatingformula(seeCaprara,Fischetti,andToth1999)

p
+
1
=
max
ˆ

p
+

g
(

p
)
jj
S
(

p
)
jj
2
S
(

p
),0
˙
(9)
where
S
(

p
)
=
B

AX
(

p
)soastotakeintoaccountthemulti-representationcon-
straints.Parameter

isadjustedtotheconvergencefastnessaccordingtothemethod
proposedbyCaprara,Fischetti,andToth(1999).
Atthecallof3-phases,

0
isarbitrarilyasfollows:foreach
i
2
M
,

0
i
=
min
j
2
N
a
ij
6
=
0
c
j

j
(10)
AsforUB,itsinitialvalueissettothecostofacoveringpreviouslycalculated.In
ordertoevaluatehowmuchthecoveringcostderivedbyASAcanbeimproved,we
havechosentoinitializeUBbythisvalue.Atthefollowingiterationsof3-phases,

0
is
givenbyarandomperturbation(lessthan10%)ofthebestknownvector
Ÿ

(ofwhich
theentriesofthesentencesinthelastcolumnphaseareremoved)andUB
correspondstothecostofthebestcoveringfound(aftersubtractionofthecostofthe
sentencesinthelastcolumnphase).InanotherapproachproposedbyCeria,
Nobili,andSassano(1998),UBcorrespondstotheupperboundofadualLagrangian
problem,andthesubgradientproceduresimultaneouslyestimatestheupperboundand
thelowerbound,generatingtwosequencesofmultipliers.
Thesubgradientphasealsocallstwoprocedures:
pricing
and
spitting
.Procedure
pricingaimstoreducethesizeofthesearchspace.Foreachunit
u
i
,thepricingselects
the5
b
i
smallestLagrangiancostsentencescovering
u
i
.Ifthisselectioncontainslessthan
5
m
sentences,where
m
isthemaximalentryof
B
,itiscompletedbylowLagrangiancost
sentences(lessthan0.1)tothelimitof5
m
sentences.Thesetofthechosensentencesis
denoted
P
anditsdesignguaranteesasufnumberofinstancesforeachunitto
coverandasmallvarietyinitscomposition.Actually,thesubgradientmethodisapplied
on
P
insteadof
A
,and
P
isupdatedevery10subgradientiterations.
363



ComputationalLinguistics
Volume41,Number3
Finally,ateachiteration,(7)of
X
(

p
)andthelargenumberofLagrangian
costsclosetozeroallowaconsiderablenumberofvectors
S
(

p
).Inordertogetaround
thecomputationaldifofthesteepestaccentdirection,Caprara,Fischetti,
andToth(1999)proposeaheuristicthat,accordingtotheexperimentalresults,accel-
eratestheconvergenceofthesubgradientphase.Thisheuristicisimplementedinthe
spittingprocedure.Calledateachiteration,thisprocedureextractsfrom
P
thesubset
S
ofsentenceswithaLagrangiancostlowerthan0.001.
S
isthenreducedusingaspitting
greedyalgorithmtoremoveitsredundantelementsindecreasingLagrangiancostorder.
Atlast,forevery
j
2
M
,
x
j
(

p
)
=
1if
s
j
2
S
and
x
j
(

p
)
=
0otherwise.Thus,
S
(

p
)does
notnecessarilycorrespondtoasubgradientvector.
4.2.2HeuristicPhase.
Theheuristicphasecalculatesalargenumberofcoveringsbefore
keepingthebestone.Tothatend,asequenceof150multipliervectorsisgeneratedby
perturbing
Ÿ

usingtheformula
Ÿ

p
+
1
=
max

Ÿ

p
+

g
(
Ÿ

p
)
S
(
Ÿ

p
),0

where
Ÿ

0
=
Ÿ

and

isprovidedbythesubgradientphase,soastoallowforachangeinalargenumberof
Ÿ

p
.Witheach
Ÿ

p
,anagglomerativegreedyalgorithmfollowedbyaspittinggreedyone
areassociatedinordertocalculateacovering.
Theagglomerativegreedychoosesateachiterationthesentence
s
j
withthelowest
cost
˙
j
(
Ÿ

p
)where
˙
j
(
Ÿ

p
)
=
8
<
:
c
j
(
Ÿ

p
)

Ÿ

j
if
c
j
(
Ÿ

p
)
<
0andŸ

j
>
0
c
j
(
Ÿ

p
)
=
Ÿ

j
if
c
j
(
Ÿ

p
)

0andŸ

j
>
0
1
ifŸ

j
=
0
(11)
ThiscostfunctionprovidesanadvantagetolowLagrangiancostsentences
s
j
containing
Ÿ

j
unitinstancesthatcouldbehelpfultothecoveringunderconstruction.Theagglom-
erativestepusesseveralheuristicssoastoreducethesearchspace.Itisrunwithina
limitedsubset
P
l
of
P
,composedofthesentences
s
j
withthelowestcosts
˙
j
(
Ÿ

k
).At
eachiteration,asentenceof
P
l
isselectedandthecostofthesentencesof
P
areupdated.
Ifthemaximumsentencecostin
P
l
becomesgreaterthantheminimalcostin
P
n
P
l
,
theworkingsubset
P
l
isalsoupdated.Theof
P
and
P
l
guaranteethatthe
agglomerationstepprovidesanonpartialsolutionoftheconsideredSCP.Thissolution
isthenreducedduringthespittingstepbyiterativelyremovingitsredundantsentences
s
j
withthehighestcosts
c
j
.
Attheendoftheheuristicphase,thebestfoundcovering
X

anditscost
CX

(storedinUB)arekeptaswellasthehighestvalueof
L
(
Ÿ

p
)(foundduringthesub-
gradientorheuristicphases).
4.2.3ColumnFixingPhase.
Thecolumnphaseaimstoreducetheproblemsize
bychoosingﬁpromisingﬂsentencesamongtheoneswithverylowLagrangiancostor
containingrareunitinstances.Theunselectedsentencesarelessinterestingforresolving
theSCPandtheresidualproblemassociatedwiththesesentencesshouldbethesubject
ofanothercallofthe3-phases.
Moreprecisely,thecolumnphasecalculatesthesubset
Q
composedofsen-
tences
s
j
withanegativeLagrangiancost
c
j
(
Ÿ

).
Q
isrepresentedbythebinaryvector
Q
=
(
q
1
,
:::
,
q
n
)
T
where
q
j
=
1if
s
j
2
Q
.Foreach
u
i
2
U
,thenumberofitsinstances
coveredby
Q
isgivenby(
AQ
)
i
.If(
AQ
)
i

b
i
,then
u
i
isconsideredasarareunit
andalltheelementsof
Q
containingsomeinstancesof
u
i
areinaset
F
.The
coveringconstraintsthatarenotby
F
constitutearesidualSCP.Inorderto
364



Barbotetal.
LargeLinguisticCorpusReduction
complete
F
withafewsentencesofthebestknowncovering,agreedy-typealgorithm
isrunon
X

n
F
toderiveasolutionofthisresidualSCP.Fromtheobtainedsolution,
themax
f

B
T
1
R
m

=
20,1
g
lowestLagrangiancostsentencesarealsoaddedin
F
.The
sentencesthatarein
F
duringthecolumnphasestayin
F
uptotheend
ofthe3-phases.Afterthecolumnphase,theresidualsub-problemisprocessed
byiteratingthethreephasesandthenextsentencesareaddedto
F
.
4.3Procedure
The3-phasesprocedureisencapsulatedinanouterloopthatpermitsthepartialre-
considerationofthesolution
X

providedbythisprocedure.Tothatend,the

procedure,proposedinCaprara,Fischetti,andToth(1999),isusedinordertoselect
elementsof
X

thatcontributeatleasttothegap
g
(
Ÿ

).InthecaseoftheSCPwithmulti-
representationconstraints,theofthecontributionof
s
j
2
X

canbeadapted
asfollows:

j
=
max
f
c
j
(
Ÿ

),0
g
+
X
i
2
M
a
ij
>
0
Ÿ

i
(
AX


B
)
i
a
ij
(
AX

)
i
(12)
ThesecondtermofEquation(12)consistsofsharingthecontribution
Ÿ

i
(
AX


B
)
i
ofthe
excessinstancesof
u
i
in
X

accordingtothedistributionof
u
i
instancesin
X

.Therefore,
therprocedureranksinanincreasingordertheelements
s
j
of
X

accordingto
their

j
value,andtheelementsinaset
G
untilitsgivencoveringrate
˝
G
reaches
ˇ
.
˝
G
representstherateofcoveringconstraintsby
G
andisby
˝
G
=
1

P
i
2
M
max
f
b
i

(
AG
)
i
,0
g
B
T
1
R
m
(13)
where
G
denotesthebinaryvectorcorrespondingto
G
.
4.4TheOverallLamSCP
TheLamSCPismadeupofthemainproceduresintroducedintheprevioussections
interlinkedbythefollowingsteps.First,becauseoftheadaptationofthealgorithmto
theSCPwithmulti-representationconstraints,theentriesofmatrix
A
areclippedto
theconstraintvectorinordertosimplifythecalculationssuchastheonesof

1
,
:::
,

n
.
Thisthresholdapplicationimpliesthattheexcessinstancesofeachunit
u
i
aretakeninto
accountineachsentence
s
j
beyondthenumber
b
i
,inthederivationof

j
.
Aftertheinitializationof

0
usingEquation(10)andtheupperboundUB,pro-
cedure3-phasesiscalledandprovidesasolution
X
tothecompleteSCP.The
r
functionasentencesubset
G
suchthat
G
coversagivenrate
ˇ
ofthecovering
constraints.Parameter
ˇ
startsataminimumvalue
ˇ
min
=
0
:
3.TheresidualSCPisthen
processedby3-phases.The
ˇ
valuegrowsatarateof20percentwhenever3-phases
doesnotimprovethesolutiontothecompleteSCP.If
ˇ
isgreaterorequalto1,the
r
procedurethewholebestsolutionandtheresidualproblemisthenempty.
Ontheotherhand,
ˇ
issetto
ˇ
min
ifabettersolutionisfoundinordertochallenge
G
andimprovethissolution.Thisiterativesequencecomposedofthe3-phasesand
r
proceduresiscarriedoutuntiltheresidualproblemisnotempty,thegap
g
(
Ÿ

)
ispositive,andthenumberofiterationshasnotreached20.
365



ComputationalLinguistics
Volume41,Number3
5.Experiments
WeproposeatwofoldcomparisonoftheASAandLamSCPalgorithms:Onepartisfo-
cusedonthecoveringcostforalargeSCP,andtheotheronthestabilityofthesolutions.
Moreover,inordertoassesstheabilityandthebehaviorofbothalgorithmstoprocess
differentlinguisticdata,asetofexperimentsdealswithphonologicalattributes
(mainlycoveringco-occurrencesofphonemes)andasecondsetwithgrammatical
labels(mainlycoveringco-occurrencesofPOSlabels).
Bothattributetypes,ofteninvolvedinautomaticlinguisticprocessing,werechosen
becausetheirdistributionconsistsoffewhighlyfrequenteventsandnumerousrare
events.Ontheonehand,forTTStasks,thephonologicaltypecoveringisauseful
preliminarystepofthetextcorpusdesignbeforetherecordingstep.Inordertoproduce
thesignalcorrespondingtoarequestedsentence,theunitselectionenginerequiresat
leastoneinstanceofeachphone(or2-phone,dependingontheconcatenationprocess).
Becausetherecordingandthepost-recordingannotationprocessareexpensivetasks,
therecordinglengthofsuchacorpushastobeasshortaspossible.Ontheotherhand,
inordertotrainadependencyparser,thecoveringofPOSsequences
maybeusefulforincreasingthediversityofsyntaxpatterns.Becausethedependency
annotationisahighlyexpensivetask,theadaptationcorpustoannotateneedstobe
assmallaspossible,containingcharacteristicexamplesofthelexicalvariation
ratherthanfollowingthenaturaldistribution.Onecanexpectthatincreasingitsdiver-
sityofPOSsequencesmayleadtomorediversityinthesyntaxtrees.
Experimentsoncoveringco-occurrencesofphonemesarecarriedoutontwolarge
phonologicallyannotatedtextcorpora,andconsistofcoveringatleast
k
instancesof
eachphoneme,diphonemeuntil
n
-phoneme(i.e.,triphonemeif
n
=
3,diphonemeif
n
=
2).Thecost
c
j
ofthesentence
s
j
isgivenbyitsnumberofphones.Fromthispoint,
thiskindofSCPiscalledaﬁ
k
-coveringof
n
-phonemes.ﬂ
Acorpus,
Gutenberg
,iscomposedoftextsinEnglish,mainlyextractedfrom
novelsandshortstories.ThiscorpusistheproductionoftheGutenbergProject,pre-
sentedbyHart(2003),andhasbeenusedbyKominekandBlack(2003)todesignthe
speechcorpus
Arctic
.Asecondcorpus,inFrench,named
Le-Monde
,isextractedfrom
articlespublishedinthenewspaper
LeMonde
in1997.Table1summarizesthemain
featuresofbothcorpora.
Thephonologicalannotationofthe
Gutenberg
corpuscomesfromtheArctic/
Festvoxdatabase(seeKominekandBlack2003),andtheannotationofthe
Le-Monde
corpusisaby-productoftheNeologosproject,detailedbyKrstulovi
´
cetal.(2006).
Table1
Statisticsofthestudiedcorpora.
Le-MondeGutenberg
Numberofsentences
172,168
53,996
Numberofphonemes
35
57
Corpussize(numberofphones)
16,668,6091,539,735
Numberofdiphonemes
1,172
1,955
Numberofdiphones
16,496,441
1,485,739
Numberoftriphonemes
26,443
27,477
Numberoftriphones
16,324,2731,431,743
Sentencelengthmean(phones)&Std.Dev.96.81(60.46)28.51(10.52)
366



Barbotetal.
LargeLinguisticCorpusReduction
Table2
Statisticsof1-POSand2-POSoccurrences,corpus
Le-Monde
.Onaverage,asentencecontains
27
:
64POSwithastandarddeviationof16
:
45.
NumberofdistinctunitsNumberofoccurrences
1-POS
141
4,587,320
2-POS
6,716
4,421,371
Foreachcorpus,wehavecollectedeveryphoneme,diphoneme,triphoneme,and
theiroccurrencesineachsentencesoastotheset
U
ofunitstocoverandthe
matrix
A
.
A
isbuiltbycollectingonesentenceaftertheotherfollowingtheordering
insidethecorpus,andoneunitaftertheotherinsidethesentences.Afterthismatrix
translation,weobtaintwodescriptionandtwoindexThedescribes
thematrix
A
andthesecondonethecostvector
C
.Becauseofthelowmatrixdensity,we
havechosenasparserepresentationtosavespaceandcomputationtime:Forinstance,
the2-phoneme
Gutenberg
matrixisabout2.2%dense.Weonlystorethecellsof
A
that
haveanon-zerovaluesoastogetasparsematrix.Theindexaremadeforthe
correspondencebetweenthegeneralcoveringproblemandtheapplicationdomain.
TheimplementationismadeinC.Intermsofsoftwareengineering,ouralgorithms
areworkingonanSCPthatdoesnotdependontheapplicationdata.Forexample,
thereisnoinformationonwhattypesofunitsaretobecovered.Thealgorithmsonly
havethematrixofoccurrences
A
,thecostvector
C
,andtheconstraintvector
B
.Aset
oftranslation(fromapplicationdatatoSCPandfromSCPtoapplicationdata)is
builtbeforeeachcomputation.Asaconsequence,thereisnodifinaddressinga
differentsetoffeaturestocoveronthesameoronadifferentcorpus.
TostudytheachievementsofASAandLamSCPondifferenttypesofdata,we
havealsochosentoaddresstheﬁ
k
-coveringof
n
-POSﬂonthecorpus
Le-Monde
.The
grammaticalandsyntacticalanalysesareprocessedbytheSynapsedevelopment
analyzerpresentedinSynapse(2011).InordertoconsideraSCPwithasubstantial
numberofrequiredunits,averydetailedlevelofPOStagginghasbeenselected,
providing141distincttagsafteranalyzing
Le-Monde
.Forexample,thislevelprovides
tagslikeﬁDeterminer
male
singular
ArticleﬂorﬁNoun
female
singular,ﬂwhereasthe
simplestlevelgivesﬁDeterminerﬂorﬁNoun.ﬂThislatterlevelofdescriptionwould
havegivenonlyninedifferentPOStagsafteranalyzing
Le-Monde
.Themainassociated
statisticsaregiveninTable2.FortheseexperimentsofPOScovering,thecostofa
sentenceisasitsnumberofPOSoccurrences.
WeusedaPCwith2CPUs(E5320/1.86GHz/4cores/64bits)and32GBRAMforthe
phonologicalcoveringsandthePOScoveringswerecomputedusingaPCwith8CPUs
(IntelXeonX7550/2.00Ghz/8cores/64bits)and128GBRAM.Ourimplementationsdo
nottakeadvantageofanyparallelism.
Thefollowingsectionsdetailmorepreciselythedifferentexperimentsconducted
onFrenchorEnglish.
5.1PerformanceoftheAlgorithmsfor1-Coveringof2-PhonemesinFrench
TheaimofExperiment1istheassessmentoftheachievementsofbothalgorithms,ASA
andLamSCP,andtherobustnessoftheresultswhenthesentenceorderingis
367



ComputationalLinguistics
Volume41,Number3
inthecorpustoreduce.Indeed,oneofthedifofthegreedymethodologyisthat
thescorefunctionhasdiscretevaluesandseveralsentencescanyieldthesamescore.In
ourimplementation,amongthesentencesshowingthebestcurrentscore,theone
encounteredischosen.Wewouldliketomeasuretheofthisrandomchoiceon
thestabilityoftheresults.LamSCPusesgreedystrategiesbasedonLagrangiancosts.
BecausetheLagrangiancosts
c
j
(

)taketheSCPinitsentiretyintoaccountandare
continuousreal-valuefunctionsof

,theywouldbemoreselectivethanthesentence
costsusedbyASA.Asimplesolutionforevaluatingthestabilityconsistsofproceeding
withanimportantamountofexperimentsonthesameSCPbyrandomlymodifying
thesentencerankingin
A
.Experiment1measurestheimpactofthesepermutations
onthesolutionscomputedbybothalgorithms.TheconsideredSCPisthe1-covering
of2-phonemesonthecorpus
Le-Monde
.Consideringthecomputationtime(morethan
5hoursforLamSCP),only47instancesoftheSCPareconsidered,eachinstancecorre-
spondingtoarandomsentenceorderingin
Le-Monde
.The95%intervalsare
derivedusingthebootstrapmethod,concerningthecoveringcost,thenumberandthe
lengthofsentencesincoverings,thecomputationtime,andtheﬁdistanceﬂbetweenthe
coveringcostsandtheassociatedlowerbound
L
(
Ÿ

).
5.2StabilityoftheAlgorithmsforthe
k
-Coveringof2-PhonemesinEnglish
OneofthegoalsofExperiment2istocomparetheachievementsofbothalgorithmson
corpus
Gutenberg,
whichhasdifferentfeaturesfromtheonesof
Le-Monde
.Thesentences
in
Gutenberg
areshorteronaverageandtheassociatedvariationofsentencelengthis
lower.Furthermore,
Gutenberg
is10timessmallerthan
Le-Monde
.
Inordertocomparewiththeresultsofthepreviousexperimentdoneon
Le-Monde
,
weobserveda1-coveringof2-phonemesonthe
Gutenberg
corpus.Thesearch
spaceseemssmallerthaninExperiment1.AccordingtoTable1,
Le-Monde
iscom-
posedofmoresentencesthan
Gutenberg
.
Le-Monde
contains33,165,050occurrencesof
2-phonemesand
Gutenberg
only3,025,474.Moreover,thenumberofattributestocover
islower:1,2072-phonemesin
Le-Monde
and2,012in
Gutenberg
.Wecanalsonoticethat
2-phonemeshaveonlyoneoccurrencein
Le-Monde
andthatthetotalcostofthe
sentencescoveringtheserareunitsis751phones,whereasthisisthecasefor109
2-phonemesin
Gutenberg
andthetotalcostofthe104concernedsentencesis3,606
phones.Finally,ifweconsiderthedensityofmatrix
A
,8.4%ofthecellsarenon-empty
forExperiment1and2.2%forExperiment2.AswithExperiment1,thesentenceorder-
ingin
Gutenberg
hasbeenrandomlytoproduce60instancesoftheSCPand
similarsolutionstatisticshavebeencomputedforbothalgorithms.
Thesecondobjectiveistotestandcomparetheabilityoftwoalgorithmstodeal
withtheconstraintsofmulti-representation.Forthis,weapplythesamemethodology
tothe
k
-coveringof2-phonemesin
Gutenberg
,for
k
from2to5.Wenotethatforthesame
originalcorpustoreduce,thesizeofthesearchspacedecreaseswhen
k
increases.These
differentSCPsenableustocomparetheperformanceoftwoalgorithmsdependingon
thesizeofthesearchspace.
5.31-Coveringof3-PhonemesinEnglish
InExperiment3,theaimistoobservethebehaviorofbothalgorithmsonverycon-
strainedproblems.Forthis,westudytheirabilitytotreatacoveringof3-phonemes.We
trytoassesstheimpactonthesolutionfeaturesandonthestabilityofsuchanincrease
inthenumberofattributestocoverwithmanyrareevents.Soastocomputestatistics
368



Barbotetal.
LargeLinguisticCorpusReduction
onthe1-coveringof3-phonemes,aninstanceof
Gutenberg
hasbeenproposedtoASA
andtoLamSCP.Thisinstancecounts29,489unitstocoverandthedensityofmatrix
A
is0.24%.Thecomputationtimeisnearly5daysforLamSCPandwethenchose
tocarryout35instancesofthe1-coveringof3-phonemeson
Gutenberg
.Additionally,
itisinterestingtocomparetheseresultswiththoseofExperiment2concerningthe
1-coveringof2-phonemesonthesamecorpus,whichcorrespondstoalargersearch
space.
5.41-Coveringof3-PhonemesinFrench
InordertopursuetheobjectivesetoutinthedescriptionofExperiment3,thatis,the
abilityofalgorithmstotreatwithnumerousconstraintsandaheavy-taildistribution
ofunits,Experiment4consistedoftestingbothalgorithmsonthe1-coveringof3-
phonemeson
Le-Monde
.Thesearchspaceseemslargerthaninthepreviousexperiment.
Letusrecallthat
Le-Monde
contains3.18timesmoresentencesthan
Gutenberg
andit
counts27,650unitstocover.Furthermore,
Gutenberg
contains5,0003-phonemeswith
onlyoneoccurrence,whichrequirestheselectionof4,180sentenceswithatotallength
equalto137,714phones,whereas
Le-Monde
contains2,274rare3-phonemesscattered
in2,107sentencesmeasuringatotalof283,208phones.Theassociatedmatrixdensity
is0.69%.Becausethecomputationofainstancetakesmorethan8days,wehave
limitedthenumberofinstancesto30forthisSCP.
5.5
k
-Coveringof1-POSand2-POSinFrench
ThemaingoalofExperiment5istostudythebehaviorofbothalgorithms,ASAand
LamSCP,dealingwithanotherkindoflinguisticattribute,andtocomparethiswith
thepreviousexperiments.Toachievethisgoal,weconsiderPOSattributesandthe
associatedSCP:1-and5-coveringsofPOS,1-and5-coveringsof2-POSon
Le-Monde
.Indeed,wecanobserveinTable2thattheglobalstatisticsofPOStags
in
Le-Monde
arequitedifferentfromtheirphonologicalcounterpartssummarizedin
Table1.Inparticular,thedensityofmatrix
A
is11.03%fora1-POScoveringand0.57%
fora2-POScovering.Also,thesearchspacesizeseemstodecreasewhenconsidering
successivelythemono-andthemulti-coveringsof1-POS,andthemono-andthemulti-
coveringsof2-POS,permittingustocomparethemwiththeresultscomingfromthe
experimentsonphonologicalcoverings.Weevaluatethestabilitybycomputing50
randomlymixedversionsofthecorpus
Le-Monde
.
6.ResultsandDiscussion
Inthissection,theresultsoftheexperimentsdescribedinSection5areprovidedand
discussed.Asaconsequence,theorganizationofthissectionandSection5aresimilar.
6.1PerformanceofAlgorithmsfor1-Coveringof2-PhonemesinFrench
Table3showsthemainresultsofExperiment1,concerningthe1-coveringof2-
phonemesfromthecorpus
Le-Monde
.Symbol

indicatesthatthementionedvalue
correspondstoa95%interval,calculatedusingthebootstrapmethodfrom
369



ComputationalLinguistics
Volume41,Number3
Table3
Statisticsofthesolutionsof1-coveringof2-phonemescomputedbyASAandLamSCPfrom
Le-Monde
.ThelastcolumnrepresentsthebestlowerboundsfoundbyLamSCP.
Experiment1:1-coveringof2-phonemes,corpus
Le-Monde
ASA
LamSCP
L
(
Ÿ

)
Coveringsize(phones)
8,555

22
7,786

4
7,689

5
ReductionraterelativetoASA(%)
Œ9.00

0.20Œ10.13

0.19
Coveringsize[min;max]
[8,447;8,669][7,767;7,829][7,649;7,715]
CoveringsizeStd.Dev.
57.40
13.01
13.83
Sentencenumber
335.73

1.69268.76

1.08
Œ
Sentencelength
25.48

0.1028.97

0.12
Œ
TimeCPU(seconds)
51

0
20,478

508
Œ
the47instancesoftheSCP.Inordertocovereachofthe1,2072-phonemesof
Le-Monde
,
ASAdrasticallyreducesthesizeoftheinitialcorpusby99.94%(

0.00).However,on
average,LamSCPcalculatesa9.00%shortercovering.Thelowerbound
L
(
Ÿ

)forthe
optimalcoveringcostis7,689

5phones.
L
(
Ÿ

)isnotaminimumvalueandmay
notcorrespondtothecostofarealcovering.Becausethislowerboundisupdatedall
alongtheexecutionofLamSCP,wedonotmentionacalculationtimeforthis
result.
ForoneinstanceoftheSCP,let
CX

ASA
bethesizeofthesolutiongivenbyASA.The
quantity
˝
ASA
=
1

L
(
Ÿ

)
=
CX

ASA
indicatesthattheoptimumsolutiontoSCPisatmost
˝
ASA
timesshorterthanthecoveringcalculatedbyASA.Itcanbeobservedthatthe
optimalsolutionisatmost10.13%(

0.19)shorterthantheoneyieldedbyASAandat
most1.24%(

0.08)shorterthanthesolutionyieldedbyLamSCP.Thesolutionsobtained
byLamSCPandtheoptimalsolutiontotheSCParethereforeveryclose.Considered
amongthe47instancesoftheSCPthebestsolutionsyieldedbyASA(8,447phones)
andLamSCP(7,767phones),LamSCPis8.75%betterthanASAintermsofcovering
costs,whilethebestlowerboundfortheSCPis7,715phones,only0.67%(respectively,
8.66%)shorterthanthebestcoveringbyLamSCP(respectively,ASA).
Theaveragelengthofthesentencesselectedbybothalgorithmsisfarbelowthe
averagelengthofthesentencesinthecorpus(96.81phones).LamSCPtendstochoose
sentencesthatareslightlylongerthanASA,withanaverage28.97(

0.12)phones
comparedwith25.48(

0.10)phones.Moreover,ASAselectsonaverage335.73(

1.69)
sentencespersolution,about24.91%morethanLamSCP,whichselects268.76(

1.08)
sentencesonaverage.ThisseemstoindicatethatLamSCPmakesfewerlocalchoices
thanASA.Thishypothesiscanalsobevalidatedthroughtheanalysisofthevariability
oftheresults.TherelativevariationofthecoveringcostscalculatedbyLamSCPis
13.01/7,786=0.16%,and57.40/8,555=0.67%byASA;thatistosayastabilityofthe
costs4timesgreaterforthesolutionsyieldedbyLamSCPthanforASA.Moreover,the
solutionsarecomposedofaverystablenumberofsentences:Theassociatedrelative
standarddeviationis5.31/335.73=1.58%forthe47instancessolvedbyASA,and
3.85/268.76=1.43%fortheinstancessolvedbyLamSCP.Itturnsoutthattheresults
ofbothalgorithmsareverystablewhentheorderofthesentencesisinthe
originalcorpus.
Finally,concerningcomputationtime,theresolutionofaninstanceoftheSCP
lastsonaverage5hr41min18sec(

8min28sec)forLamSCPversus51sec
370



Barbotetal.
LargeLinguisticCorpusReduction
(

0sec)forASA.Onaverageoverthe47instances,LamSCPtakes390(

9)timesas
longasASA.
6.2StabilityoftheAlgorithmsfor
k
-Coveringof2-PhonemesinEnglish
TheconsideredSCPconsistsofcoveringatleast
k
timeseachofthe2,0122-phonemesof
the
Gutenberg
corpus,with
k
varyingfrom1to5.TheresultsaresummarizedinTable4.
ForallinstancesoftheseSCPs,ithasbeenobservedthatLamSCPcomputesshorter
coveringsthanASA.However,thatadvantagediminishesas
k
grows:Thecostadvan-
tageofferedbyLamSCPcomparedwithASAdecreasesfrom9.73%(

0.13)for
k
=
1
to4.50%(

0.04)for
k
=
5.Also,thesolutionsobtainedfromASAandLamSCPseem
togetclosertotheoptimalsolutionas
k
rises.Thecorrespondingesarepresented
inTable5:Forinstance,theoptimalsolutionisatmost0.75%(

0.02)shorterthanthat
obtainedbyLamSCPfor
k
=
1,and0.27%(

0.00)for
k
=
5.
Table4
Experiment2:Statisticsbasedon60instancesofa
k
-coveringof2-phonemesfrom
Gutenberg
.
ASA
LamSCP
L
(
Ÿ

)
k
=
1
Coveringsize(phones)
14,909

2313,458

2
13,357

2
ReductionraterelativetoASA(%)
Œ9.73

0.13Œ10.41

0.12
Coveringsize[min;max]
[14,700;15,107][13,441;13,485][13,341;13,378]
Sentencenumber
623.75

1.61520.85

0.57
Œ
Sentencelength
23.90

0.0425.83

0.02
Œ
TimeCPU(seconds)
8

0
2,711

68
Œ
k
=
2
Coveringsize(phones)
27,518

2525,604

4
25,413

1
ReductionraterelativetoASA(%)
Œ6.94

0.09Œ7.65

0.08
Coveringsize[min;max]
[27,278;27,727][25,576;25,642][25,400;25,430]
Sentencenumber
1080.19

1.60948.30

0.68
Œ
Sentencelength
25.48

0.0226.99

0.01
Œ
TimeCPU(seconds)
14

0
3,931

68
Œ
k
=
3
Coveringsize(phones)
39,319

3436,985

4
36,746

2
ReductionraterelativetoASA(%)
Œ5.92

0.07Œ6.53

0.07
Coveringsize[min;max]
[39,091;39,580][36,946;37,028][36,724;36,769]
Sentencenumber
1,507.75

1.751,359.28

1.13
Œ
Sentencelength
26.07

0.0127.20

0.02
Œ
TimeCPU(seconds)
20

0
5,974

114
Œ
k
=
4
Coveringsize(phones)
50,491

2848,023

4
47,820

4
ReductionraterelativetoASA(%)
Œ4.89

0.06Œ5.29

0.05
Coveringsize[min;max]
[50,300;50,753][47,987;48,075][47,780;47,842]
Sentencenumber
1,908.10

1.731,744.40

1.35
Œ
Sentencelength
26.46

0.0127.53

0.01
Œ
TimeCPU(seconds)
28

0
6,589

239
Œ
k
=
5
Coveringsize(phones)
61,375

30
58,610

4
58,447

2
ReductionraterelativetoASA(%)
Œ4.50

0.04Œ4.76

0.04
Coveringsize[min;max]
[61,137;61,683][58,582;58,645][58,420;58,465]
Sentencenumber
2,288.91

1.882,101.62

0.84
Œ
Sentencelength
26.81

0.0127.88

0.01
Œ
TimeCPU(seconds)
39

0
7,599

344
Œ
371



ComputationalLinguistics
Volume41,Number3
Table5
Ratios
˝
LamSCP
and
˝
ASA
forthe
k
-coveringof2-phonemesfrom
Gutenberg
.
k
1
2
3
4
5
˝
LamSCP
0.75%
0.74%
0.64%
0.42%
0.27%
(

0.02)(

0.01)(

0.01)(

0.01)(

0.00)
˝
ASA
10.41%
7.65%
6.53%
5.29%
4.76%
(

0.12)(

0.08)(

0.07)(

0.05)(

0.04)
Becausethesearchareadiminishesas
k
increases,itmaybeobservedthatthe
algorithmstendtobemorestable.Thisistruebothforthesizeofthesolutions,aswellas
forthenumberofsentencesthatthem.Table6representsthevariationofthesize
ofthesolutionsasafunctionof
k
.Thisvariationiscalculatedasfollows:Foragiven
k
numberandagivenalgorithm,thestandarddeviationofthesizeofthe
k
-covering
computedbythatalgorithmisdividedbytheaveragesizeofthesecoverings.Thus,it
canbenotedthatLamSCPoffersastability4to8timessuperiortoASAconcerningthe
sizeofthecoverings.Asforthenumberofsentences,therelativestandarddeviation
similarlydecreasesfrom0.97%to0.28%when
k
increasesfrom1to5forASAsolutions,
andfrom0.42%to0.15%forLamSCPones.
Onecannotethattheincreaseoftheminimalnumber
k
ofinstancesofeachunit
tocoverleadstoaselection,byLamSCPandASA,oflongersentencesonaverage.The
averagelengthofthesentencespickedfora1-coveringwasquitelow.Astheconstraints
increasealongwith
k
,itonlyseemsnaturalthatthealgorithmstendtoselectlonger
sentences,asshortersentencesnolongercontainenoughoccurrencesof2-phonemes.
Moreover,asdescribedinSection2,whentheminimalnumber
b
i
ofaunit
u
i
demanded
inthecoveringexceedsthenumberofinstancesofthatunitintheinitialcorpus,all
sentencescontaininginstancesof
u
i
intheinitialcorpusareselected,and
b
i
issetto
(
A
1
R
n
)
i
.Thus,as
k
increases,thealgorithmtendstoselectmoreandmoresentences,
andtheirlengthtendstowardstheaveragevalueoverthewholecorpus,whichis
28.51phonesfor
Gutenberg
.
Asforcomputationtime,althoughitincreasesas
k
grows,becauseoftheincreasing
numberofconstraintstoupdate,theratiobetweenthecomputationtimeofLamSCP
andASAtendstodiminish,asshowninTable7.Thistendencymayanexplana-
tioninthefactthatthesearchspacediminishesas
k
increases,whichcausesalesser
numberofselectedsentencestobequestionedduringthe3-phaseiterationofLamSCP.
Also,wenoticethattheaveragecomputationtimeofthetwoalgorithmsisgreater
inExperiment1,owingtoagreaternumberofsentencesincorpus
Le-Monde
anda
higherdensityofmatrix
A
.Moreover,theratiobetweenthecomputationtimesof
Table6
Relativestandarddeviationofsolutioncostforbothalgorithmsofa
k
-coveringof2-phonemes
from
Gutenberg
.
k
1
2
3
4
5
LamSCP0.07%0.05%0.05%0.04%0.02%
ASA
0.57%0.37%0.29%0.18%0.17%
372



Barbotetal.
LargeLinguisticCorpusReduction
Table7
ComputationtimeratiobetweenLamSCPandASAfora
k
-coveringof2-phonemesfrom
Gutenberg
.
k
1
2
3
4
5
TimeLamSCP/TimeASA333(

7)280(

5)292(

6)218(

9)194(

8)
LamSCPandASAdecreasesbetweenExperiments1and2,goingfrom390(

9)to
333(

7).Again,thiscanbeexplainedbythediminishingofthesearchspace.
For
k
=
1,theadvantageofferedbyLamSCPonthecoveringcostscomparedwith
ASAisslightlyhigherthanthatobservedinExperiment1:9.73%(

0.13)inthiscase,
versus9.00%(

0.20)inthepreviousexperiment.Thisseemstocontradicttheideathat
theperformanceofLamSCPimprovesasthesearchareabecomeswider.However,
thedistributionsoftheunitstocoverin
Gutenberg
and
Le-Monde
aredifferent,andthe
variationonthelengthofthesentencesin
Le-Monde
isveryhigh,whichmayaccount
forthisslightdifferenceintermsofgain.Notethatthesizeofthecalculatedcoverings
andthelowervalue
L
(
Ÿ

)arecloserintheexperimentcarriedouton
Gutenberg
.Itis
difhowever,toperformfurthercomparisonswithExperiment1regardingthe
ﬁdistanceﬁbetweenthecostsofthesolutionscomputedbythesealgorithms,andthe
optimalcoveringcost,giventhatthequalityofthelowerboundcannotbeevaluated.
ThegaininstabilityofferedbyLamSCP,bothforthecostsofthesolutionsorthe
numberofsentences,ismoreimportantthanthatnoticedduringthepreviousexper-
iment.Wethinkthattheincreaseisduetoamorerestrictedsearchspace,andless
variabilityofthelengthofthesentencesincorpus
Gutenberg
,whichmaybeobservedin
Table1.
6.31-Coveringof3-PhonemesinEnglish
Table8sumsupthemainresultsofExperiment3,where35instancesof1-covering
of3-phonemesfrom
Gutenberg
wereprocessed.Accordingtothe
L
(
Ÿ

)values,covering
all3-phonemesrequiresasolutionsizegreaterthanorequalto226,635phones.On
average,thesolutionmeasures227,360

12phonesusingLamSCP,and236,828

94
Table8
Experiment3:Statisticsbasedon35experimentsofa1-coveringof3-phonemesfrom
Gutenberg
.
ASA
LamSCP
L
(
Ÿ

)
Coveringsize(phones)
236,828

94
227,360

12
226,559

8
ReductionraterelativetoASA(%)
Œ3.99

0.04
Œ4.33

0.04
Coveringsize[min;max]
[236,075;237,615][227,317;227,425][226,518;226,635]
CoveringsizeStd.Dev.
301.75
33.12
23.67
Sentencenumber
8,005.20

5.027,606.77

2.14
Œ
Sentencelength
29.88

0.00
29.58

0.01
Œ
TimeCPU(seconds)
2,834

20
371,501

10
Œ
373



ComputationalLinguistics
Volume41,Number3
phonesusingASA.Theoptimalcoveringisatmost0.35%(

0.00)shorterthansolutions
derivedbyLamSCPand4.33%(

0.04)shorterthantheonesderivedbyASA.Wecan
thenobservethatbothalgorithmsmanagetocomputesolutionswithclosesizeswhen
scalinguptherequiredattributeset.Thesolutionsareverystable,evenmorethan
inExperiment2:Therelativevariationoftheirsizeis0.12%forASAand0.01%for
LamSCP;therelativevariationoftheirsentencenumberis0.17%forASAand0.07%
forLamSCP.Thisincreaseofstabilityisduetoasmallersearchspaceandtheincrease
ofthenumberofrareunitsrequired,whichalsocompelsthealgorithmstoselecta
highernumberofinevitablesentencesforalltheinstancesoftheSCP.Furthermore,
thedecreaseoftheratiobetweenthecomputationtimeofLamSCPandASAfrom332
forthe1-coveringof2-phonemesto130fortheoneof3-phonemeson
Gutenberg
may
thisidea,whichhasalsobeenputforwardinExperiment2.
Concerningthelengthoftheselectedsentencesbybothalgorithms,itisgreater
thantheoneforthe5-coveringof2-phonemes,observedinExperiment2,andslightly
deviatesfromtheaveragesentencelengthforthewholecorpus.Consequently,itturns
outthatcoveringlongerandgenerallyrarerunitsinvolvesaselectionoflongersen-
tences.Thisisbythefactthatthesentencesof
Gutenberg
coveringunitswith
asingleoccurrencein
Gutenberg
representmorethanhalfthesizeofthesolutionsand
arecomposedof33phonesonaverage.
6.41-Coveringof3-PhonemesinFrench
Inthissection,weanalyzetheresultsofExperiment4,the30instancesofthe1-
coveringof3-phonemesfrom
Le-Monde
carriedoutbyASAandLamSCP.Theresults
aregiveninTable9.First,althoughthemainfeaturesof
Le-Monde
and
Gutenberg
are
different,noticethattheclosenessbetweenthesizeofcoveringscalculatedbyboth
algorithmsandthelowerbound
L
(
Ÿ

)iscomparabletotheoneobservedinExperiment
3.Indeed,theoptimalcoveringsizeisatmost0.48%(

0
:
03)and4.35%(

0
:
05)shorter
thanthesolutionsizederivedbyLamSCPandASA,respectively.Similarly,thesize
ofsolutionsandthenumberofselectedsentencesareasstableasthoseobservedin
thepreviousexperiment:Thesolutionlengthvariesfrom0.01%forLamSCPto0.10%
forASAandthenumberofsentencesabout0.16%forASAand0.10%for
LamSCP.
AsforthecomparisonwiththeresultsofExperiment1(1-coveringof2-phonemes
from
Le-Monde
),themaintrendsaresimilartotheonesobservedforthetransition
fromthe1-coveringof2-phonemestothe1-coveringof3-phonemesfrom
Gutenberg
.
Table9
Experiment4:Statisticsbasedon30experimentsofa1-coveringof3-phonemesfrom
Le-Monde
.
ASA
LamSCP
L
(
Ÿ

)
Coveringsize(phones)
620,434

222596,323

27
593,417

127
ReductionraterelativetoASA(%)
Œ3.89

0.03
Œ4.35

0.04
Coveringsize[min;max]
[619,319;622,201][596,190;596,486][592,640;594,250]
CoveringsizeStd.Dev.
633.03
88.59
391.82
Sentencenumber
6,969.10

4.636,436.76

2.56
Œ
Sentencelength
89.02

0.0392.64

0.03
Œ
TimeCPU(seconds)
7,845

78660,928

24,355
Œ
374



Barbotetal.
LargeLinguisticCorpusReduction
However,inExperiment4,theaverageselectedsentencelengthhasmarkedly
increased,approachingthemeanvalueonthewholecorpus:89.02(

0
:
03)forASA
and92.64(

0
:
03)forLamSCP,whereasinExperiment1thesevaluesare,respectively,
25.48(

0
:
10)and28.97(

0
:
12).WehavealreadyobservedinExperiment3that
coveringlongerunitsincreasesthelengthofselectedsentencesbutthishighamplitude
seemstobeinherenttothedesignofcorpus
Le-Monde
.Furthermore,noticethatthe
1-coveringsof2-phonemesfrom
Le-Monde
arealmosthalfassmallastheonesfrom
Gutenberg,
whereasthe1-coveringsof3-phonemesfrom
Le-Monde
arebetweentwice
andthreetimeslongerthantheonesfrom
Gutenberg
.Thisisduetothefactthatthe
3-phonemeswithasingleinstancein
Le-Monde
areveryscatteredinlongsentences
(theirlengthmeanisabout134phones),andtheseindispensablesentencesrepresent
nearlyhalfthesizeofthesolutions.Theothersentencesofthesolutionsarearound
70phoneslong.Lastly,theratiobetweenthecomputationtimeofbothalgorithmsis
about84,whichissmallerthantheratiospreviouslyobserved,butthisSCPisthemost
timeconsuming:2hr10minforASAandmorethan7daysforLamSCP.
6.5
k
-Coveringof1-POSand2-POSinFrench
Table10sumsupthemainresultsofExperiment5,dealingwiththe1-and5-coverings
of1-POSand2-POS.ForalltheseSCP,LamSCPproducessmallercoverings,com-
posedoflongersentences,thanthecoveringsobtainedwithASA.Whenthesearch
spacediminishes,therelativeﬁdistanceﬂbetweenthesizeofsolutionsprovidedby
bothalgorithmsdecreases,aswellasbetweenthelowerbound
L
(
Ÿ

)andthesizeof
solutionsobtainedbyASA.Thesetrendswerealsoobservedintheearlierexperiments.
Inparticular,asforthe1-coveringof1-POS,notonlydoesLamSCPprovide10.06%
(

0.00)shortersolutionsthanASA,butitssolutionsareoptimalforall50instances
ofthisSCP.Indeed,thelowerboundvaluevariesfrom482.51to482.87occurrencesof
1-POSwhileallthesolutionsgivenbyLamSCParemadeofexactly483occurrences
ofPOS.Fortheother
k
-coveringsof
n
-POS,theoptimalsolutionisatworst0.39%
(

0.02)shorterthanthecoveringgivenbyLamSCPfor(
k
,
n
)
=
(5,1),0.11%(

0.00)for
(
k
,
n
)
=
(1,2),and0.22%(

0.00)for(
k
,
n
)
=
(5,2).ThesolutionsobtainedbyASAorby
LamSCPareverystable.Forexample,therelativestandarddeviationofnumberofPOS
inacoveringsolutionvariesfrom0.00%to1.23%forASA,andfrom0.00%to0.04%
forLamSCP.
Aspreviouslyobservedforbothalgorithms,theircomputationtimesgrowwhen
thenumberofrequiredcoveringfeaturesincreases.However,theratiobetweenthe
computationtimeofLamSCPandASAdoesnotbehaveasinExperiment2(seeTable7):
Forthe
k
-coveringof1-POS,thisratioincreasesfrom290(

11)to657(

43)when
k
goes
from1to5,andforthe
k
-coveringof2-POS,itincreasesfrom75(

5)to108(

6).
7.EvaluationonaText-to-SpeechSynthesisSystem
Intheprevioussections,differentalgorithmsdealingwithcorpusreductionwerein-
troducedandstudied.Theproposedexperimentsmainlyevaluatetheeffectsofthese
algorithmsintermsofcorpusreductionbutnotaccordingtoapracticaltask.This
sectionproposesanexperimenttoassesstheimpactofthecorpusreductiononaunit
selectionspeechsynthesissystem.
AsexplainedinSection1,acorpusreductionforaTTSsystemisatrade-offbetween
minimizingtherecordingandpost-processingtimetobuildthespeechcorpusand
375



ComputationalLinguistics
Volume41,Number3
Table10
Statisticsof50instancesofa
k
-coveringof
n
-POSfrom
Le-Monde
.
Experiment5:
k
-coveringof
n
-POS,corpus
Le-Monde
n
=
1
andk
=
1
ASA
LamSCP
L
(
Ÿ

)
Coveringsize(POS)
537.70

1.78
483.00

0.00482.68

0.02
ReductionraterelativetoASA(%)
Œ10.06

0.00Œ10.17

0.00
Coveringsize[min;max]
[524;552]
[483;483][482.51;482.87]
CoveringsizeStd.Dev.
6.64
0
0.08
Sentencenumber
72.07

0.51
60.65

0.31
Œ
Sentencelength
7.46

0.04
7.97

0.03
Œ
TimeCPU(seconds)
2

0
735

25
Œ
n
=
1
andk
=
5
Coveringsize(POS)
2,659

4
2,502

0
2,492

0
ReductionraterelativetoASA(%)
Œ5.90

0.11
Œ6.27

0.09
Coveringsize[min;max]
[2,628;2,682]
[2,501;2,505][2,482;2,493]
CoveringsizeStd.Dev.
12.42
1.11
1.85
Sentencenumber
285.00

0.82
238.91

0.65
Œ
Sentencelength
9.33

0.02
10.47

0.02
Œ
TimeCPU(seconds)
5

0
2,712

162
Œ
n
=
2
andk
=
1
Coveringsize(POS)
77,022

18
75,281

1
75,192

2
ReductionraterelativetoASA(%)
Œ2.25

0.02
Œ2.37

0.02
Coveringsize[min;max]
[76,913;77,187][75,273;75,297][75,176;75,220]
CoveringsizeStd.Dev.
63.33
3.44
8.26
Sentencenumber
3,288.22

1.483,120.89

0.91
Œ
Sentencelength
23.42

0.00
24.12

0.00
Œ
TimeCPU(seconds)
175

0
13,095

881
Œ
n
=
2
andk
=
5
Coveringsize(POS)
281,532

17
278,114

5
277,497

4
ReductionraterelativetoASA(%)
Œ1.21

0.00
Œ1.43

0.00
Coveringsize[min;max]
[281,362;281,652][278,084;278,152][277,459;277,524]
CoveringsizeStd.Dev.
72.09
15.27
14.33
Sentencenumber
10,379.67

1.6710,034.96

2.66
Œ
Sentencelength
27.12

0.00
27.71

0.00
Œ
TimeCPU(seconds)
2,863

113309,095

901
Œ
keepingthehighestphonologicalrichnessofthecorpustoensurethequalityofthe
syntheticspeech.Thegoalofthisexperimentistomeasurethistrade-offbyevalu-
atingthequalityofthesameTTSsystemfedwithdifferentspeechcorporauttered
bythesamespeaker.Notethattheintrinsicqualityofthissystemisnotthepurpose
here.
Firstly,abriefpresentationofastate-of-the-artunitselectionŒbasedTTSsystem
isproposedinSection7.1.ThelinguisticparametersusedbytheTTSsystemarede-
tailedbecausetheyarelinkedtotherequiredfeaturesinthereductionstage.InSec-
tion7.2,corporausedintheexperimentareintroduced.Theattributestocoverandthe
376



Barbotetal.
LargeLinguisticCorpusReduction
methodologyofevaluationaredescribedinSection7.3;theresultsaregivenanddis-
cussedinSection7.4.
7.1Text-to-SpeechSystem
Forthisexperiment,astate-of-the-artunitselectionŒbasedTTSsystemisusedtopro-
duceanacousticsignalfromaninputtext.Alinguisticfrontendprocessesthetextto
extractfeaturestakenintoaccountbythealgorithmthatselectssegmentsinaspeech
corpus(seeBo
¨
effardandd'Alessandro2012).
TheinputtextisconvertedintoasequenceofphonemesusingaFrenchphonetizer
proposedbyB
´
echet(2001).Non-speechsoundlabelscanbeaddedtothissequence
(silences,breaths,para-verbalevents,etc.).Avectoroffeaturesisasfollows:
1.
Thephoneornon-speechsoundlabel
2.
Isthedescribedsegmentanon-speechsound?
3.
Isthephoneintheonsetofthesyllable?
4.
Isthephoneinthecodaofthesyllable?
5.
Isthephoneinthelastsyllableofitssyntagm?
6.
Isthecurrentsyllableattheendofaword?
7.
Isthecurrentsyllableatthebeginningofaword?
ExtractionoffeaturesisdoneusingtheR
OOTS
toolkitdescribedinBo
¨
effardetal.
(2012).Theunitselectionprocessaimstoassociateasignalsegmentfromthespeech
corpustoeachvectoroffeaturescomputedfromtheinputtext.Thisisperformed
intwosteps.Inthestep,foreachunit,asetofcandidatesthatmatchthesame
featuresareextractedfromthespeechcorpus.Inthesecondstep,givenallcandidates,
thebestpathissearchedusinganoptimizationalgorithmsoastoproducethesequence
ofspeechunits.Thealgorithmtriestominimizethreesub-costs,commonlyusedinunit
selectionbasedTTSsystems,whicharespectraldiscrepanciesbasedonMFCCdistance,
amplitude,andf0distances.
7.2Corpora
Twocorporaareusedinthisexperiment.Theone,
Learningcorpus
,isanannotated
acousticcorpususedtoprovidespeechdatafortheTTSengine.Itisanexpressive
corpusinFrench,spokenbyamalespeakerreading
Albertinedisparue
,anexcerptfrom
˚
A
larecherchedutempsperdu
byMarcelProust.Thecorpusiscomposedof3,138sentences
automaticallyannotatedusingaprocessdescribedinBo
¨
effardetal.(2012).Theoverall
lengthofthespeechcorpusis9hr57min.WhencreatingavoiceforaunitselectionŒ
basedTTSsystem,longsentencesaregenerallyremovedorsplitintosyntagmgroups
inordertohelpthespeaker.
Asecondcorpus,named
Testcorpus
,isatextcorpusthatissynthesizedandused
inthelisteningexperiment.Itiscomposedof30shortsentencesrandomlyextracted
fromaphoneticallybalancedcorpusinFrench,proposedbyCombescure(1981).The
useofacorpuswithadifferentlinguisticstyleminimizesthebiasintroducedbythe
learningcorpus.
StatisticsaregiveninTable11.
377



ComputationalLinguistics
Volume41,Number3
Table11
Characteristicsofthetwocorpora.
LearningcorpusTestcorpus
Numberofsyntagms
19,587
30
Numberoflabels
36
36
Corpussize(labels)
392,865
813
Numberof2-phoneme
1,033
317
Numberofdiphones
373,278
787
Syntagmlengthmean(phones)&Std.Dev.20.1(11.9)27.1(5.6)
7.3Methodology
Forthisexperiment,tworeducedcorporaareevaluated.Theyarebuiltbyreducing
thefulllearningcorpususingthetwodifferentalgorithmspresentedintheprevious
sections:ASAandLamSCP.
AsdescribedinSection7.1,theunitselectionprocessofthespeechsynthesissystem
isbasedonasetofphonologicalattributes.Itseemsnaturaltotrytocoverfeaturesthat
rthevariabilityoftheseattributes.Forthisexperiment,algorithmsmustcoverall
theunitsatleastonce,whereaunitisdescribedbythefollowing:
r
Itslabel,thatis,oneofthe35phonemesoranon-speechsoundlabel
r
Thestructureofthesyllablethatcontainsthephoneme,ifitisavowel
r
Thepositionoftheassociatedsyllableintheword(start,middle,orend)
r
ABooleanindicatediftheassociatedsyllableisattheendofasyntagm
Thefeatureextractionisperformedbythesamesetoftoolsusedbythespeechsynthesis
engine.Giventhissetoffeatures,thelearningcorpuscontains1,497classesofunits.The
costfunctiontominimizebythereductionalgorithmsisthetotallengthofthesetofthe
selectedsyntagms,inphones.
Twospeechsynthesissystemsareextractingthespeechunitstoconcate-
natefromthecoveringsprovidedbyLamSCPandASA.Twoothersystemsareaddedas
baselines.First,asystemnamed
Full
,builtwiththewholelearningcorpus,isusedasan
upperbound.Second,asystemnamed
Random
usesarandomreductionofthe
Learning
corpus
asapoolcorpusofspeechunits.Thisreductionisdonebyrandomlyselecting
sentencesfromthewholelearningcorpusuntilthesizeofthecoveringobtainedby
LamSCPisreached.
Random
isusedasalowerbound.
Whereastheoptimizationefismeasuredbystatisticsonthereducedcor-
pora,thequalityofthesynthesizedspeechsignalsisevaluatedbyalisteningtest.
TheprotocolisbasedonaM
USHRA
test,presentedinITU-R(2003),whereforevery
sentenceof
Testcorpus
,thesignalssynthesizedbythefoursystemsarepresentedto
eachtesterinarandomorder.Ifasystemisnotabletoproduceasignalforare-
questedsentence(becauseofamissing2-phoneinthepoolcorpus),anemptysignal
ispresented.TennativeFrenchtesters(fournaivesandsixexperts)areaskedtoeval-
uatetheoverallqualityofthestimuliandtogiveamarkfrom0to100(bystepsof
5points).
378



Barbotetal.
LargeLinguisticCorpusReduction
Table12
StatisticsaboutthereducedcorporacomputedbyASAandLamSCPfrom
Learning
corpus.
ThelastcolumnconcernsthebestlowerboundfoundbyLamSCP.
Speechsynthesisexperiment:corpusreduction
ASALamSCP
L
(
Ÿ

)
Coveringsize(phones)
36,53835,68135,655
Reductionraterelativetothefullcorpus(%)
Œ90.70Œ90.92Œ90.92
ReductionraterelativetoASA(%)
Œ2.35Œ2.42
Numberofsyntagms
2,191
2,119Œ
7.4ResultsandDiscussion
The
Learningcorpus
composedof19,587syntagmsisreducedusingASAand
LamSCP.StatisticsoftheresultingsolutionsaresummarizedinTable12.Thecovering
ofthe1,497constraintsdividestheinputcorpussizebyalmost10andreducesthe
10hoursofspeechtoaround1hr20min.Asforthepreviousexperiments,eventhough
differentkindsoffeaturesaremixed(phonemes,syllablestructures,positioninaword
orasyntagm)theASAalgorithmproducesasolutionclosetotheoptimalone.However,
LamSCPisagainslightlybetterintermsofcoveringsize.
Forthemeasureoftheacousticimpactofcorpusreduction,thelisteningtestresults
arepresentedinTable13fortheaveragemarksandinTable14fortheaverageranks.
Notethatwithoutanaturalspeechreferenceduringthetest,themarksshouldnotbe
seenasanabsolutescore.EveniftheLamSCPcorpusisslightlysmallerthantheone
fromASA,theacousticqualityofbothsystemsiscomparableaccordingtothetesters
(withaslightadvantagefortheLamSCPcorpus).Incomparisonwiththebaseline,
whichusesthewholelearningcorpus,theacousticdegradationisThis
illustrateswellthetrade-offbetweencorpussizeandspeechquality:Fora90%corpus
sizereduction,theacousticqualitydropsby10points.Furtherresearchshouldbe
focusedonthesetofattributestocoverandtheirnumberofoccurrencesinorderto
improvethiscompromise.Asexpected,thebaselinebuiltfromrandomsentencesis
preferredlessthantheothersystemsbecauseofthelackofrelativelyrare
acousticunits.
Table13
M
USHRA
averagemarksof30sentencesfromthe
Testcorpus
with10listenerspersentence
(thehigherthebetter).
379



ComputationalLinguistics
Volume41,Number3
Table14
M
USHRA
averageranksfor30sentencesfromthe
Testcorpus
with10listenerspersentence
(thelowerthebetter).
8.Conclusion
Thisarticlediscussedthebuildingoflinguistic-richcorporaunderanobjectiveof
parsimony.Thistask,ageneralizationofSCP,turnsouttobeanNP-hardproblemthat
cannotbepolynomiallyapproximated.Westudiedthebehaviorofseveralalgorithms
intheparticulardomainofNLP,wheretheconsideredeventsfollowaheavy-tailedtype
distribution.
Theproposedalgorithmshavebeencomparedthroughthreekindsofexperiments:
Theoneisthecoveringsof2-and3-phonemesfromtwotextcorpora,oneinFrench,
theotherinEnglish;thesecondoneconsistsofthecoveringsofpart-of-speechlabels
fromacorpusinFrench;thethirdoneevaluatestheimpactofbothalgorithmsonthe
acousticqualityofacorpus-basedTTSsystem.Thealgorithm,ASA,iscomposedof
anagglomerativegreedystrategyfollowedbyaspittinggreedystage.Thesecondone,
LamSCP,isbasedonLagrangianrelaxationprinciplescombinedwithgreedystrate-
gies.LamSCPisouradaptationofanalgorithmproposedinCaprara,Fischetti,and
Toth(1999)tothemulti-representationconstraints.ThecomparisonofSCPsolutions
ismainlyabouttheirsize,theirmaximaldistancewiththeoptimalcovering,andtheir
robustnessincaseofperturbationoftheinitialcorpusordering.
AlthoughASAismuchfasterthanLamSCP,itdoesnotpermitustosingle-
handedlyassessthequalityofitssolutionintermsofsize.ThemainassetsofLamSCP
arethecalculationofalowerboundtotheoptimalcoveringsizeandshortersolutions
thantheonesobtainedbyASA.Indeed,inourexperimentsofphonologicalcoverings,
theoptimalsolutionisatmost1.24%(10.13%,respectively)smallerthanthesolutions
derivedbyLamSCP(ASA,respectively).Asforthecoveringsof1-POS,LamSCPpro-
videstheoptimalsolutioninacaseofamono-representationconstraint,whereasthe
ASAsolutionis10.17%greaterthantheoptimalone.Theserelativegapsbetween
thelowerboundsandsolutionsizesofbothalgorithmsgenerallydecreasewhenthe
sizeofthesearchspacedecreases.ThankstothelowerboundderivedbyLamSCP,
weempiricallyshowthatitispossibletogetalmostoptimalsolutionsinalinguistic
frameworkfollowingZipf'slawdistribution,despitethetheoreticcomplexityofthe
multi-representedSCP.ConcerningthelastexperimentintheTTSframework,evenif
LamSCPprovidesasmallercorpus,thesubjectivetestshowsnodifference
betweentheTTSsystemsbasedonLamSCPandASAcorpora.Therefore,wethinkthat
ASAremainsthemostadequatestrategy,intermsofperformance,easeofdevelopment,
andcomputationtimetosolveSCPintheNLPHowever,itwouldbeinteresting
380



Barbotetal.
LargeLinguisticCorpusReduction
totestaparallelizedversionoftheheuristicphasethatcallsanimportantnumberof
greedysub-procedures.
Ourfutureprospectsforthisworkareinautomaticlanguageprocessingandspeech
synthesis.First,intheframeworkofthePhorevoxprojectsupportedbytheFrench
NationalResearchAgency,weareconsideringtheautomaticdesignofexercisecontents
forlanguagelearningbytheselectionoftextscoveringsomephonologicalorlinguistic
difSecondly,thisworkisapreliminarysteptobuildingaphoneticallyrich
scriptbeforeitsrecordinginordertoproduceahighqualityspeechsynthesis.The
coveringchoices,suchastheattributestocover,thenumberofrequiredoccurrences,
ortheﬁsentenceﬂlength(utterances,syntagms,etc.)needtobevalidated.Moreover,
inthisarticle,wehaveobservedthegreatimpactofthedistributionofrareunitsin
thecorpustoreduce,andwebelieveitwillbeinterestingtoadapttheﬁsentenceﬂ
granularityaccordingtothisdistribution.
References
Alon,Noga,DanaMoshkovitz,andShmuel
Safra.2006.Algorithmicconstructionof
setsfork-restrictions.
ACMTransactionson
Algorithms(TALG)
,2(2):153Œ177.
Barbot,Nelly,OlivierBo
¨
effard,andArnaud
Delhay.2012.Comparingperformanceof
differentset-coveringstrategiesfor
linguisticcontentoptimizationinspeech
corpora.In
ProceedingsoftheInternational
ConferenceonLanguageResourcesand
Evaluation(LREC)
,pages969Œ974,Istanbul.
B
´
echet,Fr
´
ed
´
eric.2001.Liaphon:unsysteme
completdephon
´
etisationdetextes.
Traitementautomatiquedeslangues
,
42(1):47Œ67.
Bo
¨
effard,Olivier,LaureCharonnat,S
´
ebastien
LeMaguer,DamienLolive,andGa
¨
elle
Vidal.2012.Towardsfullyautomatic
annotationofaudiobooksforTTS.In
ProceedingsoftheInternationalConferenceon
LanguageResourcesandEvaluation(LREC)
,
pages975Œ980,Istanbul.
Bo
¨
effard,OlivierandChristophe
d'Alessandro,2012.
SpeechSynthesis
.Wiley.
Bunnell,H.Timothy.2010.Craftingsmall
databasesforunitselectionTTS:Effectson
intelligibility.In
ProceedingsoftheISCA
TutorialandResearchWorkshoponSpeech
Synthesis(SSW7)
,pages40Œ44,Kyoto.
Cadic,Didier,C
´
edricBoidin,andChristophe
d'Alessandro.2010.TowardsoptimalTTS
corpora.In
ProceedingsoftheInternational
ConferenceonLanguageResourcesand
Evaluation(LREC)
,pages99Œ104,Malta.
Candito,Marie,EnriqueHenestroza
Anguiano,andDjam
´
eSeddah.2011.A
wordclusteringapproachtodomain
adaptation:Effectiveparsingofbiomedical
texts.In
Proceedingsofthe12thInternational
ConferenceonParsingTechnologies
,
pages37Œ42,Dublin.
Caprara,Alberto,MatteoFischetti,andPaolo
Toth.1999.Aheuristicmethodfortheset
coveringproblem.
OperationsResearch
,
47(5):730Œ743.
Caprara,Alberto,PaoloToth,andMatteo
Fischetti.2000.Algorithmsfortheset
coveringproblem.
AnnalsofOperations
Research
,98(1-4):353Œ371.
Ceria,Sebasti
´
an,PaoloNobili,andAntonio
Sassano.1998.ALagrangian-based
heuristicforlarge-scalesetcovering
problems.
MathematicalProgramming
,
81(2):215Œ228.
Chevelu,Jonathan,NellyBarbot,Olivier
Bo
¨
effard,andArnaudDelhay.2007.
Lagrangianrelaxationforoptimalcorpus
design.In
ProceedingsoftheISCATutorial
andResearchWorkshoponSpeechSynthesis
(SSW6)
,pages211Œ216,Bonn.
Chevelu,Jonathan,NellyBarbot,Olivier
Bo
¨
effard,andArnaudDelhay.2008.
Comparingset-coveringstrategiesfor
optimalcorpusdesign.In
Proceedingsofthe
InternationalConferenceonLanguage
ResourcesandEvaluation(LREC)
,
pages2951Œ2956,Marrakech.
Combescure,Pierre.1981.20listesde10
phrasesphon
´
etiquement
´
equilibr
´
ees.
Revued'Acoustique
,56:34Œ38.
Fisher,MarshallL.1981.TheLagrangian
relaxationmethodforsolvinginteger
programmingproblems.
Management
Science
,27(1):1Œ18.
Franc¸ois,H
´
el
˚
eneandOlivierBo
¨
effard.2001.
Designofanoptimalcontinuousspeech
databasefortext-to-speechsynthesis
consideredasasetcoveringproblem.In
ProceedingsoftheEuropeanConferenceon
SpeechCommunicationandTechnology
(Eurospeech)
,pages829Œ832,Aalborg.
Franc¸ois,H
´
el
˚
eneandOlivierBo
¨
effard.2002.
Thegreedyalgorithmanditsapplication
381



ComputationalLinguistics
Volume41,Number3
totheconstructionofacontinuous
speechdatabase.In
Proceedingsofthe
InternationalConferenceonLanguage
ResourcesandEvaluation(LREC)
,
pages1420Œ1426,LasPalmas,
CanaryIslands.
Gauvain,Jean-Luc,LoriLamel,andMaxine
Esk
´
enazi.1990.Designconsiderationsand
textselectionforBref,alargeFrench
readspeechcorpus.In
Proceedingsofthe
InternationalConferenceofSpokenLanguage
Processing(ICSLP)
,pages1097Œ1100,
Kobe.
Gotab,Pierre,Fr
´
ed
´
ericB
´
echet,andG
´
eraldine
Damnati.2009.Activelearningfor
rule-basedandcorpus-basedspoken
languageunderstandingmodels.In
ProceedingsoftheIEEEworkshopon
AutomaticSpeechRecognitionand
Understanding(ASRU)
,pages444Œ449,
Merano.
Hart,Michael.2003.Projectgutenberg.
http://www.gutenberg.org/
(Last
consultedApril2015).
ITU-R.2003.ITU-Rrecommendationbs.1534:
Methodforthesubjectiveassessmentof
intermediatequalitylevelsofcoding
systems.RadiocommunicationBureau,
Geneva.
Karp,RichardM.1972.Reducibility
amongcombinatorialproblems.In
ComplexityofComputerComputations
,
theIBMResearchSymposiaSeries.
Springer,pages85Œ103.
Kawai,Hisashi,SeiichiYamamoto,Norio
Higuchi,andTohruShimizu.2000.A
designmethodofspeechcorpusfor
text-to-speechsynthesistaking
accountofprosody.In
Proceedingsofthe
InternationalConferenceonSpokenLanguage
Processing(ICSLP)
,pages420Œ425,
Beijing.
Kominek,JohnandAlanW.Black.2003.The
CMUArcticspeechdatabasesforspeech
synthesisresearch.TechnicalReport
CMU-LTI-03-177,CarnegieMellon
UniversityLanguageTechnologies
Institute.Pittsburg,PA.
Krstulovi
´
c,Sacha,Fr
´
ed
´
ericBimbot,Olivier
Bo
¨
effard,DelphineCharlet,Dominique
Fohr,andOdileMella.2006.Optimizing
thecoverageofaspeechdatabasethrough
aselectionofrepresentativespeaker
recordings.
SpeechCommunication
,
48(10):1319Œ1348.
Krul,Aleksandra,G
´
eraldineDamnati,
Franc¸oisYvon,C
´
edricBoidin,andThierry
Moudenc.2007.Adaptivedatabase
reductionfordomainspeech
synthesis.In
ProceedingsoftheISCA
TutorialandResearchWorkshoponSpeech
Synthesis(SSW6)
,pages217Œ222,
Bonn.
Krul,Aleksandra,G
´
eraldineDamnati,
Franc¸oisYvon,andThierryMoudenc.
2006.Corpusdesignbasedonthe
Kullback-Leiblerdivergencefor
Text-To-Speechsynthesisapplication.
In
ProceedingsoftheInternational
ConferenceonSpokenLanguageProcessing
(ICSLP)
,pages2030Œ2033,
Pittsburgh,PA.
Neubig,GrahamandShinsukeMori.2010.
Word-basedpartialannotationforef
corpusconstruction.In
Proceedingsofthe
InternationalConferenceonLanguage
ResourcesandEvaluation(LREC)
,
pages2723Œ2727,Malta.
Raz,RanandShmuelSafra.1997.A
sub-constanterror-probabilitylow-degree
test,andasub-constanterror-probability
PCPcharacterizationofNP.In
Proceedings
ofthetwenty-ninthannualACMsymposium
onTheoryofcomputing
,STOC'97,
pages475Œ484,ElPaso,TX.
Rojc,MatejandZdravkoKa

ci

c.2000.Design
ofoptimalSlovenianspeechcorpusforuse
intheconcatenativespeechsynthesis
system.In
ProceedingsoftheInternational
ConferenceonLanguageResourcesand
Evaluation(LREC)
,pages321Œ326,
Athens.
Schein,AndrewI.,TedS.Sandler,and
LyleH.Ungar.2004.Bayesianexample
selectionusingBaBiES.TechnicalReport
MS-CIS-04-08,DepartmentofComputer
andInformationScience,Universityof
Pennsylvania.
Settles,Burr.2010.Activelearningliterature
survey.TechnicalReport1648,Department
ofComputerSciences,Universityof
Wisconsin,Madison.
Synapse.2011.Documentationtechnique:
Composantd'
´
etiquetageetlemmatisation.
http://www.synapse-fr.com/
.
Tian,JileiandJaniNurminen.2009.
Optimizationoftextdatabaseusing
hierachicalclustering.In
Proceedingsofthe
IEEEInternationalConferenceonAcoustics,
Speech,andSignalProcessing(ICASSP)
,
pages4269Œ4272,Taipei.
Tian,Jilei,JaniNurminen,andImre
Kiss.2005.Optimalsubsetselection
fromtextdatabases.In
Proceedingsof
theIEEEInternationalConferenceon
Acoustics,Speech,andSignal
Processing(ICASSP)
,pages305Œ308,
Philadelphia,PA.
382



Barbotetal.
LargeLinguisticCorpusReduction
Tomanek,KatrinandFredrikOlsson.2009.A
Websurveyontheuseofactivelearningto
supportannotationoftextdata.In
ProceedingsoftheNAACLHLT2009
WorkshoponActiveLearningforNatural
LanguageProcessing
,pages45Œ48,
Boulder,CO.
VanSanten,JanP.H.andAdamL.
Buchsbaum.1997.Methodsforoptimal
textselection.In
ProceedingsoftheEuropean
ConferenceonSpeechCommunicationand
Technology(Eurospeech)
,pages553Œ556,
Rhodes.
Zhang,Jin-SongandSatoshiNakamura.
2008.Animprovedgreedysearch
algorithmforthedevelopment
ofaphoneticallyrichspeech
corpus.
IEICETransactions
onInformationandSystems
,
E91-D(3):615Œ630.
383



[Finished in 0.7s]