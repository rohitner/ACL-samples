#######################
## output of pdf2txt ##
#######################

Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 143–152,

Denver, Colorado, May 31 – June 5, 2015. c(cid:13)2015 Association for Computational Linguistics

143

What’sCookin’?InterpretingCookingVideosusingText,SpeechandVisionJonathanMalmaud,JonathanHuang,VivekRathod,NickJohnston,AndrewRabinovich,andKevinMurphyGoogle1600AmphitheatreParkwayMountainView,CA94043malmaud@mit.edu{jonathanhuang,rathodv,nickj,amrabino,kpmurphy}@google.comAbstractWepresentanovelmethodforaligningase-quenceofinstructionstoavideoofsome-onecarryingoutatask.Inparticular,wefo-cusonthecookingdomain,wheretheinstruc-tionscorrespondtotherecipe.OurtechniquereliesonanHMMtoaligntherecipestepstothe(automaticallygenerated)speechtran-script.Wethenreﬁnethisalignmentusingastate-of-the-artvisualfooddetector,basedonadeepconvolutionalneuralnetwork.Weshowthatourtechniqueoutperformssimplertechniquesbasedonkeywordspotting.Italsoenablesinterestingapplications,suchasauto-maticallyillustratingrecipeswithkeyframes,andsearchingwithinavideoforeventsofin-terest.1IntroductionInrecentyears,therehavebeenmanysuccessfulattemptstobuildlarge“knowledgebases”(KBs),suchasNELL(Carlsonetal.,2010),KnowItAll(Et-zionietal.,2011),YAGO(Suchaneketal.,2007),andGoogle’sKnowledgeGraph/Vault(Dongetal.,2014).TheseKBsmostlyfocusondeclarativefacts,suchas“BarackObamawasborninHawaii”.Buthumanknowledgealsoencompassesproceduralin-formationnotyetwithinthescopeofsuchdeclara-tiveKBs–instructionsanddemonstrationsofhowtodancethetango,forexample,orhowtochangeatireonyourcar.AKBfororganizingandretrievingsuchproceduralknowledgecouldbeavaluableresourceforhelpingpeople(andpotentiallyevenrobots–e.g.,(Saxenaetal.,2014;Yangetal.,2015))learntoperformvarioustasks.Incontrasttodeclarativeinformation,proceduralknowledgetendstobeinherentlymultimodal.Inparticular,bothlanguageandperceptualinformationaretypicallyusedtoparsimoniouslydescribeproce-dures,asevidencedbythelargenumberof“how-to”videosandillustratedguidesontheopenweb.Toautomaticallyconstructamultimodaldatabaseofproceduralknowledge,wethusneedtoolsforextractinginformationfrombothtextualandvi-sualsources.Crucially,wealsoneedtoﬁgureouthowthesevariouskindsofinformation,whichoftencomplementandoverlapeachother,ﬁttogethertoaformastructuredknowledgebaseofprocedures.Asasmallsteptowardthebroadergoalofalign-inglanguageandperception,wefocusinthispa-perontheproblemofaligningvideodepictionsofprocedurestostepsinanaccompanyingtextthatcorrespondstotheprocedure.Wefocusonthecookingdomainduetotheprevalenceofcookingvideosonthewebandtherelativeeaseofinter-pretingtheirrecipesaslinearsequencesofcanon-icalactions.Inthisdomain,thetextualsourceisauser-uploadedrecipeattachedtothevideoshow-ingtherecipe’sexecution.Theindividualstepsofproceduresarecookingactionslike“peelanonion”,“sliceanonion”,etc.However,ourtechniquescanbeappliedtoanydomainthathastextualinstruc-tionsandcorrespondingvideos,includingvideosatsitessuchasyoutube.com,howcast.com,howdini.comorvideojug.com.Theapproachwetakeinthispaperleveragesthefactthatthespeechsignalininstructionalvideosisoftencloselyrelatedtotheactionsthatthepersonisperforming(whichisnottrueinmoregeneral144

videos).ThusweﬁrstaligntheinstructionalstepstothespeechsignalusinganHMM,andthenreﬁnethisalignmentbyusingastateoftheartcomputervisionsystem.Insummary,ourcontributionsareasfollows.First,weproposeanovelsystemthatcombinestext,speechandvisiontoperformanalignmentbetweentextualinstructionsandinstructionalvideos.Sec-ond,weuseoursystemtocreatealargecorpusof180kalignedrecipe-videopairs,andanevenlargercorpusof1.4Mshortvideoclips,eachlabeledwithacookingactionandanounphrase.Weevaluatethequalityofourcorpususinghumanraters.Third,weshowhowwecanuseourmethodstosupportapplicationssuchaswithin-videosearchandrecipeauto-illustration.2Dataandpre-processingWeﬁrstdescribehowwecollectedourcorpusofrecipesandvideos,andthepre-processingstepsthatwerunbeforeapplyingouralignmentmodel.Thecorpusofrecipes,aswellastheresultsofthealign-mentmodel,willbemadeavailablefordownloadatgithub.com/malmaud/whats_cookin.2.1CollectingalargecorpusofcookingvideoswithrecipesWeﬁrstsearchedYoutubeforvideoswhichhavebeenautomaticallytaggedwiththeFreebasemids/m/01mtb(Cooking)and/m/0p57p(recipe),andwhichhave(automaticallyproduced)English-languagespeechtranscripts,whichyieldedacollec-tionof7.4Mvideos.Ofthesevideos,wekeptthevideosthatalsohadaccompanyingdescriptivetext,leaving6.2Mvideos.Sometimestherecipeforavideoisincludedinthistextdescription,butsometimesitisstoredonanexternalsite.Forexample,avideo’stextde-scriptionmightsay“Clickherefortherecipe”.Toﬁndtherecipeinsuchcases,welookforsentencesinthevideodescriptionwithanyofthefollowingkeywords:“recipe”,“steps”,“cook”,“procedure”,“preparation”,“method”.Ifweﬁndanysuchto-kens,weﬁndanyURLsthatarementionedinthesamesentence,andextractthecorrespondingdocu-ment,givingusanadditional206kdocuments.WethencombinetheoriginaldescriptivetextwithanyClassPrecisionRecallF1Background0.970.950.96Ingredient0.930.950.94Recipestep0.940.950.94Table1:Testsetperformanceoftext-basedrecipeclassiﬁer.additionaltextthatweretrieveinthisway.Finally,inordertoextracttherecipefromthetextdescriptionofavideo,wetrainedaclassiﬁerthatclassiﬁeseachsentenceinto1of3classes:recipestep,recipeingredient,orbackground.Wekeeponlythevideoswhichhaveatleastoneingredientsentenceandatleastonerecipesentence.Thislaststepleavesuswith180,000videos.Totraintherecipeclassiﬁer,weneedlabeledexamples,whichweobtainbyexploitingthefactthatmanytextwebpagescontainingrecipesusethemachine-readablemarkupdeﬁnedathttp://schema.org/Recipe.Fromthisweextract500kexamplesofrecipesentences,and500kexam-plesofingredientsentences.Wealsosample500ksentencesatrandomfromwebpagestorepresentthenon-recipeclass.Finally,wetraina3-classna¨ıveBayesmodelonthisdatausingsimplebag-of-wordsfeaturevectors.TheperformanceofthismodelonaseparatetestsetisshowninTable1.2.2ParsingtherecipetextForeachrecipe,weapplyasuiteofin-houseNLPtools,similartotheStanfordCoreNLPpipeline.Inparticular,weperformPOStagging,entitychunk-ing,andconstituencyparsing(basedonare-implementationof(Petrovetal.,2006)).1Following(DruckandPang,2012),weusetheparsetreestruc-turetopartitioneachsentenceinto“microsteps”.Inparticular,wesplitatanytokencategorizedbytheparserasaconjunctiononlyifthattoken’sparentinthesentence’sconstituencyparseisaverbphrase.Anyrecipestepthatismissingaverbisconsiderednoiseanddiscarded.Wethenlabeleachrecipestepwithanoptionalactionandalistof0ormorenounchunks.Theac-1Sometimestheparserperformspoorly,becausethelan-guageusedinrecipesisoftenfullofimperativesentences,suchas“Mixtheﬂour”,whereastheparseristrainedonnewswiretext.Asasimpleheuristicforovercomingthis,weclassifyanytokenatthebeginningofasentenceasaverbifitlexicallymatchesamanually-deﬁnedlistofcooking-relatedverbs.145

tionlabelisthelemmatizedversionoftheheadverboftherecipestep.Welookatallchunkednounen-titiesinthestepwhicharethedirectobjectoftheaction(eitherdirectlyorviathepreposition“of”,asin“Addacupofﬂour”).Wecanonicalizetheseentitiesbycomputingtheirsimilaritytothelistofingredientsassociatedwiththisrecipe.Ifaningredientissufﬁcientlysimilar,thatingredientisaddedtothisstep’sentitylist.Oth-erwise,thestemmedentityisused.Forexample,considerthestep“Mixtomatosauceandpasta”;iftherecipehasaknowningredientcalled“spaghetti”,wewouldlabeltheactionas“mix”andtheentitiesas“tomatosauce”and“spaghetti”,becauseofitshighsemanticsimilarityto“pasta”.(Semanticsim-ilarityisestimatedbasedonEuclideandistancebe-tweenwordembeddingvectorscomputedusingthemethodof(Mikolovetal.,2013)trainedongeneralwebtext.)Inmanycases,thedirectobjectofatransitiveverbiselided(notexplicitlystated);thisisknownasthe“zeroanaphora”problem.Forexample,thetextmaysay“Addeggsandﬂourtothebowl.Mixwell.”.Theobjectoftheverb“mix”isclearlythestuffthatwasjustaddedtothebowl(namelytheeggsandﬂour),althoughthisisnotexplicitlystated.Tohandlethis,weuseasimplerecencyheuristic,andinserttheen-titiesfromtheprevioussteptothecurrentstep.2.3ProcessingthespeechtranscriptTheoutputofYoutube’sASRsystemisasequenceoftime-stampedtokens,producedbyastandardViterbidecodingsystem.Weconcatenatetheseto-kensintoasinglelongdocument,andthenapplyourNLPpipelinetoit.Notethat,inadditiontoerrorsin-troducedbytheASRsystem2,theNLPsystemcanintroduceadditionalerrors,becauseitdoesnotworkwellontextthatmaybeungrammaticalandwhichisentirelydevoidofpunctuationandsentencebound-arymarkers.Toassesstheimpactofthesecombinedsources2Accordingto(Liaoetal.,2013),theYoutubeASRsystemweused,basedonusingGaussianmixturemodelsfortheacous-ticmodel,hasaworderrorrateofabout52%(averagedoverallEnglish-languagevideos;somegenres,suchasnews,hadlowererrorrates).Thenewersystem,whichusesdeepneuralnetsfortheacousticmodel,hasanaverageWERof44%;however,thiswasnotavailabletousatthetimewedidourexperiments.Figure1:GraphicalmodelrepresentationofthefactoredHMM.Seetextfordetails.oferror,wealsocollectedamuchsmallersetof480cookingvideos(withcorrespondingrecipetext)forwhichthevideocreatorhaduploadedamanuallycuratedspeechtranscript;thishasnotranscriptionerrors,itcontainssentenceboundarymarkers,anditalsoalignswholephraseswiththevideo(insteadofjustsingletokens).WeappliedthesameNLPpipelinetothesemanualtranscripts.Intheresultssection,wewillseethattheaccuracyofourend-to-endsystemisindeedhigherwhenthespeechtran-scriptiserror-freeandwell-formed.However,wecanstillgetgoodresultsusingnoisier,automaticallyproducedtranscripts.3MethodsInthissection,wedescribeoursystemforaligninginstructionaltextandvideo.3.1HMMtoalignrecipewithASRtranscriptWealigneachstepoftherecipetoacorrespondingsequenceofwordsintheASRtranscriptbyusingtheinput-outputHMMshowninFigure1.HereX(1:K)representsthetextualrecipesteps(obtainedus-ingtheprocessdescribedinSection2.2);Y(1:T)representtheASRtokens(spokenwords);R(t)∈{1,...,K}istherecipestepnumberforframet;andB(t)∈{0,1}representswhethertimesteptisgeneratedbythebackground(B=1)orforegroundmodel(B=0).Thisbackgroundvariableisneededsincesometimessequencesofspokenwordsareun-relatedtothecontentoftherecipe,especiallyatthebeginningandendofavideo.Theconditionalprobabilitydistributions(CPDs)fortheMarkovchainisasfollows:p(R(t)=r|R(t−1)=r0)=αifr=r0+11−αifr=r00.0otherwisep(B(t)=b|B(t−1)=b)=γ.146

Thisencodesourassumptionthatthevideofol-lowsthesameorderingastherecipeandthatback-ground/foregroundtokenstendtoclustertogether.Obviouslytheseassumptionsdonotalwayshold,buttheyareareasonableapproximation.Foreachrecipe,wesetα=K/T,theratioofrecipestepstotranscripttokens.Thissettingcorre-spondstoanaprioribeliefthateachrecipestepisalignedwiththesamenumberoftranscripttokens.Theparameterγinourexperimentsissetbycross-validationto0.7basedonasmallsetofmanually-labeledrecipes.Fortheforegroundobservationmodel,wegener-atetheobservedwordfromthecorrespondingrecipestepvia:logp(Y(t)=y|R(t)=k,X(1:K),B(t)=0)∝max({WordSimilarity(y,x):x∈X(k)}),whereX(k)isthesetofwordsinthek’threcipestep,andWordSimilarity(s,t)isameasureofsimi-laritybetweenwordssandt,basedonwordvectordistance.Ifthisframeisalignedtothebackground,wegenerateitfromtheempiricaldistributionofwords,whichisestimatedbasedonpoolingallthedata:p(Y(t)=y|R(t)=k,B(t)=1)=ˆp(y).Finally,thepriorforp(B(t))isuniform,andp(R(1))issettoadeltafunctiononR(1)=1(i.e.,weassumevideosstartatstep1oftherecipe).Havingdeﬁnedthemodel,we“ﬂatten”ittoastandardHMM(bytakingthecrossproductofRtandBt),thenestimatetheMAPsequenceusingtheViterbialgorithm.SeeFigure2foranexample.Finally,welabeleachsegmentofthevideoasfollows:usethesegmentationinducedbythealign-ment,andextracttheactionandobjectfromthecor-respondingrecipestepasdescribedinSection2.2.IfthesegmentwaslabeledasbackgroundbytheHMM,wedonotapplyanylabeltoit.3.2KeywordspottingAsimplerapproachtolabelingvideosegmentsistojustsearchforverbsintheASRtranscript,andthentoextractaﬁxed-sizedwindowaroundthetimes-tampwherethekeywordoccurred.Wecallthisap-proach“keywordspotting”.Asimilarmethodfrom(Yuetal.,2014)ﬁltersASRtranscriptsbypart-of-speechtagandﬁndstokensthatmatchasmallvo-cabularytocreateacorpusofvideoclips(extractedfrominstructionalvideos),eachlabeledwithanac-tion/objectpair.Inmoredetail,wemanuallydeﬁneawhitelistof∼200actions(alltransitiveverbs)ofinterest,suchas“add”,“chop”,“fry”,etc.Wethenidentifywhenthesewordsarespoken(relyingonthePOStagstoﬁlteroutnon-verbs),andextractan8secondvideocliparoundthistimestamp.(Using2secondspriortotheactionbeingmentioned,and6secondsfollow-ing.)Toextracttheobject,wetakealltokenstaggedas“noun”within5tokensaftertheaction.3.3HybridHMM+keywordspottingWecannotusekeywordspottingifthegoalistoaligninstructionaltexttovideos.However,ifourgoalisjusttocreatealabeledcorpusofvideoclips,keywordspottingisareasonableapproach.Unfor-tunately,wenoticedthatthequalityofthelabels(especiallytheobjectlabels)generatedbykeywordspottingwasnotveryhigh,duetoerrorsintheASR.Ontheotherhand,wealsonoticedthattherecalloftheHMMapproachwasabout5timeslowerthanus-ingkeywordspotting,andfurthermore,thatthetem-porallocalizationaccuracywassometimesworse.Togetthebestofbothworlds,weemploythefol-lowinghybridtechnique.Weperformkeywordspot-tingfortheactionintheASRtranscriptasbefore,butusetheHMMalignmenttoinferthecorrespond-ingobject.Toavoidfalsepositives,weonlyusetheoutputoftheHMMforthisvideoifatleasthalfoftherecipestepsarealignedbyittothespeechtranscript;otherwisewebackofftothebaselineap-proachofextractingthenounphrasefromtheASRtranscriptinthewindowaftertheverb.3.4TemporalreﬁnementusingvisionInourexperiments,wenoticedthatsometimesthenarratordescribesanactionbeforeactuallyperform-ingit(thiswasalsonotedin(Yuetal.,2014)).Topartiallycombatthisproblem,weusedcomputervi-siontoreﬁnecandidatevideosegmentsasfollows.Weﬁrsttrainedvisualdetectorsforalargecollec-tionoffooditems(describedbelow).Then,givenacandidatevideosegmentannotatedwithanac-tion/objectpair(comingfromanyoftheprevious147

1: In a bowl combine ﬂour, chilli powder, cumin, paprika and ﬁve spice. Once thoroughly mixed, add in chicken strips and coat in mixture. 2: Heat oil in a wok or large pan on medium to high heat. Add in chicken and cook until lightly brown for 3 -- 5 minutes. 3: Add in chopped vegetables along with garlic, lime juice, hot sauce and Worcestershire sauce. 4: Cook for a further 15 minutes on medium heat. 5: As the mixture cooks, chop the tomatoes and add lettuce, and cucumber into a serving bowl. 6: Once cooked, serve fajita mix with whole wheat wrap. Add a spoonful of fajita mix into wrap with salsa and natural yogurt. Wrap or roll up the tortilla and serve with side salad. in a bowl combine the ﬂower chili powder paprika cumen and ﬁve-spice do 130 mixed add in the chicken strips and post in the ﬂour mixture he's oil in a walk for large pan on medium to high heat add in the chicken and cook until lightly browned for three to ﬁve minutes add in chopped vegetables along with the garlic lime juice hot sauce and Worcestershire sauce dome cook for a further 15 minutes on medium peace and the mixture coax chop the tomatoes and as blessed tomato and cucumber into a serving bowl up we've cooked add a spoonful up the fajita mix into a wrap with the salsa and after yogurt throughout the rack and served with side salad this recipe makes to avalanche portions done they have just taken but he says and delicious introduction to Mexican ﬂavors blue thatRecipe StepsAutomatic Speech Transcription0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Video Position Fried chicken Tomato Step 1Step 2Step 5Figure2:ExamplesfromaChickenFajitasrecipeathttps://www.youtube.com/watch?v=mGpvZE3udQ4(ﬁgurebestviewedincolor).Top:Alignmentbetween(left)recipestepsto(right)automaticspeechtranscript.TokensfromtheASRareallowedtobeclassiﬁedasbackgroundsteps(seee.g.,theuncoloredtextattheend).Bottom:Detectorscoresfortwoingredientsasafunctionofpositioninthevideo.threemethods),weﬁndatranslationofthewindow(ofupto3secondsineitherdirection)forwhichtheaveragedetectorscorecorrespondingtotheobjectismaximized.Theintuitionisthatbydetectingwhentheobjectinquestionisvisuallypresentinthescene,itismorelikelythatthecorrespondingactionisac-tuallybeingperformed.Trainingvisualfooddetectors.Wetrainedadeepconvolutionalneuralnetwork(CNN)classi-ﬁer(speciﬁcally,the16layerVGGmodelfrom(Si-monyanandZisserman,2014))ontheFoodFood-101datasetof(Bossardetal.,2014),usingtheCaffeopensourcesoftware(Jiaetal.,2014).TheFood-101datasetcontains1000imagesfor101differentkindsoffood.Tocompensateforthesmalltrainingset,wepretrainedtheCNNontheImageNetdataset(Russakovskyetal.,2014),whichhas1.2Mimages,andthenﬁne-tunedonFood-101.Afterafewhoursofﬁnetuning(usingasingleGPU),weobtained79%classiﬁcationaccuracy(assumingall101labelsaremutuallyexclusive)onthetestset,whichiscon-sistentwiththestateoftheartresults.33Inparticular,thewebsitehttps://www.metamind.io/vision/food(accessedon2/25/15)claimstheyalsogot79%onthisdataset.Thisismuchbetterthanthe56.4%foraCNNreportedin(Bossardetal.,2014).Webelievethemainreasonfortheimprovedperformanceistheuseofpre-trainingonImageNet.Wethentrainedourmodelonaninternal,propri-etarydatasetof220millionimagesharvestedfromGoogleImagesandFlickr.About20%oftheseim-agescontainfood,therestareusedtotraintheback-groundclass.Inthisset,thereare2809classesoffood,including1005rawingredients,suchasavo-cadoorbeef,and1804dishes,suchasratatouilleorcheeseburgerwithbacon.Weusethemodeltrainedonthismuchlargerdatasetinthecurrentpaper,duetoitsincreasedcoverage.(Unfortunately,wecannotreportquantitativeresults,sincethedatasetisverynoisy(sometimeshalfofthelabelsarewrong),sowehavenogroundtruth.Nevertheless,qualitativebehaviorisreasonable,andthemodeldoeswellonFood-101,aswediscussedabove.)Visualreﬁnementpipeline.Forstorageandtimeefﬁciency,wedownsampleeachvideotemporallyto5framespersecondandeachframeto224×224beforeapplyingtheCNN.Runningthefooddetectoroneachvideothenproducesavectorofscores(oneentryforeachof2809classes)pertimeframe.Thereisnotaperfectmapfromthenamesofingredientstothenamesofthedetectoroutputs.Forexample,anomeletterecipemaysay“egg”,buttherearetwokindsofvisualdetectors,onefor“scrambledegg”andonefor“rawegg”.Wethereforedecidedtodeﬁnethematchscorebetweenaningredientandaframebytakingthemaximum148

scoreforthatframeoveralldetectorswhosenamesmatchedanyoftheingredienttokens(afterlemma-tizationandstopwordﬁltering).Finally,thematchscoreofavideosegmenttoanobjectiscomputedbytakingtheaveragescoreofallframeswithinthatsegment.Bythenscoringandmaximizingoveralltranslationsofthecandi-datesegment(ofuptothreesecondsaway),wepro-duceaﬁnal“reﬁned”segment.3.5QuantifyingconﬁdenceviavisionandaffordancesTheoutputofthekeywordspottingand/orHMMsystemsisan(action,object)labelassignedtocer-tainvideoclips.Inordertoestimatehowmuchcon-ﬁdencewehaveinthatlabel(sothatwecantradeoffprecisionandrecall),weusealinearcombinationoftwoquantities:(1)theﬁnalmatchscoreproducedbythevisualreﬁnementpipeline,whichmeasuresthevisibilityoftheobjectinthegivenvideoseg-ment,and(2)anaffordanceprobability,measuringtheprobabilitythatoappearsasadirectobjectofa.Theaffordancemodelallowsusto,forexample,prioritizeasegmentlabeledas(peel,garlic)overasegmentlabeledas(peel,sugar).TheprobabilitiesP(object=o|action=a)areestimatedbyﬁrstforminganinversedocumentfrequencymatrixcap-turingaction/objectco-occurrences(treatingactionsasdocuments).Togeneralizeacrossactionsandob-jectsweformalow-rankapproximationtothisIDFmatrixusingasingularvaluedecompositionandsetaffordanceprobabilitiestobeproportionaltoexpo-nentiatedentriesoftheresultingmatrix.Figure3vi-sualizestheseaffordanceprobabilitiesforaselectedsubsetoffrequentlyusedaction/objectpairs.4EvaluationandapplicationsInthissection,weexperimentallyevaluatehowwellourmethodswork.Wethenbrieﬂydemonstratesomeprototypeapplications.4.1EvaluatingtheclipdatabaseOneofthemainoutcomesofourprocessisasetofvideoclips,eachofwhichislabeledwithaverb(ac-tion)andanoun(object).Wegenerated3suchla-beledcorpora,using3differentmethods:keywordspotting(“KW”),thehybridHMM+keywordspot-ting(“Hybrid”),andthehybridsystemwithvisualFigure3:Visualizationofaffordancemodel.Entries(a,o)arecoloredaccordingtoP(object=o|action=a).Score00.511.52ActionsObjectsKWVisual Reﬁnement (alone)HybridHybrid (manual)Visual Reﬁnement + recipeKW (manual)KWVisual Reﬁnement (alone)HybridHybrid (manual)Visual Reﬁnement + recipeKW (manual)Figure4:Clipquality,asassessedbyMechanicalTurkexper-imentson900trials.Figurebestviewedincolor;seetextfordetails.fooddetector(“visualreﬁnement”).Thetotalnum-berofclipsproducedbyeachmethodisverysimilar,approximately1.4million.Thecoverageoftheclipsisapproximately260kunique(action,nounphrase)pairs.Toevaluatethequalityofthesemethods,wecre-atedarandomsubsetof900clipsfromeachcorpususingstratiﬁedsampling.Thatis,wepickedanac-tionuniformlyatrandom,andthenpickedacorre-spondingobjectforthatactionfromitssupportsetuniformlyatrandom,andﬁnallypickedaclipwiththat(action,object)labeluniformlyatrandomfromtheclipcorpusesproducedinSection3;thisensures149

1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2 0 0.5 1 1.5 Mean clip qualityNumber of clipsMillions ActionObjectFigure5:Averageclipquality(precision)afterﬁlteringoutlowconﬁdenceclipsversus#clipsretained(recall).# RatersRating-2-10121701211647670Figure6:HistogramofhumanratingscomparingrecipestepsagainstASRdescriptionsofavideoclip.“2”indicateastrongpreferencefortherecipestep;“-2”astrongpreferenceforthetranscript.Seetextfordetails.thetestsetisnotdominatedbyfrequentactionsorobjects.WethenperformedaMechanicalTurkexperi-mentoneachtestset.Eachclipwasshownto3raters,andeachraterwasaskedthequestion“Howwelldoesthisclipshowthegivenaction/object?”.Ratersthenhadtoanswerona3-pointscale:0means“notatall”,1means“somewhat”,and2means“verywell”.TheresultsareshowninFigure4.Weseethatthequalityofthehybridmethodissigniﬁcantlybet-terthanthebaselinekeywordspottingmethod,forbothactionsandobjects.4Whileamanuallycurated4Inter-rateragreement,measuredviaFleiss’skappabyag-gregatingacrossalljudgmenttasks,is.41,whichisstatisticallysigniﬁcantatap<.05level.speechtranscriptindeedyieldsbetterresults(seethebarslabeled‘manual’),weobservethatautomati-callygeneratedtranscriptsallowustoperformal-mostaswell,especiallyusingouralignmentmodelwithvisualreﬁnement.Comparingaccuracyonactionsagainstthatonobjectsinthesameﬁgure,weseethatkeywordspot-tingisfarmoreaccurateforactionsthanitisforobjects(byover30%).Thisdisparityisnotsurpris-ingsincekeywordspottingsearchesonlyforactionkeywordsandreliesonaroughheuristictorecoverobjects.Wealsoseethatusingalignment(whichextractstheobjectfromthe“clean”recipetext)andvisualreﬁnement(whichistrainedexplicitlytode-tectingredients)bothhelptoincreasetherelativeac-curacyofobjects—underthehybridmethod,forexample,theaccuracyforactionsisonly8%betterthanthatofobjects.NotethatclipsfromtheHMMandhybridmeth-odsvariedinlengthbetween2and10seconds(mean4.2seconds),whileclipsfromthekeywordspottingmethodwerealwaysexactly8seconds.Thuscliplengthispotentiallyaconfoundingfactorintheevaluationwhencomparingthehybridmethodtothekeyword-spottingmethod;however,ifthereisabiastoassignhigherratingstolongerclips(whichareapriorimorelikelytocontainadepictionofagivenactionthanshorterclips),itwouldbeneﬁtthekeywordspotingmethod.Segmentconﬁdencescores(fromSection3.5)canbeusedtoﬁlteroutlowconﬁdencesegments,thusimprovingtheprecisionofclipretrievalatthecostofrecall.Figure5visualizesthistrade-offaswevaryourconﬁdencethreshold,showingthatindeed,seg-mentswithhigherconﬁdencestendtohavethehigh-estqualityasjudgedbyourhumanraters.More-over,thetop167,000segmentsasrankedbyourcon-ﬁdencemeasurehaveanaverageratingexceeding1.75.WeadditionallysoughttoevaluatehowwellrecipestepsfromtherecipebodycouldserveascaptionsforvideoclipsincomparisontotheoftennoisyASRtransript,whichservesasaroughproxyforevaluatingthequalityofthealignmentmodelaswellasdemonstrationapotentialapplicationofourmethodfor“cleaningup”noisyASRcaptionsintocompletegrammaticalsentences.Tothatend,werandomlyselected200clipsfromourcorpusthat150

bothhaveanassociatedactionkeywordfromthetranscriptaswellasanalignedrecipestepselectedbytheHMMalignmentmodel.Foreachclip,threeratersonMechanicalTurkwereshowntheclip,thetextfromtherecipestep,andafragmentoftheASRtranscript(thekeyword,plus5tokenstotheleftandrightofthekeyword).Ratersthenindicatedwhichdescriptiontheypreferred:2indicatesastrongpref-erencefortherecipestep,1aweakpreference,0indifference,-1aweakpreferenceforthetranscriptfragment,and-2astrongpreference.ResultsareshowninFigure6.Excludingraterswhoindicatedindifﬁerence,67%ofraterspreferredtherecipestepastheclip’sdescription.ApotentialconfoundforusingthisanalysisasaproxyforthequalityofthealignmentmodelisthattheASRtranscriptisgenerallyanungrammat-icalsentencefragmentasopposedtothegrammati-calrecipesteps,whichislikelytoreducetheraters’approvalofASRcaptionsinthecasewhenbothac-curatelydescribethescene.However,ifusersstillonaveragepreferanASRsentencefragmentwhichdescribestheclipcorrectlyversusafullrecipestepwhichisunrelatedtothescene,thenthisexperimentstillprovidesevidenceofthequalityofthealign-mentmodel.4.2AutomaticallyillustratingarecipeOneusefulbyproductofouralignmentmethodisthateachrecipestepisassociatedwithasegmentofthecorrespondingvideo.5Weuseastandardkeyframeselectionalgorithmtopickthebestframefromeachsegment.Wecanthenassociatethisframewiththecorrespondingrecipestep,thusautomati-callyillustratingtherecipesteps.AnillustrationofthisprocessisshowninFigure7.4.3SearchwithinavideoAnotherapplicationwhichourmethodsenableissearchwithinavideo.Forexample,ifauserwouldliketoﬁndaclipillustratinghowtokneaddough,wecansimplysearchourcorpusoflabeledclips,5TheHMMmayassignmultiplenon-consecutiveregionsofthevideotothesamerecipestep(sincethebackgroundstatecanturnonandoff).Insuchcases,wejusttakethe“convexhull”oftheregionsastheintervalwhichcorrespondstothatstep.ItisalsopossiblefortheHMMnottoassignagivensteptoanyintervalofthevideo.Figure8:Searchingfor“kneaddough”.Notethatthevideoshaveautomaticallybeenadvancedtotherelevantframe.andreturnalistofmatches(rankedbyconﬁdence).Sinceeachcliphasacorresponding“provenance”,wecanreturntheresultstotheuserasasetofvideosinwhichwehaveautomatically“fastforwarded”totherelevantsectionofthevideo(seeFigure8foranexample).ThisstandsincontrasttostandardvideosearchonYoutube,whichreturnsthewholevideo,butdoesnot(ingeneral)indicatewherewithinthevideotheuser’ssearchqueryoccurs.5RelatedworkThereareseveralpiecesofrelatedwork.(Yuetal.,2014)performskeywordspottinginthespeechtran-scriptinordertolabelclipsextractedfrominstruc-tionalvideos.However,ourhybridapproachper-formsbetter;thegainisespeciallysigniﬁcantonau-tomaticallygeneratedspeechtranscripts,asshowninFigure4.TheideaofusinganHMMtoaligninstructionalstepstoavideowasalsoexploredin(Naimetal.,2014).However,theirconditionalmodelhastogen-erateimages,whereasoursjusthastogenerateASRwords,whichisaneasiertask.Furthermore,theyonlyconsider6videoscollectedinacontrolledlabsetting,whereasweconsiderover180kvideoscol-lected“inthewild”.AnotherpaperthatusesHMMstoprocessrecipetextis(DruckandPang,2012).TheyusetheHMMtoalignthestepsofarecipetothecommentsmadebyusersinanonlineforum,whereaswealignthestepsofarecipetothespeechtranscript.Also,weusevideoinformation,whichwasnotconsideredinthisearlierwork.(Joshietal.,2006)describesasystemtoautomat-icallyillustrateatextdocument,howevertheyonlygenerateoneimage,notasequence,andtheirtech-niquesareverydifferent.Thereisalsoalargebodyofotherworkoncon-nectinglanguageandvision;weonlyhavespace151

De-stem 2 medium plum tomatoes.Cut them in half lengthwise andremove the seeds.Finely chop the tomatoes,combining them with 1/4 cup of ﬁnely chopped red onion, 2 minced cloves of garlic, 1 tablespoon of olive oil, 2 tablespoons of fresh lime juice, and 1/8 teaspoon of black pepperCut an avocado into chunks and mash until it's smooth with just a few pieces intact.Stir the mashed avocados into the other mixture for a homemade guacamole recipe that 's perfect for any occasion! Use this easy guacamole for parties,or serve chips with guacamole for an easy appetizer.You could even add some cayenne, jalapenos, or ancho chili for even more kick to add to your Mexican food night!Figure7:AutomaticallyillustratingaGuacamolerecipefromhttps://www.youtube.com/watch?v=H7Ne3s202lU.tobrieﬂymentionafewkeypapers.(Rohrbachetal.,2012b)describestheMPIICookingCompositeActivitiesdataset,whichconsistsof212videoscol-lectedinthelabofpeopleperformingvariouscook-ingactivities.(Thisextendsthedatasetdescribedintheirearlierwork,(Rohrbachetal.,2012a).)Theyalsodescribeamethodtorecognizeobjectsandac-tionsusingstandardvisionfeatures.However,theydonotleveragethespeechsignal,andtheirdatasetissigniﬁcantlysmallerthanours.(Guadarramaetal.,2013)describesamethodforgeneratingsubject-verb-objecttriplesgivenashortvideoclip,usingstandardobjectandactiondetec-tors.Thetechniquewasextendedin(Thomasonetal.,2014)toalsopredictthelocation/place.Further-more,theyusealinear-chainCRFtocombinethevisualscoreswithasimple(s,v,o,p)languagemodel(similartoouraffordancemodel).Theyappliedtheirtechniquetothedatasetin(ChenandDolan,2011),whichconsistsof2000shortvideoclips,eachdescribedwith1-3sentences.Bycontrast,wefocusonaligninginstructionaltexttothevideo,andourcorpusissigniﬁcantlylarger.(YuandSiskind,2013)describesatechniqueforestimatingthecompatibilitybetweenavideoclipandasentence,basedonrelativemotionoftheobjects(whicharetrackedusingHMMs).Theirmethodistestedon159videoclips,createdundercarefullycontrolledconditions.Bycontrast,wefo-cusonaligninginstructionaltexttothevideo,andourcorpusissigniﬁcantlylarger.6DiscussionandfutureworkInthispaper,wehavepresentedanovelmethodforaligninginstructionaltexttovideos,leveragingbothspeechrecognitionandvisualobjectdetection.Wehaveusedthistoalign180krecipe-videopairs,fromwhichwehaveextractedacorpusof1.4Mlabeledvideoclips–asmallbutcrucialsteptowardbuild-ingamultimodalproceduralknowlegebase.Inthefuture,wehopetousethislabeledcorpustotrainvisualactiondetectors,whichcanthenbecombinedwiththeexistingvisualobjectdetectorstointerpretnovelvideos.Additionally,webelievethatcombin-ingvisualandlinguisticcuesmayhelpovercomelongstandingchallengestolanguageunderstanding,suchasanaphoraresolutionandwordsensedisam-biguation.Acknowledgments.WewouldliketothankAlexGorbanandAnoopKorattikaraforhelpingwithsomeoftheexperiments,andNancyChangforfeed-backonthepaper.ReferencesLukasBossard,MatthieuGuillaumin,andLucVanGool.2014.Food-101–miningdiscriminativecomponentswithrandomforests.InProc.EuropeanConf.onComputerVision.A.Carlson,J.Betteridge,B.Kisiel,B.Settles,E.Hr-uschkaJr.,andT.Mitchell.2010.Towardanarchi-tecturefornever-endinglanguagelearning.InProcs.AAAI.DavidLChenandWilliamBDolan.2011.Collectinghighlyparalleldataforparaphraseevaluation.InProc.ACL,HLT’11,pages190–200,Stroudsburg,PA,USA.AssociationforComputationalLinguistics.X.Dong,E.Gabrilovich,G.Heitz,W.Horn,N.Lao,K.Murphy,T.Strohmann,S.Sun,andW.Zhang.2014.Knowledgevault:Aweb-scaleapproachtoprobabilisticknowledgefusion.InProc.oftheInt’lConf.onKnowledgeDiscoveryandDataMining.GregoryDruckandBoPang.2012.Spiceitup?:Mining152

reﬁnementstoonlineinstructionsfromusergeneratedcontent.InProc.ACL,pages545–553.O.Etzioni,A.Fader,J.Christensen,S.Soderland,andMausam.2011.OpenInformationExtraction:theSecondGeneration.InIntl.JointConf.onAI.S.Guadarrama,N.Krishnamoorthy,G.Malkarnenkar,S.Venugopalan,R.Mooney,T.Darrell,andK.Saenko.2013.YouTube2Text:Recognizinganddescribingarbitraryactivitiesusingsemantichierar-chiesandZero-Shotrecognition.InIntl.Conf.onComputerVision,pages2712–2719.YangqingJia,EvanShelhamer,JeffDonahue,SergeyKarayev,JonathanLong,RossGirshick,SergioGuadarrama,andTrevorDarrell.2014.Caffe:Con-volutionalarchitectureforfastfeatureembedding,20June.DhirajJoshi,JamesZWang,andJiaLi.2006.ThestorypicturingEngine-Asystemforautomatictextillus-tration.ACMTrans.MultimediaComp.,Comm.andAppl.,2(1):1–22.HankLiao,ErikMcDermott,andAndrewSenior.2013.Largescaledeepneuralnetworkacousticmodelingwithsemi-supervisedtrainingdataforYouTubevideotranscription.InASRU(IEEEAutomaticSpeechRecognitionandUnderstandingWorkshop).TomasMikolov,KaiChen,GregCorrado,andJeffreyDean.2013.Efﬁcientestimationofwordrepresenta-tionsinvectorspace.http://arxiv.org/abs/1301.3781.INaim,YCSong,QLiu,HKautz,JLuo,andDGildea.2014.Unsupervisedalignmentofnaturallanguagein-structionswithvideosegments.InProcs.ofAAAI.SlavPetrov,LeonBarrett,RomainThibaux,andDanKlein.2006.Learningaccurate,compact,andinter-pretabletreeannotation.InProceedingsofthe21stIn-ternationalConferenceonComputationalLinguisticsand44thAnnualMeetingoftheAssociationforCom-putationalLinguistics,pages433–440,Sydney,Aus-tralia,July.AssociationforComputationalLinguistics.MRohrbach,SAmin,MAndriluka,andBSchiele.2012a.Adatabaseforﬁnegrainedactivitydetectionofcookingactivities.InCVPR,pages1194–1201.MarcusRohrbach,MichaelaRegneri,MykhayloAn-driluka,SikandarAmin,ManfredPinkal,andBerntSchiele.2012b.ScriptdataforAttribute-Basedrecog-nitionofcompositeactivities.InProc.EuropeanConf.onComputerVision,pages144–157.OlgaRussakovsky,JiaDeng,HaoSu,JonathanKrause,SanjeevSatheesh,SeanMa,ZhihengHuang,An-drejKarpathy,AdityaKhosla,MichaelBernstein,AlexanderC.Berg,andLiFei-Fei.2014.Im-ageNetLargeScaleVisualRecognitionChallenge.http://arxiv.org/abs/1409.0575.AshutoshSaxena,AsheshJain,OzanSener,AdityaJami,DipendraKMisra,andHemaSKoppula.2014.RoboBrain:Large-Scaleknowledgeengineforrobots.http://arxiv.org/pdf/1412.0691.pdf.KarenSimonyanandAndrewZisserman.2014.VerydeepconvolutionalnetworksforLarge-Scaleimagerecognition,4September.http://arxiv.org/abs/1409.1556.F.M.Suchanek,G.Kasneci,andG.Weikum.2007.YAGO:ALargeOntologyfromWikipediaandWord-Net.J.WebSemantics,6:203217.JThomason,SVenugopalan,SGuadarrama,KSaenko,andRMooney.2014.Integratinglanguageandvisiontogeneratenaturallanguagedescriptionsofvideosinthewild.InIntl.Conf.onComp.Linguistics.YezhouYang,YiLi,CorneliaFerm¨uller,andYiannisAloimonos.2015.Robotlearningmanipulationac-tionplansbywatchingunconstrainedvideosfromtheworldwideweb.InTheTwenty-NinthAAAIConfer-enceonArtiﬁcialIntelligence(AAAI-15).HaonanYuandJMSiskind.2013.Groundedlanguagelearningfromvideodescribedwithsentences.InProc.ACL.Shoou-IYu,LuJiang,andAlexanderHauptmann.2014.Instructionalvideosforunsupervisedharvestingandlearningofactionexamples.InIntl.Conf.Multimedia,pages825–828.ACM.

######################
## output of PY2PDF ##
######################

144



145



146



147



148



149



150



151



152



