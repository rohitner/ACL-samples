#######################
## output of pdf2txt ##
#######################

Proceedings of the 19th Conference on Computational Language Learning, pages 52–61,

Beijing, China, July 30-31, 2015. c(cid:13)2015 Association for Computational Linguistics

52

AnIterativeSimilaritybasedAdaptationTechniqueforCrossDomainTextClassiﬁcationHimanshuS.BhattDeepaliSemwalXeroxResearchCenterIndia,Bengaluru,INDIA{Himanshu.Bhatt,Deepali.Semwal,Shourya.Roy}@xerox.comShouryaRoyAbstractSupervisedmachinelearningclassiﬁca-tionalgorithmsassumebothtrainandtestdataaresampledfromthesamedomainordistribution.However,performanceofthealgorithmsdegradefortestdatafromdifferentdomain.Suchcrossdo-mainclassiﬁcationisarduousasfeaturesinthetestdomainmaybedifferentandabsenceoflabeleddatacouldfurtherex-acerbatetheproblem.Thispaperproposesanalgorithmtoadaptclassiﬁcationmodelbyiterativelylearningdomainspeciﬁcfea-turesfromtheunlabeledtestdata.More-over,thisadaptationtranspiresinasimi-larityawaremannerbyintegratingsimilar-itybetweendomainsintheadaptationset-ting.Cross-domainclassiﬁcationexper-imentsondifferentdatasets,includingarealworlddataset,demonstrateefﬁcacyoftheproposedalgorithmoverstate-of-the-art.1IntroductionAfundamentalassumptioninsupervisedstatis-ticallearningisthattrainingandtestdataareindependentlyandidenticallydistributed(i.i.d.)samplesdrawnfromadistribution.Otherwise,goodperformanceontestdatacannotbeguar-anteedevenifthetrainingerrorislow.Inreallifeapplicationssuchasbusinessprocessautoma-tion,thisassumptionisoftenviolated.Whilere-searchersdevelopnewtechniquesandmodelsformachinelearningbasedautomationofoneorahandfulbusinessprocesses,largescaleadoptionishinderedowingtopoorgeneralizedperformance.Inourinteractionswithanalyticssoftwaredevel-opmentteams,wenoticedsuchpervasivediver-sityoflearningtasksandassociatedinefﬁciency.Novelpredictiveanalyticstechniquesonstandarddatasets(orlimitedclientdata)didnotgeneral-izeacrossdifferentdomains(newproducts&ser-vices)andhaslimitedapplicability.Trainingmod-elsfromscratchforeverynewdomainrequireshu-manannotatedlabeleddatawhichisexpensiveandtimeconsuming,hence,notpragmatic.Ontheotherhand,transferlearningtechniquesallowdomains,tasks,anddistributionsusedintrainingandtestingtobedifferent,butrelated.Itworksincontrasttotraditionalsupervisedtech-niquesontheprincipleoftransferringlearnedknowledgeacrossdomains.Whiletransferlearn-inghasgenerallyprovedusefulinreducingthelabelleddatarequirement,bruteforcetechniquessufferfromtheproblemofnegativetransfer(PanandYang,2010a).Onecannotusetransferlearn-ingastheproverbialhammer,butneedstogaugewhentotransferandalsohowmuchtotransfer.Toaddresstheseissues,thispaperproposesadomainadaptationtechniqueforcross-domaintextclassiﬁcation.Inoursettingforcross-domainclassiﬁcation,aclassiﬁertrainedononedomainwithsufﬁcientlabelledtrainingdataisappliedtoadifferenttestdomainwithnolabelleddata.AsshowninFigure1,thispaperproposesaniterativesimilaritybasedadaptationalgorithmwhichstartswithasharedfeaturerepresentationofsourceandtargetdomains.Toadapt,ititerativelylearnsdo-mainspeciﬁcfeaturesfromtheunlabeledtargetdomaindata.Inthisprocess,similaritybetweentwodomainsisincorporatedintheadaptationset-tingforsimilarity-awaretransfer.Themajorcon-tributionsofthisresearchare:•Aniterativealgorithmforlearningdomainspeciﬁcdiscriminativefeaturesfromunla-beleddatainthetargetdomainstartingwithaninitialsharedfeaturerepresentation.•Facilitatingsimilarity-awaredomainadapta-tionbyseamlesslyintegratingsimilaritybe-tweentwodomainsintheadaptationsettings.53

Figure1:Outlinesdifferentstagesoftheproposedalgorithmi.e.sharedfeaturerepresentation,do-mainsimilarity,andtheiterativelearningprocess.Tothebestofourknowledge,thisistheﬁrst-of-its-kindapproachincross-domaintextclassiﬁca-tionwhichintegratessimilaritybetweendomainsintheadaptationsettingtolearndomainspeciﬁcfeaturesinaniterativemanner.Therestofthepaperisorganizedasfollows:Section2summa-rizestherelatedwork,Section3presentsdetailsabouttheproposedalgorithm.Section4presentsdatabases,experimentalprotocol,andresults.Fi-nally,Section5concludesthepaper.2RelatedWorkTransferlearningintextanalysis(domainadapta-tion)hasshownpromisingresultsinrecentyears(PanandYang,2010a).Priorworkondomainadaptationfortextclassiﬁcationcanbebroadlyclassiﬁedintoinstancere-weighingandfeature-representationbasedadaptationapproaches.Instancere-weighingapproachesaddressthedifferencebetweenthejointdistributionsofob-servedinstancesandclasslabelsinsourcedo-mainwiththatoftargetdomain.Towardsthisdi-rection,Liaoetal.(2005)learnedmismatchbe-tweentwodomainsandusedactivelearningtoselectinstancesfromthesourcedomaintoen-hanceadaptabilityoftheclassiﬁer.JiangandZhai(2007)proposedinstanceweighingschemefordo-mainadaptationinNLPtaskswhichexploitinde-pendencebetweenfeaturemappingandinstanceweighingapproaches.Sahaetal.(2011)lever-agedknowledgefromsourcedomaintoactivelyselectthemostinformativesamplesfromthetar-getdomain.Xiaetal.(2013)proposedahybridmethodforsentimentclassiﬁcationtaskthatalsoaddressesthechallengeofmutuallyoppositeori-entationwords.Anumberofdomainadaptationtechniquesarebasedonlearningcommonfeaturerepresentation(PanandYang,2010b;Blitzeretal.,2006;Jietal.,2011;Daum´eIII,2009)fortextclassiﬁcation.Thebasicideabeingidentifyingasuitablefea-turespacewhereprojectedsourceandtargetdo-maindatafollowsimilardistributionsandhence,astandardsupervisedlearningalgorithmcanbetrainedontheformertopredictinstancesfromthelatter.Amongthem,StructuralCorrespon-denceLearning(SCL)(Blitzeretal.,2007)isthemostrepresentativeone,explainedlater.Daum´e(2009)proposedaheuristicbasednon-linearmap-pingofsourceandtargetdatatoahighdimen-sionalspace.Panetal.(2008)proposedadi-mensionalityreductionmethodMaximumMeanDiscrepancyEmbeddingtoidentifyalatentspace.Subsequently,Panetal.(2010)proposedtomapdomainspeciﬁcwordsintouniﬁedclustersusingspectralclusteringalgorithm.Inanotherfollowupwork,Panetal.(2011)proposedanovelfea-turerepresentationtoperformdomainadaptationviaReproducingKernelHilbertSpaceusingMax-imumMeanDiscrepancy.Asimilarapproach,basedonco-clustering(Dhillonetal.,2003),wasproposedinDaietal.(2007)toleveragecommonwordsasbridgebetweentwodomains.Bollegalaetal.(2011)usedsentimentsensitivethesaurustoexpandfeaturesforcross-domainsentimentclas-siﬁcation.Inacomprehensiveevaluationstudy,itwasobservedthattheirapproachtendstoincreasetheadaptationperformancewhenmultiplesourcedomainswereused(Bollegalaetal.,2013).DomainadaptationbasedoniterativelearninghasbeenexploredbyChenetal.(2011)andGarcia-Fernandezetal.(2014)andaresimilartothephilosophyoftheproposedapproachinap-pendingpseudo-labeledtestdatatothetrainingset.Theﬁrstapproachusesanexpensivefea-turesplittoco-traintwoclassiﬁerswhilethefor-merpresentsasingleclassiﬁerself-trainingbasedsetting.However,theproposedalgorithmoffersnovelcontributionsintermsof1)leveragingtwoindependentfeaturerepresentationscapturingthesharedandtargetspeciﬁcrepresentations,2)anensembleofclassiﬁersthatuseslabelledsourcedomainandpseudolabelledtargetdomainin-stancescarefullymoderatedbasedonsimilaritybetweentwodomains.Ensemblebaseddomainadaptationfortextclassiﬁcationwasﬁrstpro-posedbyAueandGammon(2005)thoughtheirapproachcouldnotachievesigniﬁcantimprove-mentsoverbaseline.Later,Zhaoetal.(2010)proposedonlinetransferlearning(OTL)frame-54

workwhichformsthebasisofourensemblebaseddomainadaptation.However,theproposedalgo-rithmdiffersinthefollowingways:1)anunsuper-visedapproachthattransformsunlabeleddataintopseudolabeleddataunlikeOTLwhichissuper-vised,and2)incorporatessimilarityintheadapta-tionsettingforgradualtransfer.3IterativeSimilaritybasedAdaptationThephilosophyofouralgorithmisgradualtrans-ferofknowledgefromthesourcetothetargetdo-mainwhilebeingcognizantofsimilaritybetweentwodomains.Toaccomplishthis,wehavedevel-opedatechniquebasedonensembleoftwoclassi-ﬁers.Transferoccurswithintheensemblewhereaclassiﬁerlearnedonsharedrepresentationtrans-formsunlabeledtestdataintopseudolabeleddatatolearndomainspeciﬁcclassiﬁer.Beforeexplain-ingthealgorithm,wehighlightitssalientfeatures:CommonFeatureSpaceRepresentation:Ourobjectiveistoﬁndagoodfeaturerepresentationwhichminimizesdivergencebetweenthesourceandtargetdomainsaswellastheclassiﬁcationerror.Therehavebeenseveralworkstowardsfeature-representation-transferapproachsuchas(Blitzeretal.,2007;Jietal.,2011)whichderivesatransformationmatrixQthatgivesasharedrepre-sentationbetweenthesourceandtargetdomains.OneofthewidelyusedapproachesisStructuralCorrespondenceLearning(SCL)(Blitzeretal.,2006)whichaimstolearntheco-occurrencebe-tweenfeaturesexpressingsimilarmeaningindif-ferentdomains.TopkEigenvectorsofmatrix,W,representtheprincipalpredictorsforweightspace,Q.Featuresfrombothdomainsareprojectedonthisprincipalpredictorspace,Q,toobtainasharedrepresentation.Sourcedomainclassiﬁerinourap-proachisbasedonthisSCLrepresentation.InSection4,weempiricallyshowhowouralgorithmgeneralizestodifferentsharedrepresentations.IterativeBuildingofTargetDomainLabeledData:Ifwehaveenoughlabeleddatafromthetargetdomainthenaclassiﬁercanbetrainedwith-outtheneedforadaptation.Hence,wewantedtoexploreifandhow(pseudo)labeleddataforthetargetdomaincanbecreated.Ourhypothe-sisisthatcertaintargetdomaininstancesaremoresimilartosourcedomaininstancesthantherest.Henceaclassiﬁertrainedon(asuitablychosentransformedrepresentationof)sourcedomainin-stanceswillbeabletocategorizesimilartargetdo-maininstancesconﬁdently.Suchconﬁdentlypre-dictedinstancescanbeconsideredaspseudola-beleddatawhicharethenusedtoinitializeaclas-siﬁerintargetdomain.Onlyhandfulofinstancesinthetargetdomaincanbeconﬁdentlypredictedusingthesharedrep-resentation,therefore,wefurtheriteratetocreatepseudolabeledinstancesintargetdomain.Inthenextroundofiterations,remainingunlabeledtar-getdomaininstancesarepassedthroughboththeclassiﬁersandtheiroutputaresuitablycombined.Again,conﬁdentlylabeledinstancesareaddedtothepoolofpseudolabeleddataandtheclassi-ﬁerinthetargetdomainisupdated.Thispro-cessisrepeatedtillallunlabeleddataislabeledorcertainmaximumnumberofiterationsisper-formed.Thiswaywegraduallyadaptthetargetdomainclassiﬁeronpseudolabeleddatausingtheknowledgetransferredfromsourcedomain.InSection4,weempiricallydemonstrateeffective-nessofthistechniquecomparedtoone-shotadap-tationapproaches.DomainSimilarity-basedAggregation:Perfor-manceofdomainadaptationisoftenconstrainedbythedissimilaritybetweenthesourceandtargetdomains(Luoetal.,2012;Rosensteinetal.,2005;Chin,2013;Blitzeretal.,2007).Ifthetwodo-mainsarelargelysimilar,theknowledgelearnedinthesourcedomaincanbeaggressivelytransferredtothetargetdomain.Ontheotherhand,ifthetwodomainsarelesssimilar,knowledgelearnedinthesourcedomainshouldbetransferredinaconserva-tivemannersoastomitigatetheeffectsofnegativetransfer.Therefore,itisimperativefordomainadaptationtechniquestoaccountforsimilaritybe-tweendomainsandtransferknowledgeinasimi-larityawaremanner.Whilethismaysoundobvi-ous,wedonotseemanyworksindomainadapta-tionliteraturethatleverageinter-domainsimilar-ityfortransferofknowledge.Inthiswork,weusethecosinesimilaritymeasuretocomputesimilar-itybetweentwodomainsandbasedonthatgradu-allytransferknowledgefromthesourcetothetar-getdomain.Whileitwouldbeinterestingtocom-parehowdifferentsimilaritymeasurescomparetowardspreventingnegativetransferbutthatisnotthefocusofthiswork.InSection4,weempiri-callyshowmarginalgainsoftransferringknowl-edgeinasimilarityawaremanner.55

Table1:Notationsusedinthisresearch.SymbolDescription{xsi,ysi}i=1:ns;xsi∈Rd;ysi∈{−1,+1}Labeledsourcedomaininstances{xti}i=1:nt;ˆyi∈{−1,+1}Unlabeledtargetdomaininstancesandpre-dictedlabelfortargetdomainQCo-occurrencebasedprojectionmatrixPu,PsPoolofunlabeledandpseudo-labeledtargetdomaininstancesrespectivelyCs,Ct;functionfromClassiﬁerCsistrainedon{(Qxsi,ysi)};classiﬁerCtistrainedon{xti,ˆyti}wherexti∈PsandˆyisthepseudolabelRd→{−1,+1}predictedlabelsbyEnsembleEαconﬁdenceofpredictionEWeightedensembleofCsandCtθ1,θ2conﬁdencethresholdforCsandensembleEws,wtWeightsforCsandCtrespectively3.1AlgorithmTable1liststhenotationsusedinthisresearch.In-putstothealgorithmarelabeledsourcedomainin-stances{xsi,ysi}i=1:nsandapoolofunlabeledtar-getdomaininstances{xti}i=1:nt,denotedbyPu.AsshowninFigure2,thestepsofthealgorithmareasfollows:1.LearnQ,asharedrepresentationprojectionmatrixfromthesourceandtargetdomains,usinganyoftheexistingtechniques.SCLisusedinthisresearch.2.LearnCsonSCL-basedrepresentationofla-beledsourcedomaininstances{Qxsi,ysi}.3.UseCstopredictlabels,ˆyi,forinstancesinPuusingtheSCL-basedrepresentationQxti.Instanceswhicharepredictedwithcon-ﬁdencegreaterthanapre-deﬁnedthreshold,θ1,aremovedfromPutoPswithpseudola-bel,ˆy.4.LearnCtfrominstancesinPs∈{xti,ˆyti}toincorporatetargetspeciﬁcfeatures.Psonlycontainsinstancesaddedinstep-3andwillbegrowingiteratively(hencethetrainingsethereissmall).5.CsandCtarecombinedinanensemble,E,asaweightedcombinationwithweightsaswsandwtwhicharebothinitializedto0.5.6.EnsembleEisappliedtoallremainingin-stancesinPutoobtainthelabelˆyias:E(xti)→ˆyi→wsCs(Qxti)+wtCt(xti)(1)(a)Iftheensembleclassiﬁesaninstancewithconﬁdencegreaterthanthethresh-oldθ2,thenitismovedfromPutoPsalongwithpseudolabelˆyi.Figure2:Illustrateslearningoftheinitialclassi-ﬁersanditerativelearningprocessoftheproposedsimilarity-awaredomainadaptationalgorithm.(b)Repeatstep-6forallxti∈Pu.7.WeightswsandwtareupdatedasshowninEqs.2and3.Thisupdatefacilitatesknowl-edgetransferwithintheensembleguidedbythesimilaritybetweendomains.ws(l+1)=(sim∗wsl∗I(Cs))(sim∗wsl∗I(Cs)+(1−sim)∗wtl∗I(Ct))(2)wt(l+1)=((1−sim)∗wtl∗I(Ct))(sim∗wsl∗I(Cs)+(1−sim)∗wtl∗I(Ct))(3)where,listheiteration,simisthesimilarityscorebetweendomainscomputedusingco-sinesimilaritymetricasshowninEq.4sim=a·b||a||||b||(4)wherea&barenormalizedvectorrepresen-tationsforthetwodomains.I(·)isthelossfunctiontomeasuretheerrorsofindividualclassiﬁersineachiteration:I(·)=exp{−ηl(C,Y)}(5)where,ηislearningratesetto0.1,l(y,ˆy)=(y−ˆy)2isthesquarelossfunction,yisthelabelpredictedbytheclassiﬁerandˆyisthelabelpredictedbytheensemble.8.Re-trainclassiﬁerCtonPs.9.Repeatstep6−8untilPuisemptyormaxi-mumnumberofiterationsisreached.56

Inthisiterativemanner,theproposedalgorithmtransformsunlabeleddatainthetestdomainintopseudolabeleddataandprogressivelylearnsclas-siﬁerCt.Conﬁdenceofprediction,αiforithin-stance,ismeasuredasthedistancefromthede-cisionboundary(Hsuetal.,2003)whichiscom-putedasshowninEq.6.α=R|v|(6)whereRistheun-normalizedoutputfromthesupportvectormachine(SVM)classiﬁer,vistheweightvectorforsupportvectorsand|v|=vTv.Weightsofindividualclassiﬁersintheensem-bleareupdatedwitheachiterationthatgradu-allyshiftsemphasisfromtheclassiﬁerlearnedonsharedrepresentationtotheclassiﬁerlearnedontargetdomain.Algorithm1illustratesthepro-posediterativelearningalgorithm.Algorithm1IterativeLearningAlgorithmInput:Cstrainedonsharedco-occurrencebasedrepresentationQx,CtinitiatedonTFIDFrepresentationfromPs,Puremainingunlabeledtargetdomaininstances.Iterate:l=0:tillPu={φ}orl≤iterMaxProcess:ConstructensembleEasweightedcombinationofCsandCtwithinitialsweightswslandwtlas0.5andsim=similaritybetweendomains.fori=1ton(sizeofPu)doPredictlabels:E(Qxi,xi)→ˆyi;calculateαiifαi>θ2thenRemoveithinstancefromPuandaddtoPswithpseudolabelˆyi.endif.endfor.RetrainCtonPsandupdatewslandwtl.enditerate.Output:UpdatedCt,wslandwtl.4ExperimentalResultsTheefﬁcacyoftheproposedalgorithmiseval-uatedondifferentdatasetsforcross-domaintextclassiﬁcation(Blitzeretal.,2007),(Daietal.,2007).Inourexperiments,performanceiseval-uatedontwo-classclassiﬁcationtaskandreportedintermsofclassiﬁcationaccuracy.4.1Datasets&ExperimentalProtocolTheﬁrstdatasetistheAmazonreviewdataset(Blitzeretal.,2007)whichhasfourdifferentdomains,Books,DVDs,KitchenappliancesandElectronics.Eachdomaincomprises1000pos-itiveand1000negativereviews.Inallexperi-ments,1600labeledreviewsfromthesourceand1600unlabeledreviewsfromthetargetdomainsareusedintrainingandperformanceisreportedonthenon-overlapping400reviewsfromthetar-getdomain.Theseconddatasetisthe20Newsgroupsdataset(Lang,1995)whichisatextcollectionofapproximately20,000documentsevenlypar-titionedacross20newsgroups.Forcross-domaintextclassiﬁcationonthe20Newsgroupsdataset,wefollowedtheprotocolofDaietal.(2007)whereitisdividedintosixdifferentdatasetsandthetoptwocategoriesineacharepickedasthetwoclasses.Thedataisfurthersegregatedbasedonsub-categories,whereeachsub-categoryiscon-sideredasadifferentdomain.Table2listshowdifferentsub-categoriesarecombinedtorepresentthesourceandtargetdomains.Inourexperiments,4/5thofthesourceandtargetdataisusedtolearnsharedfeaturerepresentationandresultsarere-portedontheremaining1/5thofthetargetdata.Table2:Elaboratesdatasegregationonthe20Newsgroupsdatasetforcross-domainclassiﬁca-tion.datasetDsDtcompvsreccomp.graphicscomp.os.ms-windows.misccomp.sys.ibm.pc.hardwarecomp.windows.xcomp.sys.mac.hardwarerec.autosrec.motorcyclesrec.sport.baseballrec.sport.hockeycompvsscicomp.graphicscomp.sys.ibm.pc.hardwarecomp.os.ms-windows.misccomp.sys.mac.hardwaresci.cryptcomp.windows.xsci.electronicssci.medsci.spacecompvstalkcomp.graphicscomp.os.ms-windows.miscnewlinecomp.sys.mac.hardwarecomp.sys.ibm.pc.hardwarecomp.windows.xtalk.politics.gunstalk.politics.mideasttalk.politics.misctalk.religion.miscrecvsscirec.autosrec.motorcyclesrec.sport.baseballrec.sport.hockeysci.medsci.cryptsci.spacesci.electronicsrecvstalkrec.autosrec.sport.baseballrec.motorcyclesrec.sport.hockeytalk.politics.gunstalk.politics.mideasttalk.politics.misctalk.religion.miscscivstalksci.electronicssci.cryptsci.medsci.spacetalk.politics.misctalk.politics.gunstalk.religion.misctalk.politics.mideastThethirddatasetisarealworlddatasetcom-prisingtweetsabouttheproductsandservicesindifferentdomains.Thedatasetcomprisestweets/postsfromthreecollections,Coll1aboutgaming,Coll2aboutMicrosoftproductsandColl3aboutmobilesupport.Eachcollectionhas218positiveandnegativetweets.Thesetweetsarecollectedbasedonuser-deﬁnedkeywordscap-57

turedinalisteningenginewhichthencrawlsthesocialmediaandfetchescommentsmatchingthekeywords.Thisdatasetbeingnoisyandcompris-ingshort-textismorechallengingthantheprevi-oustwodatasets.Alldatasetsarepre-processedbyconvertingtolowercasefollowedbystemming.Featureselec-tionbasedondocumentfrequency(DF=5)reducesthenumberoffeaturesaswellasspeeduptheclassiﬁcationtask.ForAmazonreviewdataset,TFisusedforfeatureweighingwhereasTFIDFisusedforfeatureweighinginothertwodatasets.Inallourexperiments,constituentclas-siﬁersusedintheensemblearesupportvectorma-chines(SVMs)withradialbasisfunctionkernel.Performanceoftheproposedalgorithmforcross-domainclassiﬁcationtaskiscomparedwithdif-ferenttechniques1including1)in-domainclassi-ﬁertrainedandtestedonthesamedomaindata,2)baselineclassiﬁerwhichistrainedonthesourceanddirectlytestedonthetargetdomain,3)SCL2,awidelyuseddomainadaptationtechniqueforcross-domaintextclassiﬁcation,4)‘Proposedw/osim’,removingsimilarityfromEqs.2&3.4.2ResultsandAnalysisForcross-domainclassiﬁcation,theperformancedegradesmainlydueto1)featuredivergenceand2)negativetransferowingtolargelydissimilardo-mains.Table3showstheaccuracyofindivid-ualclassiﬁersandtheensembleforcross-domainclassiﬁcationontheAmazonreviewdataset.Theensemblehasbetteraccuracythantheindividualclassiﬁers,therefore,inourexperimentstheﬁ-nalreportedperformanceistheaccuracyoftheensemble.Thecombinationweightsintheen-semblerepresentthecontributionsofindividualclassiﬁerstowardclassiﬁcationaccuracy.Inourexperiments,themaximumnumberofiterations(iterMax)issetto30.Itisobservedthatattheendoftheiterativelearningprocess,thetargetspe-ciﬁcclassiﬁerisassignedmoreweightmassascomparedtotheclassiﬁertrainedonthesharedrepresentation.Onaverage,theweightsforthetwoclassiﬁersconvergetows=0.22andwt=0.78attheendoftheiterativelearningprocess.1Wealsocomparedourperformancewithsentimentsen-sitivethesaurus(SST)proposedby(Bollegalaetal.,2013)andouralgorithmoutperformedonourprotocol.However,wedidnotincludecomparativeresultsbecauseofdifferenceinexperimentalprotocolasSSTistailoredforusingmultiplesourcedomainsandourprotocolusessinglesourcedomain.2OurimplementationofSCLisusedinthispaper.Table3:ComparingtheperformanceofindividualclassiﬁersandtheensemblefortrainingonBooksdomainandtestacrossdifferentdomains.CsandCtareappliedonthetestdomaindatabeforeper-formingtheiteratinglearningprocess.SD→TDCsCtEnsembleB→D63.134.872.1B→E64.539.175.8B→K68.442.376.2Table4:Listsomeexamplesofdomainspeciﬁcdiscriminativefeatureslearnedbytheproposedal-gorithmontheAmazonreviewdataset.DomainDomainspeciﬁcfeaturesBookspicturesillustrations,moredetail,toreadDvDsDeﬁnitebuy,deliverypromptKitcheninvaluableresource,rust,deliciousElectronicsBargain,Energysaving,actuallyuseThisfurthervalidatesourassertionthatthetar-getspeciﬁcfeaturesaremorediscriminativethanthesharedfeaturesinclassifyingtargetdomainin-stances,whichareefﬁcientlycapturedbythepro-posedalgorithm.Keyobservationsandanalysisfromtheexperimentsondifferentdatasetsissum-marizedbelow.4.2.1ResultsontheAmazonReviewdatasetTostudytheeffectsofdifferentcomponentsoftheproposedalgorithm,comprehensiveexperimentsareperformedontheAmazonreviewdataset3.1)Effectoflearningtargetspeciﬁcfeatures:Re-sultsinFigure3showthatiterativelylearningtar-getspeciﬁcfeaturerepresentation(slowtransferasopposedtoone-shottransfer)yieldsbetterperfor-manceacrossdifferentcross-domainclassiﬁcationtasksascomparedtoSCL,SFA(Panetal.,2010)4andthebaseline.UnlikeSCLandSFA,thepro-posedapproachusessharedandtargetspeciﬁcfea-turerepresentationsforthecross-domainclassiﬁ-cationtask.Table4illustratessomeexamplesofthetargetspeciﬁcdiscriminativefeatureslearnedbytheproposedalgorithmthatleadstoenhancedperformance.At95%conﬁdence,parametrict-testsuggeststhattheproposedalgorithmandSCLaresigniﬁcantly(statistically)different.2)Effectofsimilarityonperformance:Itisob-servedthatexistingdomainadaptationtechniquesenhancetheaccuracyforcross-domainclassiﬁca-tion,though,negativetransferexistsincamou-3Duetospacerestrictions,weshowthisanalysisonlyononedataset;howeversimilarconclusionsweredrawnfromotherdatasetsaswell.4Wedirectlycomparedourresultswiththeperformancereportedin(Panetal.,2010).58

Figure3:Comparingtheperformanceoftheproposedapproachwithexistingtechniquesforcross-domainclassiﬁcationonAmazonreviewdataset.ﬂage.ResultsinFigure3(b)(forthecaseK→B)describesanevidentscenariofornegativetrans-ferwheretheadaptationperformancewithSCLdescendslowerthanthebaseline.However,theproposedalgorithmstillsustainstheperformancebytransferringknowledgeproportionatetosimi-laritybetweenthetwodomains.Tofurtheran-alyzetheeffectofsimilarity,wesegregatedthe12cross-domainclassiﬁcationcasesintotwocat-egoriesbasedonsimilaritybetweentwothepar-ticipatingdomainsi.e.1)>0.5and2)<0.5.Table5showsthatfor6outof12casesthatfallintheﬁrstcategory,theaverageaccuracygainis10.8%ascomparedtothebaseline.Whilefortheremaining6casesthatfallinthesecondcat-egory,theaverageaccuracygainis15.4%ascom-paredtothebaseline.Thisstronglyelucidatesthattheproposedsimilarity-basediterativealgorithmnotonlyadaptswellwhenthedomainsimilarityishighbutalsoyieldsgainintheaccuracywhenthedomainsarelargelydissimilar.Figure4alsoshowshowweightforthetargetdomainclassi-ﬁerwtvarieswiththenumberofiterations.Itfurtherstrengthensourassertionthatifdomainsaresimilar,algorithmcanreadilyadaptandcon-vergesinafewiterations.Ontheotherhandfordissimilardomains,slowiterativetransfer,asop-posedtoone-shottransfer,canachievesimilarper-formance;however,itmaytakemoreiterationstoconverge.Whiletheeffectofsimilarityondo-mainadaptationperformanceisevident,thisworkopenspossibilitiesforfurtherinvestigations.3)Effectofvaryingthresholdθ1&θ2:Figure5(a)explainstheeffectofvaryingθ1ontheﬁnalclassiﬁcationaccuracy.Ifθ1islow,Ctmaygettrainedonincorrectlypredictedpseudolabeledin-stances;whereas,ifθ1ishigh,Ctmaybedeﬁ-cientofinstancestolearnagooddecisionbound-ary.Ontheotherhand,θ2inﬂuencesthenumberofiterationsrequiredbythealgorithmtoreachtheTable5:Effectofsimilarityonaccuracygainforcross-domainclassiﬁcationontheAmazonreviewdataset.CategorySD→TDSimGainAvg.(SD)>0.5E→K0.7813.110.8(4.9)K→E0.7810.6B→K0.548.0K→B0.542.9B→E0.5213.1E→B0.5217.2<0.5K→D0.348.915.4(4.4)D→K0.3421.6E→D0.3314.5D→E0.3314.5B→D0.2914.1D→B0.2919.1Figure4:Illustrateshowtheweight(wt)fortar-getdomainclassiﬁersvariesforthemostandleastsimilardomainswithnumberofiterations.stoppingcriteria.Ifthisthresholdislow,thealgo-rithmconvergesaggressively(inafewiterations)anddoesnotbeneﬁtfromtheiterativenatureoflearningthetargetspeciﬁcfeatures.Whereasahighthresholdtendstomakethealgorithmcon-servative.Ithamperstheaccuracybecauseoftheunavailabilityofsufﬁcientinstancestoupdatetheclassiﬁeraftereachiterationwhichalsoleadstolargenumberofiterationstoconverge(maynotevenconverge).θ1andθ2aresetempiricallyonaheld-outset,withvaluesrangingfromzerotodistanceoffarthestclassiﬁedinstancefromtheSVMhyper-plane(Hsuetal.,2003).Theknee-shapedcurveonthegraphsinFigure5showsthatthereexists59

Figure5:Barplotshows%ofdatathatcrossesconﬁdencethreshold,lowerandupperpartofthebarrepresents%correctlyandwronglypredictedpseudolabels.Theblacklineshowshowtheﬁnalclassiﬁcationaccuracyiseffectedwiththreshold.anoptimalvalueforθ1andθ2whichyieldsthebestaccuracy.Weobservedthatthebestaccuracyisobtainedwhenthethresholdsaresettothedis-tancebetweenthehyperplaneandthefarthestsup-portvectorineachclass.4)Effectofusingdifferentsharedrepresen-tationsinensemble:Tostudythegeneraliza-tionabilityoftheproposedalgorithmtodiffer-entsharedrepresentations,experimentsareper-formedusingthreedifferentsharedrepresenta-tionsontheAmazonreviewdataset.ApartfromusingtheSCLrepresentation,theaccuracyiscomparedwiththeproposedalgorithmusingtwootherrepresentations,1)commonfeaturesbe-tweenthetwodomains(“common”)and2)multi-viewprincipalcomponentanalysisbasedrepre-sentation(“MVPCA”)(Jietal.,2011)astheyarepreviouslyusedforcross-domainsentimentclas-siﬁcationonthesamedataset.Table6showsthattheproposedalgorithmyieldssigniﬁcantgainsincross-domainclassiﬁcationaccuracywithallthreerepresentationsandisnotrestrictedtoanyspe-ciﬁcrepresentation.Theﬁnalaccuracydependsontheinitialclassiﬁertrainedonthesharedrepre-sentation;therefore,ifasharedrepresentationsuf-ﬁcientlycapturesthecharacteristicsofbothsourceandtargetdomains,theproposedalgorithmcanbebuiltonanysuchrepresentationforenhancedcross-domainclassiﬁcationaccuracy.4.2.2Resultson20NewsgroupsdataResultsinFigure6comparestheaccuracyofpro-posedalgorithmwithexistingapproachesonthe20Newsgroupsdataset.Sincedifferentdomainarecraftedoutfromthesub-categoriesofthesamedataset,domainsareexceedinglysimilarandtherefore,thebaselineaccuracyisrelativelybetterTable6:Comparingtheaccuracyofproposedal-gorithmbuiltondifferentsharedrepresentations.SD→TDCommonMVPCASCLB→D66.876.478.2B→E69.079.280.6B→K71.479.279.8D→B64.578.479.3D→E62.876.476.2D→K64.380.982.4E→B68.977.878.5E→D65.777.077.3E→K75.185.486.2K→B71.371.071.1K→D70.475.076.1K→E76.785.786.4Figure6:Resultscomparingtheaccuracyofpro-posedapproachwithexistingtechniquesforcrossdomaincategorizationon20Newsgroupsdataset.thanthatontheothertwodatasets.Theproposedalgorithmstillyieldsanimprovementofatleast10.8%overthebaselineaccuracy.AscomparedtootherexistingdomainadaptationapproacheslikeSCL(Blitzeretal.,2007)andCoCC(Daietal.,2007),theproposedalgorithmoutperformsbyatleast4%and1.9%respectively.Thisalsovali-datesourassertionthatgenerallydomainadapta-tiontechniquesaccomplisheswellwhenthepar-ticipatingdomainsarelargelysimilar;however,thesimilarityaggregationandtheiterativelearn-ingoffertheproposedalgorithmanedgeoverone-shotadaptationalgorithms.4.2.3ResultsonrealworlddataResultsinFigure7exhibitchallengesassociatedwithrealworlddataset.Thebaselineaccuracyforcross-domainclassiﬁcationtaskisseverelyaf-fectedforthisdataset.SCLbaseddomainadap-tationdoesnotyieldsgenerousimprovementsasselectingthepivotfeaturesandcomputingtheco-occurrencestatisticswithnoisyshorttextisardu-ousandinept.Ontheotherhand,theproposedalgorithmiterativelylearnsdiscriminativetargetspeciﬁcfeaturesfromsuchperplexingdataandtranslatesittoanimprovementofatleast6.4%and3.5%overthebaselineandtheSCLrespec-60

Figure7:Resultscomparingtheaccuracyoftheproposedapproachwithexistingtechniquesforcrossdomaincategorizationontherealworlddataset.tively.5ConclusionThepaperpresentsaniterativesimilarity-awaredomainadaptationalgorithmthatprogressivelylearnsdomainspeciﬁcfeaturesfromtheunlabeledtestdomaindatastartingwithasharedfeaturerep-resentation.Ineachiteration,theproposedalgo-rithmassignspseudolabelstotheunlabeleddatawhicharethenusedtoupdatetheconstituentclas-siﬁersandtheirweightsintheensemble.Updatingthetargetspeciﬁcclassiﬁerineachiterationhelpsbetterlearnthedomainspeciﬁcfeaturesandthus,resultsinenhancedcross-domainclassiﬁcationac-curacy.Similaritybetweenthetwodomainsisag-gregatedwhileupdatingweightsoftheconstituentclassiﬁerswhichfacilitatesgradualshiftofknowl-edgefromthesourcetothetargetdomain.Finally,experimentalresultsforcross-domainclassiﬁca-tionondifferentdatasetsshowtheefﬁcacyoftheproposedalgorithmascomparedtootherexistingapproaches.ReferencesA.AueandM.Gamon.2005.Customizingsentimentclas-siﬁerstonewdomains:Acasestudy.Technicalreport,MicrosoftResearch.J.Blitzer,R.McDonald,andF.Pereira.2006.Domainadap-tationwithstructuralcorrespondencelearning.InPro-ceedingsofConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages120–128.J.Blitzer,M.Dredze,andF.Pereira.2007.Biographies,bollywood,boomboxesandblenders:Domainadaptationforsentimentclassiﬁcation.InProceedingsofAssociationforComputationalLinguistics,pages187–205.D.Bollegala,D.Weir,andJ.Carroll.2011.Usingmulti-plesourcestoconstructasentimentsensitivethesaurusforcross-domainsentimentclassiﬁcation.InProceedingsofAssociationforComputationalLinguistics:HumanLan-guageTechnologies,pages132–141.D.Bollegala,D.Weir,andJ.Carroll.2013.Cross-domainsentimentclassiﬁcationusingasentimentsensitivethe-saurus.IEEETransactionsonKnowledgeandDataEn-gineering,25(8):1719–1731.M.Chen,K.QWeinberger,andJ.Blitzer.2011.Co-trainingfordomainadaptation.InProceedingsofAd-vancesinNeuralInformationProcessingSystems,pages2456–2464.Si-ChiChin.2013.Knowledgetransfer:what,how,andwhy.WDai,G-RXue,QYang,andYYu.2007.Co-clusteringbasedclassiﬁcationforout-of-domaindocuments.InPro-ceedingsofInternationalConferenceonKnowledgeDis-coveryandDataMining,pages210–219.HalDaum´eIII.2009.Frustratinglyeasydomainadaptation.arXivpreprintarXiv:0907.1815.I.S.Dhillon,S.Mallela,andD.SModha.2003.Information-theoreticco-clustering.InProceedingsofInternationalConferenceonKnowledgeDiscoveryandDataMining,pages89–98.A.Garcia-Fernandez,O.Ferret,andM.Dinarelli.2014.Evaluationofdifferentstrategiesfordomainadaptationinopinionmining.InProceedingsoftheInternationalCon-ferenceonLanguageResourcesandEvaluation,pages26–31.C-W.Hsu,C.-C.Chang,andC.-J.Lin.2003.Apracticalguidetosupportvectorclassiﬁcation.Technicalreport,DepartmentofComputerScience,NationalTaiwanUni-versity.Y.-S.Ji,J.-J.Chen,G.Niu,L.Shang,andX.-Y.Dai.2011.Transferlearningviamulti-viewprincipalcompo-nentanalysis.JournalofComputerScienceandTechnol-ogy,26(1):81–98.J.JiangandC.Zhai.2007.Instanceweightingfordo-mainadaptationinNLP.InProceedingsofAssociationforComputationalLinguistics,volume7,pages264–271.KLang.1995.Newsweeder:Learningtoﬁlternetnews.InProceedingsofInternationalConferenceonMachineLearning.X.Liao,Y.Xue,andL.Carin.2005.Logisticregressionwithanauxiliarydatasource.InProceedingsofInternationalConferenceonMachineLearning,pages505–512.C.Luo,Y.Ji,X.Dai,andJ.Chen.2012.Activelearningwithtransferlearning.InProceedingsofAssociationforCom-putationalLinguisticsStudentResearchWorkshop,pages13–18.AssociationforComputationalLinguistics.S.J.PanandQ.Yang.2010a.Asurveyontransferlearning.IEEETransactionsonKnowledgeandDataEngineering,22(10):1345–1359.SinnoJialinPanandQiangYang.2010b.Asurveyontrans-ferlearning.KnowledgeandDataEngineering,IEEETransactionson,22(10):1345–1359.SinnoJialinPan,JamesTKwok,andQiangYang.2008.Transferlearningviadimensionalityreduction.InAAAI,volume8,pages677–682.61

S.J.Pan,X.Ni,J-TSun,Q.Yang,andZ.Chen.2010.Cross-domainsentimentclassiﬁcationviaspectralfeaturealign-ment.InProceedingsInternationalConferenceonWorldWideWeb,pages751–760.ACM.S.J.Pan,I.W.Tsang,J.T.Kwok,andQ.Yang.2011.Do-mainadaptationviatransfercomponentanalysis.IEEETransactionsonNeuralNetworks,22(2):199–210.M.T.Rosenstein,Z.Marx,L.P.Kaelbling,andT.G.Di-etterich.2005.Totransferornottotransfer.InPro-ceedingsofAdvancesinNeuralInformationProcessingSystemsWorkshop,InductiveTransfer:10YearsLater.A.Saha,P.Rai,H.Daum´e,S.Venkatasubramanian,andS.L.DuVall.2011.Activesuperviseddomainadaptation.InProceedingsofEuropeanConferenceonMachineLearn-ingandKnowledgeDiscoveryinDatabases,pages97–112.R.Xia,C.Zong,X.Hu,andE.Cambria.2013.Featureen-sembleplussampleselection:domainadaptationforsen-timentclassiﬁcation.IEEEIntelligentSystems,28(3):10–18.P.ZhaoandS.C.H.Hoi.2010.OTL:AFrameworkofOnlineTransferLearning.InProceedingofInternationalConferenceonMachineLearning.

######################
## output of PY2PDF ##
######################

53



54



55



56



57



58



59



60



61



