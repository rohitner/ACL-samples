#######################
## output of pdf2txt ##
#######################

Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1–11,

Lisbon, Portugal, 17-21 September 2015. c(cid:13)2015 Association for Computational Linguistics.

1

LanguageUnderstandingforText-basedGamesusingDeepReinforcementLearningKarthikNarasimhan∗CSAIL,MITkarthikn@csail.mit.eduTejasDKulkarni∗CSAIL,BCS,MITtejask@mit.eduReginaBarzilayCSAIL,MITregina@csail.mit.eduAbstractInthispaper,weconsiderthetaskoflearn-ingcontrolpoliciesfortext-basedgames.Inthesegames,allinteractionsinthevir-tualworldarethroughtextandtheun-derlyingstateisnotobserved.There-sultinglanguagebarriermakessuchenvi-ronmentschallengingforautomaticgameplayers.Weemployadeepreinforcementlearningframeworktojointlylearnstaterepresentationsandactionpoliciesusinggamerewardsasfeedback.Thisframe-workenablesustomaptextdescriptionsintovectorrepresentationsthatcapturethesemanticsofthegamestates.Weeval-uateourapproachontwogameworlds,comparingagainstbaselinesusingbag-of-wordsandbag-of-bigramsforstaterep-resentations.Ouralgorithmoutperformsthebaselinesonbothworldsdemonstrat-ingtheimportanceoflearningexpressiverepresentations.11IntroductionInthispaper,weaddressthetaskoflearningcon-trolpoliciesfortext-basedstrategygames.Thesegames,predecessorstomoderngraphicalones,stillenjoyalargefollowingworldwide.2Theyof-teninvolvecomplexworldswithrichinteractionsandelaboratetextualdescriptionsoftheunderly-ingstates(seeFigure1).Playersreaddescriptionsofthecurrentworldstateandrespondwithnaturallanguagecommandstotakeactions.Sincetheun-derlyingstateisnotdirectlyobservable,theplayerhastounderstandthetextinordertoact,makingit∗Bothauthorscontributedequallytothiswork.1Codeisavailableathttp://people.csail.mit.edu/karthikn/mud-play.2http://mudstats.com/State1:TheoldbridgeYouarestandingveryclosetothebridge’seasternfoundation.Ifyougoeastyouwillbebackonsolidground...Thebridgeswaysinthewind.Command:GoeastState2:RuinedgatehouseTheoldgatehouseisnearcollapse.Partofitsnorthernwallhasalreadyfallendown...Eastofthegatehouseleadsouttoasmallopenareasurroundedbytheremainsofthecastle.Thereisalsoastandingarchwayof-feringpassagetoapathalongtheoldsouth-erninnerwall.Exits:Standingarchway,castlecorner,BridgeovertheabyssFigure1:SamplegameplayfromaFantasyWorld.Theplayerwiththequestofﬁndingasecrettomb,iscurrentlylocatedonanoldbridge.Shethenchoosesanactiontogoeastthatbringshertoaruinedgatehouse(State2).challengingforexistingAIprogramstoplaythesegames(DePristoandZubek,2001).Indesigninganautonomousgameplayer,wehaveconsiderablelatitudewhenselectinganad-equatestaterepresentationtouse.Thesimplestmethodistouseabag-of-wordsrepresentationderivedfromthetextdescription.However,thisschemedisregardstheorderingofwordsandtheﬁnernuancesofmeaningthatevolvefromcom-posingwordsintosentencesandparagraphs.Forinstance,inState2inFigure1,theagenthastounderstandthatgoingeastwillleadittothecas-tlewhereasmovingsouthwilltakeittothestand-ingarchway.Analternativeapproachistoconverttextdescriptionstopre-speciﬁedrepresentationsusingannotatedtrainingdata,commonlyusedin2

languagegroundingtasks(Matuszeketal.,2013;Kushmanetal.,2014).Incontrast,ourgoalistolearnusefulrepresen-tationsinconjunctionwithcontrolpolicies.Weadoptareinforcementlearningframeworkandfor-mulategamesequencesasMarkovDecisionPro-cesses.Anagentplayingthegameaimstomaxi-mizerewardsthatitobtainsfromthegameengineupontheoccurrenceofcertainevents.Theagentlearnsapolicyintheformofanaction-valuefunc-tionQ(s,a)whichdenotesthelong-termmeritofanactionainstates.Theaction-valuefunctionisparametrizedus-ingadeeprecurrentneuralnetwork,trainedus-ingthegamefeedback.Thenetworkcontainstwomodules.Theﬁrstoneconvertstextualdescrip-tionsintovectorrepresentationsthatactasprox-iesforstates.Thiscomponentisimplementedus-ingLongShort-TermMemory(LSTM)networks(HochreiterandSchmidhuber,1997).Thesecondmoduleofthenetworkscorestheactionsgiventhevectorrepresentationcomputedbytheﬁrst.WeevaluateourmodelusingtwoMulti-UserDungeon(MUD)games(Curtis,1992;AmirandDoyle,2002).Theﬁrstgameisdesignedtopro-videacontrolledsetupforthetask,whilethesec-ondisapubliclyavailableoneandcontainshu-mangeneratedtextdescriptionswithsigniﬁcantlanguagevariability.Wecompareouralgorithmagainstbaselinesofarandomplayerandmod-elsthatusebag-of-wordsorbag-of-bigramsrep-resentationsforastate.WedemonstratethatourmodelLSTM-DQNsigniﬁcantlyoutperformsthebaselinesintermsofnumberofcompletedquestsandaccumulatedrewards.Forinstance,onafan-tasyMUDgame,ourmodellearnstocomplete96%ofthequests,whilethebag-of-wordsmodelandarandombaselinesolveonly82%and5%ofthequests,respectively.Moreover,weshowthattheacquiredrepresentationcanbereusedacrossgames,speedinguplearningandleadingtofasterconvergenceofQ-values.2RelatedWorkLearningcontrolpoliciesfromtextisgainingin-creasinginterestintheNLPcommunity.Exampleapplicationsincludeinterpretinghelpdocumenta-tionforsoftware(Branavanetal.,2010),navi-gatingwithdirections(VogelandJurafsky,2010;Kollaretal.,2010;ArtziandZettlemoyer,2013;Matuszeketal.,2013;AndreasandKlein,2015)andplayingcomputergames(Eisensteinetal.,2009;Branavanetal.,2011a).Gamesprovidearichdomainforgroundedlan-guageanalysis.Priorworkhasassumedperfectknowledgeoftheunderlyingstateofthegametolearnpolicies.GorniakandRoy(2005)developedagamecharacterthatcanbecontrolledbyspokeninstructionsadaptabletothegamesituation.Thegroundingofcommandstoactionsislearnedfromatranscriptmanuallyannotatedwithactionsandstateattributes.Eisensteinetal.(2009)learngamerulesbyanalyzingacollectionofgame-relateddocumentsandprecompiledtracesofthegame.Incontrasttotheabovework,ourmodelcombinestextinterpretationandstrategylearninginasingleframework.Asaresult,textualanalysisisguidedbythereceivedcontrolfeedback,andthelearnedstrategydirectlybuildsonthetextinterpretation.OurworkcloselyrelatestoanautomaticgameplayerthatutilizestextmanualstolearnstrategiesforCivilization(Branavanetal.,2011a).Similartoourapproach,textanalysisandcontrolstrate-giesarelearnedjointlyusingfeedbackprovidedbythegamesimulation.Intheirsetup,statesarefullyobservable,andthemodellearnsastrategybycombiningstate/actionfeaturesandfeaturesextractedfromtext.However,inourapplication,thestaterepresentationisnotprovided,buthastobeinferredfromatextualdescription.Therefore,itisnotsufﬁcienttoextractfeaturesfromtexttosupplementasimulation-basedplayer.Anotherrelatedlineofworkconsistsofauto-maticvideogameplayersthatinferstaterepre-sentationsdirectlyfromrawpixels(Koutn´ıketal.,2013;Mnihetal.,2015).Forinstance,Mnihetal.(2015)learncontrolstrategiesusingconvolu-tionalneuralnetworks,trainedwithavariantofQ-learning(WatkinsandDayan,1992).Whilebothapproachesusedeepreinforcementlearningfortraining,ourworkhasimportantdifferences.Inordertohandlethesequentialnatureoftext,weuseLongShort-TermMemorynetworkstoauto-maticallylearnusefulrepresentationsforarbitrarytextdescriptions.Additionally,weshowthatde-composingthenetworkintoarepresentationlayerandanactionselectorisusefulfortransferringthelearntrepresentationstonewgamescenarios.3BackgroundGameRepresentationWerepresentagamebythetuplehH,A,T,R,Ψi,whereHisthesetof3

allpossiblegamestates,A={(a,o)}isthesetofallcommands(action-objectpairs),T(h0|h,a,o)isthestochastictransitionfunctionbetweenstatesandR(h,a,o)istherewardfunction.ThegamestateHishiddenfromtheplayer,whoonlyre-ceivesavaryingtextualdescription,producedbyastochasticfunctionΨ:H→S.Speciﬁcally,theunderlyingstatehinthegameenginekeepstrackofattributessuchastheplayer’slocation,herhealthpoints,timeofday,etc.ThefunctionΨ(alsopartofthegameframework)thenconvertsthisstateintoatextualdescriptionofthelocationtheplayerisatoramessageindicatinglowhealth.WedonotassumeaccesstoeitherHorΨforouragentduringbothtrainingandtestingphasesofourexperiments.Wedenotethespaceofallpossi-bletextdescriptionsstobeS.Rewardsaregener-atedusingRandareonlygiventotheplayeruponcompletionofin-gamequests.Q-LearningReinforcementLearningisacom-monlyusedframeworkforlearningcontrolpoli-ciesingameenvironments(Silveretal.,2007;AmatoandShani,2010;Branavanetal.,2011b;Szita,2012).Thegameenvironmentcanbeformulatedasasequenceofstatetransitions(s,a,r,s0)ofaMarkovDecisionProcess(MDP).Theagenttakesanactionainstatesbyconsult-ingastate-actionvaluefunctionQ(s,a),whichisameasureoftheaction’sexpectedlong-termre-ward.Q-Learning(WatkinsandDayan,1992)isamodel-freetechniquewhichisusedtolearnanoptimalQ(s,a)fortheagent.Startingfromaran-domQ-function,theagentcontinuouslyupdatesitsQ-valuesbyplayingthegameandobtainingre-wards.TheiterativeupdatesarederivedfromtheBellmanequation(SuttonandBarto,1998):(1)Qi+1(s,a)=E[r+γmaxa0Qi(s0,a0)|s,a]whereγisadiscountfactorforfuturerewardsandtheexpectationisoverallgametransitionsthatin-volvedtheagenttakingactionainstates.UsingtheseevolvingQ-values,theagentchoosestheactionwiththehighestQ(s,a)tomaximizeitsexpectedfuturerewards.Inpractice,thetrade-offbetweenexplorationandexploitationcanbeachievedfollowingan-greedypolicy(Sut-tonandBarto,1998),wheretheagentperformsarandomactionwithprobability.DeepQ-NetworkInlargegames,itisoftenim-practicaltomaintaintheQ-valueforallpossibleMean PoolingLSTMLSTMLSTMLSTMLinearReLULinearLinearQ(s, a)Q(s, o)w1w2w3wn Rvs AFigure2:ArchitectureofLSTM-DQN:TheRep-resentationGenerator(φR)(bottom)takesasinputastreamofwordsobservedinstatesandproducesavectorrepresentationvs,whichisfedintotheactionscorer(φA)(top)toproducescoresforallactionsandargumentobjects.state-actionpairs.OnesolutiontothisproblemistoapproximateQ(s,a)usingaparametrizedfunctionQ(s,a;θ),whichcangeneralizeoverstatesandactionsbyconsideringhigher-levelat-tributes(SuttonandBarto,1998;Branavanetal.,2011a).However,creatingagoodparametrizationrequiresknowledgeofthestateandactionspaces.OnewaytobypassthisfeatureengineeringistouseaDeepQ-Network(DQN)(Mnihetal.,2015).TheDQNapproximatestheQ-valuefunctionwithadeepneuralnetworktopredictQ(s,a)forallpossibleactionsasimultaneouslygiventhecur-rentstates.Thenon-linearfunctionlayersoftheDQNalsoenableittolearnbettervaluefunctionsthanlinearapproximators.4LearningRepresentationsandControlPoliciesInthissection,wedescribeourmodel(DQN)anddescribeitsuseinlearninggoodQ-valueapproxi-mationsforgameswithstochastictextualdescrip-tions.Wedivideourmodelintotwoparts.Theﬁrstmoduleisarepresentationgeneratorthatcon-vertsthetextualdescriptionofthecurrentstateintoavector.Thisvectoristheninputintothesecondmodulewhichisanactionscorer.Fig-ure2showstheoverallarchitectureofourmodel.Welearntheparametersofboththerepresentationgeneratorandtheactionscorerjointly,usingthein-gamerewardfeedback.4

RepresentationGenerator(φR)Therepresen-tationgeneratorreadsrawtextdisplayedtotheagentandconvertsittoavectorrepresentationvs.Abag-of-words(BOW)representationisnotsuf-ﬁcienttocapturehigher-orderstructuresofsen-tencesandparagraphs.Theneedforabetterse-manticrepresentationofthetextisevidentfromtheaverageperformanceofthisrepresentationinplayingMUD-games(asweshowinSection6).Inordertoassimilatebetterrepresentations,weutilizeaLongShort-TermMemorynetwork(LSTM)(HochreiterandSchmidhuber,1997)asarepresentationgenerator.LSTMsarerecurrentneuralnetworkswiththeabilitytoconnectandrecognizelong-rangepatternsbetweenwordsintext.TheyaremorerobustthanBOWtosmallvariationsinwordusageandareabletocaptureunderlyingsemanticsofsentencestosomeex-tent.Inrecentwork,LSTMshavebeenusedsuc-cessfullyinNLPtaskssuchasmachinetransla-tion(Sutskeveretal.,2014)andsentimentanal-ysis(Taietal.,2015)tocomposevectorrepre-sentationsofsentencesfromword-levelembed-dings(Mikolovetal.,2013;Penningtonetal.,2014).Inoursetup,theLSTMnetworktakesinwordembeddingswkfromthewordsinadescrip-tionsandproducesoutputvectorsxkateachstep.Togettheﬁnalstaterepresentationvs,weaddameanpoolinglayerwhichcomputestheelement-wisemeanovertheoutputvectorsxk.3(2)vs=1nnXk=1xkActionScorer(φA)Theactionscorermoduleproducesscoresforthesetofpossibleactionsgiventhecurrentstaterepresentation.Weuseamulti-layeredneuralnetworkforthispurpose(seeFigure2).Theinputtothismoduleisthevec-torfromtherepresentationgenerator,vs=φR(s)andtheoutputsarescoresforactionsa∈A.Scoresforallactionsarepredictedsimultaneously,whichiscomputationallymoreefﬁcientthanscor-ingeachstate-actionpairseparately.Thus,bycombiningtherepresentationgeneratorandactionscorer,wecanobtaintheapproximationfortheQ-functionasQ(s,a)≈φA(φR(s))[a].AnadditionalcomplexityinplayingMUD-gamesisthattheactionstakenbytheplayerare3WealsoexperimentedwithconsideringjusttheoutputvectoroftheLSTMafterprocessingthelastword.Empiri-cally,weﬁndthatmeanpoolingleadstofasterlearning,soweuseitinallourexperiments.multi-wordnaturallanguagecommandssuchaseatappleorgoeast.Duetocomputationalcon-straints,inthisworkwelimitourselvestocon-sidercommandstoconsistofoneaction(e.g.eat)andoneargumentobject(e.g.apple).Thisas-sumptionholdsforthemajorityofthecommandsinourworlds,withtheexceptionofoneclassofcommandsthatrequiretwoarguments(e.g.movered-rootright,moveblue-rootup).Weconsiderallpossibleactionsandobjectsavailableinthegameandpredictbothforeachstateusingthesamenet-work(Figure2).WeconsidertheQ-valueoftheentirecommand(a,o)tobetheaverageoftheQ-valuesoftheactionaandtheobjecto.Fortherestofthissection,weonlyshowequationsforQ(s,a)butsimilaronesholdforQ(s,o).ParameterLearningWelearntheparametersθRoftherepresentationgeneratorandθAoftheactionscorerusingstochasticgradientdescentwithRMSprop(TielemanandHinton,2012).ThecompletetrainingprocedureisshowninAlgo-rithm1.Ineachiterationi,weupdatethepa-rameterstoreducethediscrepancybetweenthepredictedvalueofthecurrentstateQ(st,at;θi)(whereθi=[θR;θA]i)andtheexpectedQ-valuegiventherewardrtandthevalueofthenextstatemaxaQ(st+1,a;θi−1).Wekeeptrackoftheagent’spreviousexperi-encesinamemoryD.4InsteadofperformingupdatestotheQ-valueusingtransitionsfromthecurrentepisode,wesamplearandomtransition(ˆs,ˆa,s0,r)fromD.Updatingtheparametersinthiswayavoidsissuesduetostrongcorrelationwhenusingtransitionsofthesameepisode(Mnihetal.,2015).Usingthesampledtransitionand(1),weobtainthefollowinglossfunctiontominimize:(3)Li(θi)=Eˆs,ˆa[(yi−Q(ˆs,ˆa;θi))2]whereyi=Eˆs,ˆa[r+γmaxa0Q(s0,a0;θi−1)|ˆs,ˆa]isthetargetQ-valuewithparametersθi−1ﬁxedfromthepreviousiteration.Theupdatesontheparametersθcanbeper-formedusingthefollowinggradientofLi(θi):∇θiLi(θi)=Eˆs,ˆa[2(yi−Q(ˆs,ˆa;θi))∇θiQ(ˆs,ˆa;θi)]Foreachepochoftraining,theagentplaysseveralepisodesofthegame,whichisrestartedafterev-eryterminalstate.4Thememoryislimitedandrewritteninaﬁrst-in-ﬁrst-out(FIFO)fashion.5

Algorithm1TrainingProcedureforDQNwithprioritizedsampling1:InitializeexperiencememoryD2:Initializeparametersofrepresentationgenerator(φR)andactionscorer(φA)randomly3:forepisode=1,Mdo4:Initializegameandgetstartstatedescriptions15:fort=1,Tdo6:Convertst(text)torepresentationvstusingφR7:ifrandom()<then8:Selectarandomactionat9:else10:ComputeQ(st,a)forallactionsusingφA(vst)11:Selectat=argmaxQ(st,a)12:Executeactionatandobserverewardrtandnewstatest+113:Setprioritypt=1ifrt>0,elsept=014:Storetransition(st,at,rt,st+1,pt)inD15:Samplerandomminibatchoftransitions(sj,aj,rj,sj+1,pj)fromD,withfractionρhavingpj=116:Setyj=(cid:26)rjifsj+1isterminalrj+γmaxa0Q(sj+1,a0;θ)ifsj+1isnon-terminal17:PerformgradientdescentsteponthelossL(θ)=(yj−Q(sj,aj;θ))2Mini-batchSamplingInpractice,onlineup-datestotheparametersθareperformedoveraminibatchofstatetransitions,insteadofasingletransition.Thisincreasesthenumberofexperi-encesusedperstepandisalsomoreefﬁcientduetooptimizedmatrixoperations.Thesimplestmethodtocreatethesemini-batchesfromtheexperiencememoryDistosam-pleuniformlyatrandom.However,certainex-periencesaremorevaluablethanothersfortheagenttolearnfrom.Forinstance,raretransitionsthatprovidepositiverewardscanbeusedmoreof-tentolearnoptimalQ-valuesfaster.Inourex-periments,weconsidersuchpositive-rewardtran-sitionstohavehigherpriorityandkeeptrackoftheminD.Weuseprioritizedsampling(inspiredbyMooreandAtkeson(1993))tosampleafrac-tionρoftransitionsfromthehigherprioritypoolandafraction1−ρfromtherest.5ExperimentalSetupGameEnvironmentForourgameenviron-ment,wemodifyEvennia,5anopen-sourcelibraryforbuildingonlinetextualMUDgames.EvenniaisaPython-basedframeworkthatallowsonetoeasilycreatenewgamesbywritingabatchﬁledescribingtheenvironmentwithdetailsofrooms,5http://www.evennia.com/StatsHomeWorldFantasyWorldVocabularysize841340Avg.words/description10.565.21Maxdescriptions/room3100#diff.questdescriptions12-StatetransitionsDeterministicStochastic#states(underlying)16≥56Branchingfactor(#commands/state)40222Table1:Variousstatisticsofthetwogameworldsobjectsandactions.Thegameenginekeepstrackofthegamestateinternally,presentingtex-tualdescriptionstotheplayerandreceivingtextcommandsfromtheplayer.Weconductexper-imentsontwoworlds-asmallerHomeworldwecreatedourselves,andalarger,morecom-plexFantasyworldcreatedbyEvennia’sdevelop-ers.ThemotivationbehindHomeworldistoab-stractawayhigh-levelplanningandfocusonthelanguageunderstandingrequirementsofthegame.Table1providesstatisticsofthegameworlds.WeobservethattheFantasyworldismoderatelysizedwithavocabularyof1340wordsandupto100differentdescriptionsforaroom.Thesede-scriptionswerecreatedmanuallybythegamede-velopers.Thesediverse,engagingdescriptionsaredesignedtomakeitinterestingandexcitingforhu-manplayers.Severalroomshavemanyalternativedescriptions,invokedrandomlyoneachvisitby6

theplayer.Comparatively,theHomeworldissmaller:ithasaveryrestrictedvocabularyof84wordsandtheroomdescriptionsarerelativelystructured.However,boththeroomdescriptions(whicharealsovariedandrandomlyprovidedtotheagent)andthequestdescriptionswereadversariallycre-atedwithnegationandconjunctionoffactstoforceanagenttoactuallyunderstandthestateinordertoplaywell.Therefore,thisdomainpro-videsaninterestingchallengeforlanguageunder-standing.Inbothworlds,theagentreceivesapositiverewardoncompletingaquest,andnegativere-wardsforgettingintobadsituationslikefallingoffabridge,orlosingabattle.Wealsoaddsmalldeterministicnegativerewardsforeachnon-terminatingstep.Thisincentivizestheagenttolearnpoliciesthatsolvequestsinfewersteps.Thesupplementarymaterialhasdetailsontherewardstructure.HomeWorldWecreatedHomeworldtomimictheenvironmentofatypicalhouse.6Theworldconsistsoffourrooms-alivingroom,abedroom,akitchenandagardenwithconnectingpathways.Everyroomisreachablefromeveryotherroom.Eachroomcontainsarepresentativeobjectthattheagentcaninteractwith.Forinstance,thekitchenhasanapplethattheplayercaneat.Transitionsbetweentheroomsaredeterministic.Atthestartofeachgameepisode,theplayerisplacedinaran-domroomandprovidedwitharandomlyselectedquest.Thetextprovidedtotheplayercontainsboththedescriptionofhercurrentstateandthatofthequest.Thus,theplayercanbegininoneof16differentstates(4rooms×4quests),whichaddstotheworld’scomplexity.AnexampleofaquestgiventotheplayerintextisNotyouaresleepynowbutyouarehun-grynow.Tocompletethisquestandobtainare-ward,theplayerhastonavigatethroughthehousetoreachthekitchenandeattheapple(i.etypeinthecommandeatapple).Moreimportantly,theplayershouldinterpretthatthequestdoesnotre-quirehertotakeanapinthebedroom.Wecre-atedsuchmisguidingqueststomakeithardforagentstosucceedwithouthavinganadequateleveloflanguageunderstanding.6Anillustrationisprovidedinthesupplementarymaterial.FantasyWorldTheFantasyworldisconsider-ablymorecomplexandinvolvesquestssuchasnavigatingthroughabrokenbridgeorﬁndingthesecrettombofanancienthero.Thisgamealsohasstochastictransitionsinadditiontovaryingstatedescriptionsprovidedtotheplayer.Forinstance,thereisapossibilityoftheplayerfallingfromthebridgeifshelingerstoolongonit.Duetothelargecommandspaceinthisgame,7wemakeuseofcuesprovidedbythegameitselftonarrowdownthesetofpossibleobjectstoconsiderineachstate.Forinstance,intheMUDexampleinFigure1,thegameprovidesalistofpossibleexits.Ifthegamedoesnotprovidesuchcluesforthecurrentstate,weconsiderallobjectsinthegame.EvaluationWeusetwometricsformeasuringanagent’sperformance:(1)thecumulativerewardobtainedperepisodeaveragedovertheepisodesand(2)thefractionofquestscompletedbytheagent.Theevaluationprocedureisasfollows.Ineachepoch,weﬁrsttraintheagentonMepisodesofTstepseach.Attheendofthistraining,wehaveatestingphaseofrunningMepisodesofthegameforTsteps.WeuseM=50,T=20fortheHomeworldandM=20,T=250fortheFan-tasyworld.Forallevaluationepisodes,weruntheagentfollowingan-greedypolicywith=0.05,whichmakestheagentchoosethebestactionac-cordingtoitsQ-values95%ofthetime.Wereporttheagent’sperformanceateachepoch.BaselinesWecompareourLSTM-DQNmodelwiththreebaselines.TheﬁrstisaRandomagentthatchoosesbothactionsandobjectsuniformlyatrandomfromallavailablechoices.8TheothertwoareBOW-DQNandBI-DQN,whichuseabag-of-wordsandabag-of-bigramsrepresentationofthetext,respectively,asinputtotheDQNactionscorer.Thesebaselinesservetoillustratetheim-portanceofhavingagoodrepresentationlayerforthetask.SettingsForourDQNmodels,weusedD=100000,γ=0.5.Weusealearningrateof0.0005forRMSprop.Weannealthefor-greedyfrom1to0.2over100000transitions.Amini-batchgradientupdateisperformedevery4stepsofthegameplay.WerollouttheLSTM(overwords)for7Weconsider222possiblecommandcombinationsof6actionsand37objectarguments.8InthecaseoftheFantasyworld,theobjectchoicesarenarroweddownusinggamecluesasdescribedearlier.7

020406080100Epochs2.01.51.00.50.00.51.0RewardLSTM-DQNBI-DQNBOW-DQNRandomReward(Home)020406080100Epochs0.00.20.40.60.81.01.2Quest CompletionLSTM-DQNBI-DQNBOW-DQNRandomQuestcompletion(Home)01020304050Epochs1.51.00.50.00.51.0RewardNo TransferTransferTransferLearning(Home)010203040506070Epochs5.55.04.54.03.53.02.52.01.5Reward (log scale)LSTM-DQNBI-DQNBOW-DQNRandomReward(Fantasy)010203040506070Epochs0.00.20.40.60.81.01.2Quest CompletionLSTM-DQNBI-DQNBOW-DQNRandomQuestcompletion(Fantasy)020406080100Epochs2.01.51.00.50.00.51.0RewardUniformPrioritizedPrioritizedSampling(Home)Figure3:Left:GraphsshowingtheevolutionofaveragerewardandquestcompletionrateforBOW-DQN,LSTM-DQNandaRandombaselineontheHomeworld(top)andFantasyworld(bottom).NotethattherewardisshowninlogscalefortheFantasyworld.Right:GraphsshowingeffectsoftransferlearningandprioritizedsamplingontheHomeworld.amaximumof30stepsontheHomeworldandfor100stepsontheFantasyworld.Fortheprioritizedsampling,weusedρ=0.25forbothworlds.Weemployedamini-batchsizeof64andwordem-beddingsized=20inallexperiments.6ResultsHomeWorldFigure3illustratestheperfor-manceofLSTM-DQNcomparedtothebaselines.WecanobservethattheRandombaselineper-formsquitepoorly,completingonlyaround10%ofquestsonaverage9obtainingalowrewardofaround−1.58.TheBOW-DQNmodelperformssigniﬁcantlybetterandisabletocompletearound46%ofthequests,withanaveragerewardof0.20.Theimprovementinrewardisduetobothgreaterquestsuccessrateandalowerrateofissuingin-validcommands(e.g.eatapplewouldbeinvalidinthebedroomsincethereisnoapple).Weno-ticethatboththerewardandquestcompletiongraphsofthismodelarevolatile.ThisisbecausethemodelfailstopickoutdifferencesbetweenquestslikeNotyouarehungrynowbutyouaresleepynowandNotyouaresleepynowbutyou9Averagedoverthelast10epochs.arehungrynow.TheBI-DQNmodelsuffersfromthesameissuealthoughitperformsslightlybet-terthanBOW-DQNbycompleting48%ofquests.Incontrast,theLSTM-DQNmodeldoesnotsuf-ferfromthisissueandisabletocomplete100%ofthequestsafteraround50epochsoftraining,achievingclosetotheoptimalrewardpossible.10Thisdemonstratesthathavinganexpressiverep-resentationfortextiscrucialtounderstandingthegamestatesandchoosingintelligentactions.Inaddition,wealsoinvestigatedtheimpactofusingadeepneuralnetworkformodelingtheac-tionscorerφA.Figure4illustratestheperfor-manceoftheBOW-DQNandBI-DQNmodelsalongwiththeirsimplerversionsBOW-LINandBI-LIN,whichuseasinglelinearlayerforφA.ItcanbeseenthattheDQNmodelsclearlyachievebetterperformancethantheirlinearcounterparts,whichpointstothemmodelingthecontrolpolicybetter.FantasyWorldWeevaluateallthemodelsontheFantasyworldinthesamemannerasbeforeandreportreward,questcompletionratesandQ-10Notethatsinceeachstepincursapenaltyof−0.01,thebestreward(onaverage)aplayercangetisaround0.98.8

0102030405060Epochs0.10.20.30.40.50.60.7Quest CompletionBI-DQNBOW-DQNBI-LINBOW-LINFigure4:QuestcompletionratesofDQNvs.Lin-earmodelsonHomeworld.values.Thequestweevaluateoninvolvescrossingthebrokenbridge(whichtakesaminimumofﬁvesteps),withthepossibilityoffallingoffatrandom(a5%chance)whentheplayerisonthebridge.Thegamehasanadditionalquestofreachingasecrettomb.However,thisisacomplexquestthatrequirestheplayertomemorizegameeventsandperformhigh-levelplanningwhicharebeyondthescopeofthiscurrentwork.Therefore,wefocusonlyontheﬁrstquest.FromFigure3(bottom),wecanseethattheRandombaselinedoespoorlyintermsofbothav-erageper-episodereward11andquestcompletionrates.BOW-DQNconvergestoamuchhigherav-eragerewardof−12.68andachievesaround82%questcompletion.Again,theBOW-DQNisoftenconfusedbyvarying(10different)descriptionsoftheportionsofthebridge,whichreﬂectsinitser-raticperformanceonthequest.TheBI-DQNper-formsverywellonquestcompletionbyﬁnishing97%ofquests.However,thismodeltendstoﬁndsub-optimalsolutionsandgetsanaveragerewardof−26.68,evenworsethanBOW-DQN.Onerea-sonforthisisthenegativerewardstheagentob-tainsafterfallingoffthebridge.TheLSTM-DQNmodelagainperformsbest,achievinganaveragerewardof−11.33andcompleting96%ofquestsonaverage.Thoughthisworlddoesnotcon-taindescriptionsadversarialtoBOW-DQNorBI-DQN,theLSTM-DQNobtainshigheraveragere-wardbycompletingthequestinfewerstepsandshowingmoreresiliencetovariationsinthestatedescriptions.TransferLearningWewouldliketherepresen-tationslearntbyφRtobegenericenoughand11Notethattherewardsgraphisinlogscale.“Kitchen”“Living room”“Bedroom”“Garden”Figure5:t-SNEvisualizationofthewordembed-dings(exceptstopwords)aftertrainingonHomeworld.Theembeddingvaluesareinitializedran-domly.transferabletonewgameworlds.Totestthis,wecreatedasecondHomeworldwiththesamerooms,butacompletelydifferentmap,changingthelocationsoftheroomsandthepathwaysbe-tweenthem.Themaindifferentiatingfactorofthisworldfromtheoriginalhomeworldliesinthehigh-levelplanningrequiredtocompletequests.WeinitializedtheLSTMpartofanLSTM-DQNagentwithparametersθRlearntfromtheoriginalhomeworldandtraineditonthenewworld.12Figure3(topright)demonstratesthattheagentwithtransferredparametersisabletolearnquickerthananagentstartingfromscratchinitializedwithrandomparameters(NoTransfer),reachingtheoptimalpolicyalmost20epochsear-lier.Thisindicatesthatthesesimulatedworldscanbeusedtolearngoodrepresentationsforlanguagethattransferacrossworlds.PrioritizedsamplingWealsoinvestigatetheef-fectsofdifferentminibatchsamplingproceduresontheparameterlearning.FromFigure3(bottomright),weobservethatusingprioritizedsamplingsigniﬁcantlyspeedsuplearning,withtheagentachievingtheoptimalpolicyaround50epochsfasterthanusinguniformsampling.Thisshowspromiseforfurtherresearchintodifferentschemesofassigningprioritytotransitions.RepresentationAnalysisWeanalyzedtherep-resentationslearntbytheLSTM-DQNmodelontheHomeworld.Figure5showsavisualization12TheparametersfortheActionScorer(θA)areinitializedrandomly.9

DescriptionNearestneighborYouarehalfwaysoutontheunstablebridge.Fromthecastleyouhearadistanthowlingsound,likethatofalargedogorotherbeast.Thebridgeslopesprecariouslywhereitextendswestwardsto-wardsthelowestpoint-thecenterpointofthehangbridge.Youclasptheropesﬁrmlyasthebridgeswaysandcreaksunderyou.Theruinsopensuptotheskyinasmallopenarea,linedbycolumns....Tothewestisthegatehouseandentrancetothecastle,whereassouthwardsthecolumnsmakewayforawideopencourtyard.Theoldgatehouseisnearcollapse.....Eastthegatehouseleadsouttoasmallopenareasurroundedbytheremainsofthecas-tle.Thereisalsoastandingarchwayofferingpassagetoapathalongtheoldsoutherninnerwall.Table2:SampledescriptionsfromtheFantasyworldandtheirnearestneighbors(NN)accordingtotheirvectorrepresentationsfromtheLSTMrepresentationgenerator.TheNNsareoftendescriptionsofthesameorsimilar(nearby)statesinthegame.oflearntwordembeddings,reducedtotwodi-mensionsusingt-SNE(VanderMaatenandHin-ton,2008).Allthevectorswereinitializedran-domlybeforetraining.Wecanseethatsemanti-callysimilarwordsappearclosetogethertoformcoherentsubspaces.Infact,weobservefourdif-ferentsubspaces,eachforonetypeofroomalongwithitscorrespondingobject(s)andquestwords.Forinstance,fooditemslikepizzaandroomslikekitchenareveryclosetothewordhungrywhichappearsinaquestdescription.Thisshowsthattheagentlearnstoformmeaningfulassociationsbetweenthesemanticsofthequestandtheenvi-ronment.Table2showssomeexamplesofde-scriptionsfromFantasyworldandtheirnearestneighborsusingcosinesimilaritybetweentheircorrespondingvectorrepresentationsproducedbyLSTM-DQN.Themodelisabletocorrelatede-scriptionsofthesame(orsimilar)underlyingstatesandprojectthemontonearbypointsintherepresentationsubspace.7ConclusionsWeaddressthetaskofend-to-endlearningofcon-trolpoliciesfortext-basedgames.Inthesegames,allinteractionsinthevirtualworldarethroughtextandtheunderlyingstateisnotobserved.Theresultinglanguagevariabilitymakessuchenvi-ronmentschallengingforautomaticgameplay-ers.Weemployadeepreinforcementlearningframeworktojointlylearnstaterepresentationsandactionpoliciesusinggamerewardsasfeed-back.Thisframeworkenablesustomaptextde-scriptionsintovectorrepresentationsthatcapturethesemanticsofthegamestates.Ourexperimentsdemonstratetheimportanceoflearninggoodrep-resentationsoftextinordertoplaythesegameswell.Futuredirectionsincludetacklinghigh-levelplanningandstrategylearningtoimprovetheper-formanceofintelligentagents.AcknowledgementsWearegratefultothedevelopersofEvennia,thegameframeworkuponwhichthisworkisbased.WealsothankNateKushman,ClementGehring,GustavoGoretkin,membersofMIT’sNLPgroupandtheanonymousEMNLPreviewersforinsight-fulcommentsandfeedback.T.Kulkarniwasgra-ciouslysupportedbytheLeventhalFellowship.WewouldalsoliketoacknowledgeMIT’sCen-terforBrains,MindsandMachines(CBMM)forsupport.ReferencesChristopherAmatoandGuyShani.2010.High-levelreinforcementlearninginstrategygames.InPro-ceedingsofthe9thInternationalConferenceonAu-tonomousAgentsandMultiagentSystems:Volume1,pages75–82.InternationalFoundationforAu-tonomousAgentsandMultiagentSystems.EyalAmirandPatrickDoyle.2002.Adventuregames:Achallengeforcognitiverobotics.InProc.Int.CognitiveRoboticsWorkshop,pages148–155.JacobAndreasandDanKlein.2015.Alignment-basedcompositionalsemanticsforinstructionfol-lowing.InProceedingsoftheConferenceonEm-piricalMethodsinNaturalLanguageProcessing.YoavArtziandLukeZettlemoyer.2013.Weaklysu-pervisedlearningofsemanticparsersformappinginstructionstoactions.TransactionsoftheAssocia-tionforComputationalLinguistics,1(1):49–62.SRKBranavan,LukeSZettlemoyer,andReginaBarzi-lay.2010.Readingbetweenthelines:Learningtomaphigh-levelinstructionstocommands.InPro-ceedingsofthe48thAnnualMeetingoftheAsso-ciationforComputationalLinguistics,pages1268–1277.AssociationforComputationalLinguistics.10

SRKBranavan,DavidSilver,andReginaBarzilay.2011a.Learningtowinbyreadingmanualsinamonte-carloframework.InProceedingsofthe49thAnnualMeetingoftheAssociationforComputa-tionalLinguistics:HumanLanguageTechnologies-Volume1,pages268–277.AssociationforCompu-tationalLinguistics.SRKBranavan,DavidSilver,andReginaBarzilay.2011b.Non-linearmonte-carlosearchinCiviliza-tionII.AAAIPress/InternationalJointConferencesonArtiﬁcialIntelligence.PavelCurtis.1992.Mudding:Socialphenomenaintext-basedvirtualrealities.Highnoonontheelec-tronicfrontier:Conceptualissuesincyberspace,pages347–374.MarkADePristoandRobertZubek.2001.being-in-the-world.InProceedingsofthe2001AAAISpringSymposiumonArtiﬁcialIntelligenceandInteractiveEntertainment,pages31–34.JacobEisenstein,JamesClarke,DanGoldwasser,andDanRoth.2009.Readingtolearn:Constructingfeaturesfromsemanticabstracts.InProceedingsoftheConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages958–967,Singapore,August.AssociationforComputationalLinguistics.PeterGorniakandDebRoy.2005.Speakingwithyoursidekick:Understandingsituatedspeechincom-puterroleplayinggames.InR.MichaelYoungandJohnE.Laird,editors,ProceedingsoftheFirstArti-ﬁcialIntelligenceandInteractiveDigitalEntertain-mentConference,June1-5,2005,MarinadelRey,California,USA,pages57–62.AAAIPress.SeppHochreiterandJ¨urgenSchmidhuber.1997.Longshort-termmemory.Neuralcomputation,9(8):1735–1780.ThomasKollar,StefanieTellex,DebRoy,andNicholasRoy.2010.Towardunderstandingnaturallanguagedirections.InHuman-RobotInteraction(HRI),20105thACM/IEEEInternationalConferenceon,pages259–266.IEEE.JanKoutn´ık,GiuseppeCuccu,J¨urgenSchmidhuber,andFaustinoGomez.2013.Evolvinglarge-scaleneuralnetworksforvision-basedreinforce-mentlearning.InProceedingsofthe15thannualconferenceonGeneticandevolutionarycomputa-tion,pages1061–1068.ACM.NateKushman,YoavArtzi,LukeZettlemoyer,andReginaBarzilay.2014.Learningtoautomaticallysolvealgebrawordproblems.ACL(1),pages271–281.CynthiaMatuszek,EvanHerbst,LukeZettlemoyer,andDieterFox.2013.Learningtoparsenaturallanguagecommandstoarobotcontrolsystem.InExperimentalRobotics,pages403–415.Springer.TomasMikolov,KaiChen,GregCorrado,andJef-freyDean.2013.Efﬁcientestimationofwordrepresentationsinvectorspace.arXivpreprintarXiv:1301.3781.VolodymyrMnih,KorayKavukcuoglu,DavidSilver,AndreiA.Rusu,JoelVeness,MarcG.Bellemare,AlexGraves,MartinRiedmiller,AndreasK.Fidje-land,GeorgOstrovski,StigPetersen,CharlesBeat-tie,AmirSadik,IoannisAntonoglou,HelenKing,DharshanKumaran,DaanWierstra,ShaneLegg,andDemisHassabis.2015.Human-levelcon-trolthroughdeepreinforcementlearning.Nature,518(7540):529–533,02.AndrewWMooreandChristopherGAtkeson.1993.Prioritizedsweeping:Reinforcementlearningwithlessdataandlesstime.MachineLearning,13(1):103–130.JeffreyPennington,RichardSocher,andChristopherDManning.2014.Glove:Globalvectorsforwordrepresentation.ProceedingsoftheEmpiricialMethodsinNaturalLanguageProcessing(EMNLP2014),12.DavidSilver,RichardSSutton,andMartinM¨uller.2007.Reinforcementlearningoflocalshapeinthegameofgo.InIJCAI,volume7,pages1053–1058.IlyaSutskever,OriolVinyals,andQuocVVLe.2014.Sequencetosequencelearningwithneuralnet-works.InAdvancesinNeuralInformationProcess-ingSystems,pages3104–3112.RichardSSuttonandAndrewGBarto.1998.Intro-ductiontoreinforcementlearning.MITPress.Istv´anSzita.2012.Reinforcementlearningingames.InReinforcementLearning,pages539–577.Springer.KaiShengTai,RichardSocher,andChristopherD.Manning.2015.Improvedsemanticrepresenta-tionsfromtree-structuredlongshort-termmemorynetworks.InProceedingsofthe53rdAnnualMeet-ingoftheAssociationforComputationalLinguisticsandthe7thInternationalJointConferenceonNatu-ralLanguageProcessing(Volume1:LongPapers),pages1556–1566,Beijing,China,July.AssociationforComputationalLinguistics.TijmenTielemanandGeoffreyHinton.2012.Lecture6.5-rmsprop:Dividethegradientbyarunningav-erageofitsrecentmagnitude.COURSERA:NeuralNetworksforMachineLearning,4.LaurensVanderMaatenandGeoffreyHinton.2008.Visualizingdatausingt-sne.JournalofMachineLearningResearch,9(2579-2605):85.AdamVogelandDanJurafsky.2010.Learningtofol-lownavigationaldirections.InProceedingsofthe48thAnnualMeetingoftheAssociationforCompu-tationalLinguistics,pages806–814.AssociationforComputationalLinguistics.11

ChristopherJCHWatkinsandPeterDayan.1992.Q-learning.Machinelearning,8(3-4):279–292.

######################
## output of PY2PDF ##
######################

2



3



4



5



6



7



8



9



10



11



